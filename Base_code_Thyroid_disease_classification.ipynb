{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BpIX1P2OPZ5-"
      },
      "source": [
        "# **IMPORTING CRUCIAL LIBRARIES**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Kfd4nsrAunQU"
      },
      "outputs": [],
      "source": [
        "# importing crucial library\n",
        "import numpy as np \n",
        "# numpy to work on numerical data\n",
        "import pandas as pd\n",
        "# pandas to perform operation on dataset\n",
        "import matplotlib.pyplot as plt \n",
        "# visualisation the database \n",
        "import seaborn as sns\n",
        "# for heatmap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "7GLi3drSuoi2"
      },
      "outputs": [],
      "source": [
        "# import the arrival dataset\n",
        "Data_Thyroid = pd.read_csv(\"/content/drive/MyDrive/thyroid dataset.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "HKc-Fy9KFOlj"
      },
      "outputs": [],
      "source": [
        "pd.set_option('max_columns', None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "knHqrHjzuomU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 775
        },
        "outputId": "2dfd1a91-cbe0-4352-9d6e-1de321dfaec0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   age sex on thyroxine query on thyroxine on antithyroid medication sick  \\\n",
              "0   41   F            f                  f                         f    f   \n",
              "1   23   F            f                  f                         f    f   \n",
              "2   46   M            f                  f                         f    f   \n",
              "3   70   F            t                  f                         f    f   \n",
              "4   70   F            f                  f                         f    f   \n",
              "5   18   F            t                  f                         f    f   \n",
              "6   59   F            f                  f                         f    f   \n",
              "7   80   F            f                  f                         f    f   \n",
              "8   66   F            f                  f                         f    f   \n",
              "9   68   M            f                  f                         f    f   \n",
              "10  84   F            f                  f                         f    f   \n",
              "11  67   F            t                  f                         f    f   \n",
              "12  71   F            f                  f                         f    t   \n",
              "13  59   F            f                  f                         f    f   \n",
              "14  28   M            f                  f                         f    f   \n",
              "15  65   F            f                  f                         f    f   \n",
              "16  42   ?            f                  f                         f    f   \n",
              "17  63   F            f                  f                         f    f   \n",
              "18  80   F            f                  f                         f    f   \n",
              "19  28   M            f                  f                         f    f   \n",
              "\n",
              "   pregnant thyroid surgery I131 treatment query hypothyroid  \\\n",
              "0         f               f              f                 f   \n",
              "1         f               f              f                 f   \n",
              "2         f               f              f                 f   \n",
              "3         f               f              f                 f   \n",
              "4         f               f              f                 f   \n",
              "5         f               f              f                 f   \n",
              "6         f               f              f                 f   \n",
              "7         f               f              f                 f   \n",
              "8         f               f              f                 f   \n",
              "9         f               f              f                 f   \n",
              "10        f               f              f                 f   \n",
              "11        f               f              f                 f   \n",
              "12        f               f              f                 f   \n",
              "13        f               f              f                 f   \n",
              "14        f               f              f                 f   \n",
              "15        f               f              f                 t   \n",
              "16        f               f              f                 f   \n",
              "17        f               f              f                 f   \n",
              "18        f               f              f                 f   \n",
              "19        f               f              f                 f   \n",
              "\n",
              "   query hyperthyroid lithium goitre tumor hypopituitary psych TSH measured  \\\n",
              "0                   f       f      f     f             f     f            t   \n",
              "1                   f       f      f     f             f     f            t   \n",
              "2                   f       f      f     f             f     f            t   \n",
              "3                   f       f      f     f             f     f            t   \n",
              "4                   f       f      f     f             f     f            t   \n",
              "5                   f       f      f     f             f     f            t   \n",
              "6                   f       f      f     f             f     f            f   \n",
              "7                   f       f      f     f             f     f            t   \n",
              "8                   f       f      f     t             f     f            t   \n",
              "9                   f       f      f     f             f     f            t   \n",
              "10                  f       f      f     t             f     f            t   \n",
              "11                  f       f      f     f             f     f            t   \n",
              "12                  t       f      f     f             f     f            t   \n",
              "13                  f       f      f     f             f     f            t   \n",
              "14                  f       f      f     f             f     f            t   \n",
              "15                  f       f      f     f             f     f            t   \n",
              "16                  f       f      f     f             f     f            t   \n",
              "17                  f       f      f     f             f     f            t   \n",
              "18                  f       f      f     f             f     t            t   \n",
              "19                  f       f      f     f             f     t            t   \n",
              "\n",
              "     TSH T3 measured   T3 TT4 measured  TT4 T4U measured   T4U FTI measured  \\\n",
              "0    1.3           t  2.5            t  125            t  1.14            t   \n",
              "1    4.1           t    2            t  102            f     ?            f   \n",
              "2   0.98           f    ?            t  109            t  0.91            t   \n",
              "3   0.16           t  1.9            t  175            f     ?            f   \n",
              "4   0.72           t  1.2            t   61            t  0.87            t   \n",
              "5   0.03           f    ?            t  183            t   1.3            t   \n",
              "6      ?           f    ?            t   72            t  0.92            t   \n",
              "7    2.2           t  0.6            t   80            t   0.7            t   \n",
              "8    0.6           t  2.2            t  123            t  0.93            t   \n",
              "9    2.4           t  1.6            t   83            t  0.89            t   \n",
              "10   1.1           t  2.2            t  115            t  0.95            t   \n",
              "11  0.03           f    ?            t  152            t  0.99            t   \n",
              "12  0.03           t  3.8            t  171            t  1.13            t   \n",
              "13   2.8           t  1.7            t   97            t  0.91            t   \n",
              "14   3.3           t  1.8            t  109            t  0.91            t   \n",
              "15    12           f    ?            t   99            t  1.14            t   \n",
              "16   1.2           t  1.8            t   70            t  0.86            t   \n",
              "17   1.5           t  1.2            t  117            t  0.96            t   \n",
              "18     6           t  1.6            t   99            t  0.95            t   \n",
              "19   2.1           t  2.6            t  121            t  0.94            t   \n",
              "\n",
              "    FTI TBG measured TBG referral source binaryClass  \n",
              "0   109            f   ?            SVHC           P  \n",
              "1     ?            f   ?           other           P  \n",
              "2   120            f   ?           other           P  \n",
              "3     ?            f   ?           other           P  \n",
              "4    70            f   ?             SVI           P  \n",
              "5   141            f   ?           other           P  \n",
              "6    78            f   ?           other           P  \n",
              "7   115            f   ?             SVI           P  \n",
              "8   132            f   ?             SVI           P  \n",
              "9    93            f   ?             SVI           P  \n",
              "10  121            f   ?             SVI           P  \n",
              "11  153            f   ?           other           P  \n",
              "12  151            f   ?           other           P  \n",
              "13  107            f   ?             SVI           P  \n",
              "14  119            f   ?            SVHC           P  \n",
              "15   87            f   ?           other           N  \n",
              "16   81            f   ?           other           P  \n",
              "17  121            f   ?             SVI           P  \n",
              "18  104            f   ?             SVI           P  \n",
              "19  130            f   ?            SVHC           P  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7242ea38-28e9-4657-ae64-a4425fefd533\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>sex</th>\n",
              "      <th>on thyroxine</th>\n",
              "      <th>query on thyroxine</th>\n",
              "      <th>on antithyroid medication</th>\n",
              "      <th>sick</th>\n",
              "      <th>pregnant</th>\n",
              "      <th>thyroid surgery</th>\n",
              "      <th>I131 treatment</th>\n",
              "      <th>query hypothyroid</th>\n",
              "      <th>query hyperthyroid</th>\n",
              "      <th>lithium</th>\n",
              "      <th>goitre</th>\n",
              "      <th>tumor</th>\n",
              "      <th>hypopituitary</th>\n",
              "      <th>psych</th>\n",
              "      <th>TSH measured</th>\n",
              "      <th>TSH</th>\n",
              "      <th>T3 measured</th>\n",
              "      <th>T3</th>\n",
              "      <th>TT4 measured</th>\n",
              "      <th>TT4</th>\n",
              "      <th>T4U measured</th>\n",
              "      <th>T4U</th>\n",
              "      <th>FTI measured</th>\n",
              "      <th>FTI</th>\n",
              "      <th>TBG measured</th>\n",
              "      <th>TBG</th>\n",
              "      <th>referral source</th>\n",
              "      <th>binaryClass</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>41</td>\n",
              "      <td>F</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>t</td>\n",
              "      <td>1.3</td>\n",
              "      <td>t</td>\n",
              "      <td>2.5</td>\n",
              "      <td>t</td>\n",
              "      <td>125</td>\n",
              "      <td>t</td>\n",
              "      <td>1.14</td>\n",
              "      <td>t</td>\n",
              "      <td>109</td>\n",
              "      <td>f</td>\n",
              "      <td>?</td>\n",
              "      <td>SVHC</td>\n",
              "      <td>P</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>23</td>\n",
              "      <td>F</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>t</td>\n",
              "      <td>4.1</td>\n",
              "      <td>t</td>\n",
              "      <td>2</td>\n",
              "      <td>t</td>\n",
              "      <td>102</td>\n",
              "      <td>f</td>\n",
              "      <td>?</td>\n",
              "      <td>f</td>\n",
              "      <td>?</td>\n",
              "      <td>f</td>\n",
              "      <td>?</td>\n",
              "      <td>other</td>\n",
              "      <td>P</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>46</td>\n",
              "      <td>M</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>t</td>\n",
              "      <td>0.98</td>\n",
              "      <td>f</td>\n",
              "      <td>?</td>\n",
              "      <td>t</td>\n",
              "      <td>109</td>\n",
              "      <td>t</td>\n",
              "      <td>0.91</td>\n",
              "      <td>t</td>\n",
              "      <td>120</td>\n",
              "      <td>f</td>\n",
              "      <td>?</td>\n",
              "      <td>other</td>\n",
              "      <td>P</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>70</td>\n",
              "      <td>F</td>\n",
              "      <td>t</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>t</td>\n",
              "      <td>0.16</td>\n",
              "      <td>t</td>\n",
              "      <td>1.9</td>\n",
              "      <td>t</td>\n",
              "      <td>175</td>\n",
              "      <td>f</td>\n",
              "      <td>?</td>\n",
              "      <td>f</td>\n",
              "      <td>?</td>\n",
              "      <td>f</td>\n",
              "      <td>?</td>\n",
              "      <td>other</td>\n",
              "      <td>P</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>70</td>\n",
              "      <td>F</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>t</td>\n",
              "      <td>0.72</td>\n",
              "      <td>t</td>\n",
              "      <td>1.2</td>\n",
              "      <td>t</td>\n",
              "      <td>61</td>\n",
              "      <td>t</td>\n",
              "      <td>0.87</td>\n",
              "      <td>t</td>\n",
              "      <td>70</td>\n",
              "      <td>f</td>\n",
              "      <td>?</td>\n",
              "      <td>SVI</td>\n",
              "      <td>P</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>18</td>\n",
              "      <td>F</td>\n",
              "      <td>t</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>t</td>\n",
              "      <td>0.03</td>\n",
              "      <td>f</td>\n",
              "      <td>?</td>\n",
              "      <td>t</td>\n",
              "      <td>183</td>\n",
              "      <td>t</td>\n",
              "      <td>1.3</td>\n",
              "      <td>t</td>\n",
              "      <td>141</td>\n",
              "      <td>f</td>\n",
              "      <td>?</td>\n",
              "      <td>other</td>\n",
              "      <td>P</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>59</td>\n",
              "      <td>F</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>?</td>\n",
              "      <td>f</td>\n",
              "      <td>?</td>\n",
              "      <td>t</td>\n",
              "      <td>72</td>\n",
              "      <td>t</td>\n",
              "      <td>0.92</td>\n",
              "      <td>t</td>\n",
              "      <td>78</td>\n",
              "      <td>f</td>\n",
              "      <td>?</td>\n",
              "      <td>other</td>\n",
              "      <td>P</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>80</td>\n",
              "      <td>F</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>t</td>\n",
              "      <td>2.2</td>\n",
              "      <td>t</td>\n",
              "      <td>0.6</td>\n",
              "      <td>t</td>\n",
              "      <td>80</td>\n",
              "      <td>t</td>\n",
              "      <td>0.7</td>\n",
              "      <td>t</td>\n",
              "      <td>115</td>\n",
              "      <td>f</td>\n",
              "      <td>?</td>\n",
              "      <td>SVI</td>\n",
              "      <td>P</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>66</td>\n",
              "      <td>F</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>t</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>t</td>\n",
              "      <td>0.6</td>\n",
              "      <td>t</td>\n",
              "      <td>2.2</td>\n",
              "      <td>t</td>\n",
              "      <td>123</td>\n",
              "      <td>t</td>\n",
              "      <td>0.93</td>\n",
              "      <td>t</td>\n",
              "      <td>132</td>\n",
              "      <td>f</td>\n",
              "      <td>?</td>\n",
              "      <td>SVI</td>\n",
              "      <td>P</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>68</td>\n",
              "      <td>M</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>t</td>\n",
              "      <td>2.4</td>\n",
              "      <td>t</td>\n",
              "      <td>1.6</td>\n",
              "      <td>t</td>\n",
              "      <td>83</td>\n",
              "      <td>t</td>\n",
              "      <td>0.89</td>\n",
              "      <td>t</td>\n",
              "      <td>93</td>\n",
              "      <td>f</td>\n",
              "      <td>?</td>\n",
              "      <td>SVI</td>\n",
              "      <td>P</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>84</td>\n",
              "      <td>F</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>t</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>t</td>\n",
              "      <td>1.1</td>\n",
              "      <td>t</td>\n",
              "      <td>2.2</td>\n",
              "      <td>t</td>\n",
              "      <td>115</td>\n",
              "      <td>t</td>\n",
              "      <td>0.95</td>\n",
              "      <td>t</td>\n",
              "      <td>121</td>\n",
              "      <td>f</td>\n",
              "      <td>?</td>\n",
              "      <td>SVI</td>\n",
              "      <td>P</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>67</td>\n",
              "      <td>F</td>\n",
              "      <td>t</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>t</td>\n",
              "      <td>0.03</td>\n",
              "      <td>f</td>\n",
              "      <td>?</td>\n",
              "      <td>t</td>\n",
              "      <td>152</td>\n",
              "      <td>t</td>\n",
              "      <td>0.99</td>\n",
              "      <td>t</td>\n",
              "      <td>153</td>\n",
              "      <td>f</td>\n",
              "      <td>?</td>\n",
              "      <td>other</td>\n",
              "      <td>P</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>71</td>\n",
              "      <td>F</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>t</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>t</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>t</td>\n",
              "      <td>0.03</td>\n",
              "      <td>t</td>\n",
              "      <td>3.8</td>\n",
              "      <td>t</td>\n",
              "      <td>171</td>\n",
              "      <td>t</td>\n",
              "      <td>1.13</td>\n",
              "      <td>t</td>\n",
              "      <td>151</td>\n",
              "      <td>f</td>\n",
              "      <td>?</td>\n",
              "      <td>other</td>\n",
              "      <td>P</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>59</td>\n",
              "      <td>F</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>t</td>\n",
              "      <td>2.8</td>\n",
              "      <td>t</td>\n",
              "      <td>1.7</td>\n",
              "      <td>t</td>\n",
              "      <td>97</td>\n",
              "      <td>t</td>\n",
              "      <td>0.91</td>\n",
              "      <td>t</td>\n",
              "      <td>107</td>\n",
              "      <td>f</td>\n",
              "      <td>?</td>\n",
              "      <td>SVI</td>\n",
              "      <td>P</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>28</td>\n",
              "      <td>M</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>t</td>\n",
              "      <td>3.3</td>\n",
              "      <td>t</td>\n",
              "      <td>1.8</td>\n",
              "      <td>t</td>\n",
              "      <td>109</td>\n",
              "      <td>t</td>\n",
              "      <td>0.91</td>\n",
              "      <td>t</td>\n",
              "      <td>119</td>\n",
              "      <td>f</td>\n",
              "      <td>?</td>\n",
              "      <td>SVHC</td>\n",
              "      <td>P</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>65</td>\n",
              "      <td>F</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>t</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>t</td>\n",
              "      <td>12</td>\n",
              "      <td>f</td>\n",
              "      <td>?</td>\n",
              "      <td>t</td>\n",
              "      <td>99</td>\n",
              "      <td>t</td>\n",
              "      <td>1.14</td>\n",
              "      <td>t</td>\n",
              "      <td>87</td>\n",
              "      <td>f</td>\n",
              "      <td>?</td>\n",
              "      <td>other</td>\n",
              "      <td>N</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>42</td>\n",
              "      <td>?</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>t</td>\n",
              "      <td>1.2</td>\n",
              "      <td>t</td>\n",
              "      <td>1.8</td>\n",
              "      <td>t</td>\n",
              "      <td>70</td>\n",
              "      <td>t</td>\n",
              "      <td>0.86</td>\n",
              "      <td>t</td>\n",
              "      <td>81</td>\n",
              "      <td>f</td>\n",
              "      <td>?</td>\n",
              "      <td>other</td>\n",
              "      <td>P</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>63</td>\n",
              "      <td>F</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>t</td>\n",
              "      <td>1.5</td>\n",
              "      <td>t</td>\n",
              "      <td>1.2</td>\n",
              "      <td>t</td>\n",
              "      <td>117</td>\n",
              "      <td>t</td>\n",
              "      <td>0.96</td>\n",
              "      <td>t</td>\n",
              "      <td>121</td>\n",
              "      <td>f</td>\n",
              "      <td>?</td>\n",
              "      <td>SVI</td>\n",
              "      <td>P</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>80</td>\n",
              "      <td>F</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>t</td>\n",
              "      <td>t</td>\n",
              "      <td>6</td>\n",
              "      <td>t</td>\n",
              "      <td>1.6</td>\n",
              "      <td>t</td>\n",
              "      <td>99</td>\n",
              "      <td>t</td>\n",
              "      <td>0.95</td>\n",
              "      <td>t</td>\n",
              "      <td>104</td>\n",
              "      <td>f</td>\n",
              "      <td>?</td>\n",
              "      <td>SVI</td>\n",
              "      <td>P</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>28</td>\n",
              "      <td>M</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>t</td>\n",
              "      <td>t</td>\n",
              "      <td>2.1</td>\n",
              "      <td>t</td>\n",
              "      <td>2.6</td>\n",
              "      <td>t</td>\n",
              "      <td>121</td>\n",
              "      <td>t</td>\n",
              "      <td>0.94</td>\n",
              "      <td>t</td>\n",
              "      <td>130</td>\n",
              "      <td>f</td>\n",
              "      <td>?</td>\n",
              "      <td>SVHC</td>\n",
              "      <td>P</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7242ea38-28e9-4657-ae64-a4425fefd533')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-7242ea38-28e9-4657-ae64-a4425fefd533 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-7242ea38-28e9-4657-ae64-a4425fefd533');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "Data_Thyroid.head(20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yvEZFKrUuoru",
        "outputId": "134f2fdb-1278-4701-c29c-dd1947a54396"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3163, 30)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "Data_Thyroid.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "8FaCHOaZuo2H",
        "outputId": "5fde8db8-a394-4dbf-d017-aa7203ecd7fa"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# Count plot graph of target variable of thyroid dataset\n",
        "sns.countplot(x='binaryClass' ,data=Data_Thyroid ) \n",
        "plt.xlabel ( 'Class')\n",
        "plt.ylabel ( 'Number')\n",
        "plt.title ( 'Count Plot of Thyroid disease')  \n",
        "plt.show() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hxTYNCmQXP0B",
        "outputId": "dfc43dcc-fd0c-426b-ca3a-810b768723e6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "P    2919\n",
              "N     244\n",
              "Name: binaryClass, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "Data_Thyroid['binaryClass'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QxqkJ-LhPjnc"
      },
      "source": [
        "# **PREPROCESSING**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ZIUdpBIZuovn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1258045c-bb73-4255-8dcb-d5dfd3ad08a9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method DataFrame.info of      age sex on thyroxine query on thyroxine on antithyroid medication sick  \\\n",
              "0     41   F            f                  f                         f    f   \n",
              "1     23   F            f                  f                         f    f   \n",
              "2     46   M            f                  f                         f    f   \n",
              "3     70   F            t                  f                         f    f   \n",
              "4     70   F            f                  f                         f    f   \n",
              "...   ..  ..          ...                ...                       ...  ...   \n",
              "3158  25   M            f                  f                         f    f   \n",
              "3159  71   F            t                  f                         f    f   \n",
              "3160  37   F            f                  f                         f    f   \n",
              "3161  56   F            f                  f                         f    f   \n",
              "3162  76   F            f                  f                         f    t   \n",
              "\n",
              "     pregnant thyroid surgery I131 treatment query hypothyroid  \\\n",
              "0           f               f              f                 f   \n",
              "1           f               f              f                 f   \n",
              "2           f               f              f                 f   \n",
              "3           f               f              f                 f   \n",
              "4           f               f              f                 f   \n",
              "...       ...             ...            ...               ...   \n",
              "3158        f               f              f                 f   \n",
              "3159        f               f              f                 f   \n",
              "3160        f               f              f                 f   \n",
              "3161        f               f              f                 f   \n",
              "3162        f               t              f                 f   \n",
              "\n",
              "     query hyperthyroid lithium goitre tumor hypopituitary psych TSH measured  \\\n",
              "0                     f       f      f     f             f     f            t   \n",
              "1                     f       f      f     f             f     f            t   \n",
              "2                     f       f      f     f             f     f            t   \n",
              "3                     f       f      f     f             f     f            t   \n",
              "4                     f       f      f     f             f     f            t   \n",
              "...                 ...     ...    ...   ...           ...   ...          ...   \n",
              "3158                  f       f      f     f             f     t            t   \n",
              "3159                  f       f      f     f             f     f            t   \n",
              "3160                  f       f      f     f             f     f            t   \n",
              "3161                  f       f      f     f             f     f            t   \n",
              "3162                  f       f      f     f             f     f            t   \n",
              "\n",
              "       TSH T3 measured   T3 TT4 measured  TT4 T4U measured   T4U FTI measured  \\\n",
              "0      1.3           t  2.5            t  125            t  1.14            t   \n",
              "1      4.1           t    2            t  102            f     ?            f   \n",
              "2     0.98           f    ?            t  109            t  0.91            t   \n",
              "3     0.16           t  1.9            t  175            f     ?            f   \n",
              "4     0.72           t  1.2            t   61            t  0.87            t   \n",
              "...    ...         ...  ...          ...  ...          ...   ...          ...   \n",
              "3158   1.6           t  2.1            t   89            t  0.86            t   \n",
              "3159   2.7           t  1.2            t  107            t   1.1            t   \n",
              "3160   5.1           t  2.5            t   84            t  1.05            t   \n",
              "3161   7.9           t  1.2            t   73            t   0.8            t   \n",
              "3162   1.8           t  1.2            t   65            t  0.76            t   \n",
              "\n",
              "      FTI TBG measured TBG referral source binaryClass  \n",
              "0     109            f   ?            SVHC           P  \n",
              "1       ?            f   ?           other           P  \n",
              "2     120            f   ?           other           P  \n",
              "3       ?            f   ?           other           P  \n",
              "4      70            f   ?             SVI           P  \n",
              "...   ...          ...  ..             ...         ...  \n",
              "3158  103            f   ?            SVHC           P  \n",
              "3159   97            f   ?           other           P  \n",
              "3160   80            f   ?            SVHC           P  \n",
              "3161   91            f   ?             SVI           N  \n",
              "3162   86            f   ?           other           P  \n",
              "\n",
              "[3163 rows x 30 columns]>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "Data_Thyroid.info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "NCYKTpUlDZDz"
      },
      "outputs": [],
      "source": [
        "Data_Thyroid = Data_Thyroid.replace(\"?\", np.nan)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "AdRloWuGuozF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37b77f66-f3c9-4ba4-a385-aaf146cf3d73"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "age                             1\n",
              "sex                           120\n",
              "on thyroxine                    0\n",
              "query on thyroxine              0\n",
              "on antithyroid medication       0\n",
              "sick                            0\n",
              "pregnant                        0\n",
              "thyroid surgery                 0\n",
              "I131 treatment                  0\n",
              "query hypothyroid               0\n",
              "query hyperthyroid              0\n",
              "lithium                         0\n",
              "goitre                          0\n",
              "tumor                           0\n",
              "hypopituitary                   0\n",
              "psych                           0\n",
              "TSH measured                    0\n",
              "TSH                           318\n",
              "T3 measured                     0\n",
              "T3                            671\n",
              "TT4 measured                    0\n",
              "TT4                           201\n",
              "T4U measured                    0\n",
              "T4U                           332\n",
              "FTI measured                    0\n",
              "FTI                           330\n",
              "TBG measured                    0\n",
              "TBG                          3163\n",
              "referral source                 0\n",
              "binaryClass                     0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "Data_Thyroid.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "x4tkTi3QD8rS"
      },
      "outputs": [],
      "source": [
        "Data_Thyroid = Data_Thyroid.drop(['TBG'], axis=1) # to drop the columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "wSO-T8cnIGe9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f766b4d-588f-4914-8c23-c09d90b4db38"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "age                          object\n",
              "sex                          object\n",
              "on thyroxine                 object\n",
              "query on thyroxine           object\n",
              "on antithyroid medication    object\n",
              "sick                         object\n",
              "pregnant                     object\n",
              "thyroid surgery              object\n",
              "I131 treatment               object\n",
              "query hypothyroid            object\n",
              "query hyperthyroid           object\n",
              "lithium                      object\n",
              "goitre                       object\n",
              "tumor                        object\n",
              "hypopituitary                object\n",
              "psych                        object\n",
              "TSH measured                 object\n",
              "TSH                          object\n",
              "T3 measured                  object\n",
              "T3                           object\n",
              "TT4 measured                 object\n",
              "TT4                          object\n",
              "T4U measured                 object\n",
              "T4U                          object\n",
              "FTI measured                 object\n",
              "FTI                          object\n",
              "TBG measured                 object\n",
              "referral source              object\n",
              "binaryClass                  object\n",
              "dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "Data_Thyroid.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "pKLK0YJXIbWe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1cc1815c-1304-438d-bba6-ddd2339c63a6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['age', 'sex', 'on thyroxine', 'query on thyroxine',\n",
              "       'on antithyroid medication', 'sick', 'pregnant', 'thyroid surgery',\n",
              "       'I131 treatment', 'query hypothyroid', 'query hyperthyroid', 'lithium',\n",
              "       'goitre', 'tumor', 'hypopituitary', 'psych', 'TSH measured', 'TSH',\n",
              "       'T3 measured', 'T3', 'TT4 measured', 'TT4', 'T4U measured', 'T4U',\n",
              "       'FTI measured', 'FTI', 'TBG measured', 'referral source',\n",
              "       'binaryClass'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "Data_Thyroid.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "ZYy4MI3GH4a0"
      },
      "outputs": [],
      "source": [
        "# to label_encode the categorical data\n",
        "from sklearn import preprocessing\n",
        "# for converting the categorical data into numerical form\n",
        "label_encoder = preprocessing.LabelEncoder()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "3wovbHSIHtc8"
      },
      "outputs": [],
      "source": [
        "# label_encoding of the column age \n",
        "Data_Thyroid['age']= label_encoder.fit_transform(Data_Thyroid['age']) \n",
        "# to encode the value of sex\n",
        "Data_Thyroid['sex']= label_encoder.fit_transform(Data_Thyroid['sex']) \n",
        "# to encode the value of thyroxine\n",
        "Data_Thyroid['on thyroxine']= label_encoder.fit_transform(Data_Thyroid['on thyroxine']) \n",
        "# label_encoding of the column on thyroxine\n",
        "Data_Thyroid['query on thyroxine']= label_encoder.fit_transform(Data_Thyroid['query on thyroxine']) \n",
        "# to encode the value of on antithyroid medication\n",
        "Data_Thyroid['on antithyroid medication']= label_encoder.fit_transform(Data_Thyroid['on antithyroid medication']) \n",
        "# label_encoding of the column pregnant\n",
        "Data_Thyroid['pregnant']= label_encoder.fit_transform(Data_Thyroid['pregnant']) \n",
        "# to encode the value of on sick\n",
        "Data_Thyroid['sick']= label_encoder.fit_transform(Data_Thyroid['sick']) \n",
        "# label_encoding of the column query thyroid\n",
        "Data_Thyroid['query hyperthyroid'] = label_encoder.fit_transform(Data_Thyroid['query hyperthyroid'])\n",
        "# to encode the value of on thyroid surgery\n",
        "Data_Thyroid['thyroid surgery']= label_encoder.fit_transform(Data_Thyroid['thyroid surgery']) \n",
        "# label_encoding of the column I131 treatment\n",
        "Data_Thyroid['I131 treatment']= label_encoder.fit_transform(Data_Thyroid['I131 treatment']) \n",
        "# to encode the value of query hypothyroid\n",
        "Data_Thyroid['query hypothyroid']= label_encoder.fit_transform(Data_Thyroid['query hypothyroid']) \n",
        "# label_encoding of the column lithium\n",
        "Data_Thyroid['lithium']= label_encoder.fit_transform(Data_Thyroid['lithium']) \n",
        "# to encode the value of goitre \n",
        "Data_Thyroid['goitre']= label_encoder.fit_transform(Data_Thyroid['goitre']) \n",
        "# label_encoding of the column tumor \n",
        "Data_Thyroid['tumor']= label_encoder.fit_transform(Data_Thyroid['tumor']) \n",
        "# to encode the value of hypopituitary\n",
        "Data_Thyroid['hypopituitary']= label_encoder.fit_transform(Data_Thyroid['hypopituitary']) \n",
        "# label_encoding of the column psych\n",
        "Data_Thyroid['psych']= label_encoder.fit_transform(Data_Thyroid['psych']) \n",
        "# to encode the value of TSH measured \n",
        "Data_Thyroid['TSH measured']= label_encoder.fit_transform(Data_Thyroid['TSH measured']) \n",
        "# label_encoding of the column TSH 738\n",
        "Data_Thyroid['TSH']= label_encoder.fit_transform(Data_Thyroid['TSH']) \n",
        "# to encode the value of T3 measured \n",
        "Data_Thyroid['T3 measured']= label_encoder.fit_transform(Data_Thyroid['T3 measured']) \n",
        "# label_encoding of the column T3\n",
        "Data_Thyroid['T3']= label_encoder.fit_transform(Data_Thyroid['T3']) \n",
        "# to encode the value of TT4 measured \n",
        "Data_Thyroid['TT4 measured']= label_encoder.fit_transform(Data_Thyroid['TT4 measured']) \n",
        "# label_encoding of the column TT4\n",
        "Data_Thyroid['TT4']= label_encoder.fit_transform(Data_Thyroid['TT4']) \n",
        "# to encode the value of T4U measured\n",
        "Data_Thyroid['T4U measured']= label_encoder.fit_transform(Data_Thyroid['T4U measured']) \n",
        "# label_encoding of the column T4U\n",
        "Data_Thyroid['T4U']= label_encoder.fit_transform(Data_Thyroid['T4U']) \n",
        "# to encode the value of FTI measured \n",
        "Data_Thyroid['FTI measured']= label_encoder.fit_transform(Data_Thyroid['FTI measured']) \n",
        "# label_encoding of the column FTI\n",
        "Data_Thyroid['FTI']= label_encoder.fit_transform(Data_Thyroid['FTI']) \n",
        "# to encode the value of TBG measured\n",
        "Data_Thyroid['TBG measured']= label_encoder.fit_transform(Data_Thyroid['TBG measured']) \n",
        "# label_encoding of the column refferal source \n",
        "Data_Thyroid['referral source']= label_encoder.fit_transform(Data_Thyroid['referral source']) \n",
        "# to encode the value of binary class \n",
        "Data_Thyroid['binaryClass']= label_encoder.fit_transform(Data_Thyroid['binaryClass'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "oyiy3NlRIbSc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305
        },
        "outputId": "60d12dfc-b3c3-45c1-9277-7974522ba6f0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   age  sex  on thyroxine  query on thyroxine  on antithyroid medication  \\\n",
              "0   34    0             0                   0                          0   \n",
              "1   15    0             0                   0                          0   \n",
              "2   40    1             0                   0                          0   \n",
              "3   67    0             1                   0                          0   \n",
              "4   67    0             0                   0                          0   \n",
              "\n",
              "   sick  pregnant  thyroid surgery  I131 treatment  query hypothyroid  \\\n",
              "0     0         0                0               0                  0   \n",
              "1     0         0                0               0                  0   \n",
              "2     0         0                0               0                  0   \n",
              "3     0         0                0               0                  0   \n",
              "4     0         0                0               0                  0   \n",
              "\n",
              "   query hyperthyroid  lithium  goitre  tumor  hypopituitary  psych  \\\n",
              "0                   0        0       0      0              0      0   \n",
              "1                   0        0       0      0              0      0   \n",
              "2                   0        0       0      0              0      0   \n",
              "3                   0        0       0      0              0      0   \n",
              "4                   0        0       0      0              0      0   \n",
              "\n",
              "   TSH measured  TSH  T3 measured  T3  TT4 measured  TT4  T4U measured  T4U  \\\n",
              "0             1  108            1  27             1   28             1   69   \n",
              "1             1  186            1  22             1    3             0  140   \n",
              "2             1  102            0  67             1   10             1   45   \n",
              "3             1   21            1  20             1   82             0  140   \n",
              "4             1   76            1  12             1  190             1   41   \n",
              "\n",
              "   FTI measured  FTI  TBG measured  referral source  binaryClass  \n",
              "0             1   10             0                1            1  \n",
              "1             0  217             0                4            1  \n",
              "2             1   22             0                4            1  \n",
              "3             0  217             0                4            1  \n",
              "4             1  185             0                3            1  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6135f7ae-9ef0-4794-b5ce-35961d639a85\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>sex</th>\n",
              "      <th>on thyroxine</th>\n",
              "      <th>query on thyroxine</th>\n",
              "      <th>on antithyroid medication</th>\n",
              "      <th>sick</th>\n",
              "      <th>pregnant</th>\n",
              "      <th>thyroid surgery</th>\n",
              "      <th>I131 treatment</th>\n",
              "      <th>query hypothyroid</th>\n",
              "      <th>query hyperthyroid</th>\n",
              "      <th>lithium</th>\n",
              "      <th>goitre</th>\n",
              "      <th>tumor</th>\n",
              "      <th>hypopituitary</th>\n",
              "      <th>psych</th>\n",
              "      <th>TSH measured</th>\n",
              "      <th>TSH</th>\n",
              "      <th>T3 measured</th>\n",
              "      <th>T3</th>\n",
              "      <th>TT4 measured</th>\n",
              "      <th>TT4</th>\n",
              "      <th>T4U measured</th>\n",
              "      <th>T4U</th>\n",
              "      <th>FTI measured</th>\n",
              "      <th>FTI</th>\n",
              "      <th>TBG measured</th>\n",
              "      <th>referral source</th>\n",
              "      <th>binaryClass</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>34</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>108</td>\n",
              "      <td>1</td>\n",
              "      <td>27</td>\n",
              "      <td>1</td>\n",
              "      <td>28</td>\n",
              "      <td>1</td>\n",
              "      <td>69</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>15</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>186</td>\n",
              "      <td>1</td>\n",
              "      <td>22</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>140</td>\n",
              "      <td>0</td>\n",
              "      <td>217</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>40</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>102</td>\n",
              "      <td>0</td>\n",
              "      <td>67</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>1</td>\n",
              "      <td>45</td>\n",
              "      <td>1</td>\n",
              "      <td>22</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>67</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>21</td>\n",
              "      <td>1</td>\n",
              "      <td>20</td>\n",
              "      <td>1</td>\n",
              "      <td>82</td>\n",
              "      <td>0</td>\n",
              "      <td>140</td>\n",
              "      <td>0</td>\n",
              "      <td>217</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>67</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>76</td>\n",
              "      <td>1</td>\n",
              "      <td>12</td>\n",
              "      <td>1</td>\n",
              "      <td>190</td>\n",
              "      <td>1</td>\n",
              "      <td>41</td>\n",
              "      <td>1</td>\n",
              "      <td>185</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6135f7ae-9ef0-4794-b5ce-35961d639a85')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-6135f7ae-9ef0-4794-b5ce-35961d639a85 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-6135f7ae-9ef0-4794-b5ce-35961d639a85');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "Data_Thyroid.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "wgQ5lD9PIbOT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1179e3c2-008f-464c-afd9-65c5cf202d51"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method DataFrame.nunique of       age  sex  on thyroxine  query on thyroxine  on antithyroid medication  \\\n",
              "0      34    0             0                   0                          0   \n",
              "1      15    0             0                   0                          0   \n",
              "2      40    1             0                   0                          0   \n",
              "3      67    0             1                   0                          0   \n",
              "4      67    0             0                   0                          0   \n",
              "...   ...  ...           ...                 ...                        ...   \n",
              "3158   17    1             0                   0                          0   \n",
              "3159   68    0             1                   0                          0   \n",
              "3160   29    0             0                   0                          0   \n",
              "3161   51    0             0                   0                          0   \n",
              "3162   73    0             0                   0                          0   \n",
              "\n",
              "      sick  pregnant  thyroid surgery  I131 treatment  query hypothyroid  \\\n",
              "0        0         0                0               0                  0   \n",
              "1        0         0                0               0                  0   \n",
              "2        0         0                0               0                  0   \n",
              "3        0         0                0               0                  0   \n",
              "4        0         0                0               0                  0   \n",
              "...    ...       ...              ...             ...                ...   \n",
              "3158     0         0                0               0                  0   \n",
              "3159     0         0                0               0                  0   \n",
              "3160     0         0                0               0                  0   \n",
              "3161     0         0                0               0                  0   \n",
              "3162     1         0                1               0                  0   \n",
              "\n",
              "      query hyperthyroid  lithium  goitre  tumor  hypopituitary  psych  \\\n",
              "0                      0        0       0      0              0      0   \n",
              "1                      0        0       0      0              0      0   \n",
              "2                      0        0       0      0              0      0   \n",
              "3                      0        0       0      0              0      0   \n",
              "4                      0        0       0      0              0      0   \n",
              "...                  ...      ...     ...    ...            ...    ...   \n",
              "3158                   0        0       0      0              0      1   \n",
              "3159                   0        0       0      0              0      0   \n",
              "3160                   0        0       0      0              0      0   \n",
              "3161                   0        0       0      0              0      0   \n",
              "3162                   0        0       0      0              0      0   \n",
              "\n",
              "      TSH measured  TSH  T3 measured  T3  TT4 measured  TT4  T4U measured  \\\n",
              "0                1  108            1  27             1   28             1   \n",
              "1                1  186            1  22             1    3             0   \n",
              "2                1  102            0  67             1   10             1   \n",
              "3                1   21            1  20             1   82             0   \n",
              "4                1   76            1  12             1  190             1   \n",
              "...            ...  ...          ...  ..           ...  ...           ...   \n",
              "3158             1  111            1  23             1  218             1   \n",
              "3159             1  153            1  12             1    8             1   \n",
              "3160             1  206            1  27             1  213             1   \n",
              "3161             1  241            1  12             1  202             1   \n",
              "3162             1  113            1  12             1  194             1   \n",
              "\n",
              "      T4U  FTI measured  FTI  TBG measured  referral source  binaryClass  \n",
              "0      69             1   10             0                1            1  \n",
              "1     140             0  217             0                4            1  \n",
              "2      45             1   22             0                4            1  \n",
              "3     140             0  217             0                4            1  \n",
              "4      41             1  185             0                3            1  \n",
              "...   ...           ...  ...           ...              ...          ...  \n",
              "3158   40             1    4             0                1            1  \n",
              "3159   65             1  214             0                4            1  \n",
              "3160   60             1  196             0                1            1  \n",
              "3161   34             1  208             0                3            0  \n",
              "3162   30             1  202             0                4            1  \n",
              "\n",
              "[3163 rows x 29 columns]>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "Data_Thyroid.nunique"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "1SH9yukmD8Uz"
      },
      "outputs": [],
      "source": [
        "#filling the null data from the different columns \n",
        "Data_Thyroid['sex'].fillna(Data_Thyroid['sex'].mean(), inplace=True)\n",
        "# filling the missing value of Sex columns \n",
        "Data_Thyroid['TSH'].fillna(Data_Thyroid['TSH'].mean(), inplace=True)\n",
        "# to fill the missing value of TSH columns \n",
        "Data_Thyroid['T3'].fillna(Data_Thyroid['T3'].mean(), inplace=True)\n",
        "# filling the missing value of T3 columns \n",
        "Data_Thyroid['TT4'].fillna(Data_Thyroid['TT4'].mean(), inplace=True)\n",
        "# to fill the missing value of TT4 columns \n",
        "Data_Thyroid['T4U'].fillna(Data_Thyroid['T4U'].mean(), inplace=True)\n",
        "# filling the missing value of T4U columns \n",
        "Data_Thyroid['FTI'].fillna(Data_Thyroid['FTI'].mean(), inplace=True)\n",
        "# to fill the missing value of FTI columns "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "w5_EzUuxHtTP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ea8f99e-c472-4f21-8b92-2689ba4bca95"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "age                          0\n",
              "sex                          0\n",
              "on thyroxine                 0\n",
              "query on thyroxine           0\n",
              "on antithyroid medication    0\n",
              "sick                         0\n",
              "pregnant                     0\n",
              "thyroid surgery              0\n",
              "I131 treatment               0\n",
              "query hypothyroid            0\n",
              "query hyperthyroid           0\n",
              "lithium                      0\n",
              "goitre                       0\n",
              "tumor                        0\n",
              "hypopituitary                0\n",
              "psych                        0\n",
              "TSH measured                 0\n",
              "TSH                          0\n",
              "T3 measured                  0\n",
              "T3                           0\n",
              "TT4 measured                 0\n",
              "TT4                          0\n",
              "T4U measured                 0\n",
              "T4U                          0\n",
              "FTI measured                 0\n",
              "FTI                          0\n",
              "TBG measured                 0\n",
              "referral source              0\n",
              "binaryClass                  0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "Data_Thyroid.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2D3XymZBGirj",
        "outputId": "b27edd73-7f3e-4e30-8232-dbad1fd79a55"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    2919\n",
              "0     244\n",
              "Name: binaryClass, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "Data_Thyroid[\"binaryClass\"].value_counts().T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "6wrkhNUpGioL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fd32702-d2fa-46ff-ff6a-2c5cd7132054"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    3163.000000\n",
              "mean        0.922858\n",
              "std         0.266859\n",
              "min         0.000000\n",
              "25%         1.000000\n",
              "50%         1.000000\n",
              "75%         1.000000\n",
              "max         1.000000\n",
              "Name: binaryClass, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "Data_Thyroid[\"binaryClass\"].describe().T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "1SFLvmdRGilZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 775
        },
        "outputId": "70249a27-6b92-4ead-9fab-5063201218bc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    age  sex  on thyroxine  query on thyroxine  on antithyroid medication  \\\n",
              "0    34    0             0                   0                          0   \n",
              "1    15    0             0                   0                          0   \n",
              "2    40    1             0                   0                          0   \n",
              "3    67    0             1                   0                          0   \n",
              "4    67    0             0                   0                          0   \n",
              "5     9    0             1                   0                          0   \n",
              "6    54    0             0                   0                          0   \n",
              "7    78    0             0                   0                          0   \n",
              "8    62    0             0                   0                          0   \n",
              "9    64    1             0                   0                          0   \n",
              "10   82    0             0                   0                          0   \n",
              "11   63    0             1                   0                          0   \n",
              "12   68    0             0                   0                          0   \n",
              "13   54    0             0                   0                          0   \n",
              "14   20    1             0                   0                          0   \n",
              "15   61    0             0                   0                          0   \n",
              "16   35    2             0                   0                          0   \n",
              "17   59    0             0                   0                          0   \n",
              "18   78    0             0                   0                          0   \n",
              "19   20    1             0                   0                          0   \n",
              "\n",
              "    sick  pregnant  thyroid surgery  I131 treatment  query hypothyroid  \\\n",
              "0      0         0                0               0                  0   \n",
              "1      0         0                0               0                  0   \n",
              "2      0         0                0               0                  0   \n",
              "3      0         0                0               0                  0   \n",
              "4      0         0                0               0                  0   \n",
              "5      0         0                0               0                  0   \n",
              "6      0         0                0               0                  0   \n",
              "7      0         0                0               0                  0   \n",
              "8      0         0                0               0                  0   \n",
              "9      0         0                0               0                  0   \n",
              "10     0         0                0               0                  0   \n",
              "11     0         0                0               0                  0   \n",
              "12     1         0                0               0                  0   \n",
              "13     0         0                0               0                  0   \n",
              "14     0         0                0               0                  0   \n",
              "15     0         0                0               0                  1   \n",
              "16     0         0                0               0                  0   \n",
              "17     0         0                0               0                  0   \n",
              "18     0         0                0               0                  0   \n",
              "19     0         0                0               0                  0   \n",
              "\n",
              "    query hyperthyroid  lithium  goitre  tumor  hypopituitary  psych  \\\n",
              "0                    0        0       0      0              0      0   \n",
              "1                    0        0       0      0              0      0   \n",
              "2                    0        0       0      0              0      0   \n",
              "3                    0        0       0      0              0      0   \n",
              "4                    0        0       0      0              0      0   \n",
              "5                    0        0       0      0              0      0   \n",
              "6                    0        0       0      0              0      0   \n",
              "7                    0        0       0      0              0      0   \n",
              "8                    0        0       0      1              0      0   \n",
              "9                    0        0       0      0              0      0   \n",
              "10                   0        0       0      1              0      0   \n",
              "11                   0        0       0      0              0      0   \n",
              "12                   1        0       0      0              0      0   \n",
              "13                   0        0       0      0              0      0   \n",
              "14                   0        0       0      0              0      0   \n",
              "15                   0        0       0      0              0      0   \n",
              "16                   0        0       0      0              0      0   \n",
              "17                   0        0       0      0              0      0   \n",
              "18                   0        0       0      0              0      1   \n",
              "19                   0        0       0      0              0      1   \n",
              "\n",
              "    TSH measured  TSH  T3 measured  T3  TT4 measured  TT4  T4U measured  T4U  \\\n",
              "0              1  108            1  27             1   28             1   69   \n",
              "1              1  186            1  22             1    3             0  140   \n",
              "2              1  102            0  67             1   10             1   45   \n",
              "3              1   21            1  20             1   82             0  140   \n",
              "4              1   76            1  12             1  190             1   41   \n",
              "5              1    5            0  67             1   91             1   85   \n",
              "6              0  270            0  67             1  201             1   46   \n",
              "7              1  148            1   6             1  209             1   24   \n",
              "8              1   64            1  24             1   26             1   47   \n",
              "9              1  150            1  17             1  212             1   43   \n",
              "10             1  106            1  24             1   17             1   50   \n",
              "11             1    5            0  67             1   58             1   54   \n",
              "12             1    5            1  40             1   78             1   68   \n",
              "13             1  154            1  18             1  227             1   45   \n",
              "14             1  169            1  19             1   10             1   45   \n",
              "15             1  124            0  67             1  229             1   69   \n",
              "16             1  107            1  19             1  199             1   40   \n",
              "17             1  110            1  12             1   19             1   51   \n",
              "18             1  220            1  17             1  229             1   50   \n",
              "19             1  147            1  28             1   24             1   48   \n",
              "\n",
              "    FTI measured  FTI  TBG measured  referral source  binaryClass  \n",
              "0              1   10             0                1            1  \n",
              "1              0  217             0                4            1  \n",
              "2              1   22             0                4            1  \n",
              "3              0  217             0                4            1  \n",
              "4              1  185             0                3            1  \n",
              "5              1   45             0                4            1  \n",
              "6              1  193             0                4            1  \n",
              "7              1   17             0                3            1  \n",
              "8              1   35             0                3            1  \n",
              "9              1  210             0                3            1  \n",
              "10             1   23             0                3            1  \n",
              "11             1   57             0                4            1  \n",
              "12             1   55             0                4            1  \n",
              "13             1    8             0                3            1  \n",
              "14             1   21             0                1            1  \n",
              "15             1  203             0                4            0  \n",
              "16             1  197             0                4            1  \n",
              "17             1   23             0                3            1  \n",
              "18             1    5             0                3            1  \n",
              "19             1   33             0                1            1  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-99c9fcc0-5e3c-4e19-a660-ddf948c73c51\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>sex</th>\n",
              "      <th>on thyroxine</th>\n",
              "      <th>query on thyroxine</th>\n",
              "      <th>on antithyroid medication</th>\n",
              "      <th>sick</th>\n",
              "      <th>pregnant</th>\n",
              "      <th>thyroid surgery</th>\n",
              "      <th>I131 treatment</th>\n",
              "      <th>query hypothyroid</th>\n",
              "      <th>query hyperthyroid</th>\n",
              "      <th>lithium</th>\n",
              "      <th>goitre</th>\n",
              "      <th>tumor</th>\n",
              "      <th>hypopituitary</th>\n",
              "      <th>psych</th>\n",
              "      <th>TSH measured</th>\n",
              "      <th>TSH</th>\n",
              "      <th>T3 measured</th>\n",
              "      <th>T3</th>\n",
              "      <th>TT4 measured</th>\n",
              "      <th>TT4</th>\n",
              "      <th>T4U measured</th>\n",
              "      <th>T4U</th>\n",
              "      <th>FTI measured</th>\n",
              "      <th>FTI</th>\n",
              "      <th>TBG measured</th>\n",
              "      <th>referral source</th>\n",
              "      <th>binaryClass</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>34</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>108</td>\n",
              "      <td>1</td>\n",
              "      <td>27</td>\n",
              "      <td>1</td>\n",
              "      <td>28</td>\n",
              "      <td>1</td>\n",
              "      <td>69</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>15</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>186</td>\n",
              "      <td>1</td>\n",
              "      <td>22</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>140</td>\n",
              "      <td>0</td>\n",
              "      <td>217</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>40</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>102</td>\n",
              "      <td>0</td>\n",
              "      <td>67</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>1</td>\n",
              "      <td>45</td>\n",
              "      <td>1</td>\n",
              "      <td>22</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>67</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>21</td>\n",
              "      <td>1</td>\n",
              "      <td>20</td>\n",
              "      <td>1</td>\n",
              "      <td>82</td>\n",
              "      <td>0</td>\n",
              "      <td>140</td>\n",
              "      <td>0</td>\n",
              "      <td>217</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>67</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>76</td>\n",
              "      <td>1</td>\n",
              "      <td>12</td>\n",
              "      <td>1</td>\n",
              "      <td>190</td>\n",
              "      <td>1</td>\n",
              "      <td>41</td>\n",
              "      <td>1</td>\n",
              "      <td>185</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>67</td>\n",
              "      <td>1</td>\n",
              "      <td>91</td>\n",
              "      <td>1</td>\n",
              "      <td>85</td>\n",
              "      <td>1</td>\n",
              "      <td>45</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>54</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>270</td>\n",
              "      <td>0</td>\n",
              "      <td>67</td>\n",
              "      <td>1</td>\n",
              "      <td>201</td>\n",
              "      <td>1</td>\n",
              "      <td>46</td>\n",
              "      <td>1</td>\n",
              "      <td>193</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>78</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>148</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>209</td>\n",
              "      <td>1</td>\n",
              "      <td>24</td>\n",
              "      <td>1</td>\n",
              "      <td>17</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>62</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>64</td>\n",
              "      <td>1</td>\n",
              "      <td>24</td>\n",
              "      <td>1</td>\n",
              "      <td>26</td>\n",
              "      <td>1</td>\n",
              "      <td>47</td>\n",
              "      <td>1</td>\n",
              "      <td>35</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>64</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>150</td>\n",
              "      <td>1</td>\n",
              "      <td>17</td>\n",
              "      <td>1</td>\n",
              "      <td>212</td>\n",
              "      <td>1</td>\n",
              "      <td>43</td>\n",
              "      <td>1</td>\n",
              "      <td>210</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>82</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>106</td>\n",
              "      <td>1</td>\n",
              "      <td>24</td>\n",
              "      <td>1</td>\n",
              "      <td>17</td>\n",
              "      <td>1</td>\n",
              "      <td>50</td>\n",
              "      <td>1</td>\n",
              "      <td>23</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>63</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>67</td>\n",
              "      <td>1</td>\n",
              "      <td>58</td>\n",
              "      <td>1</td>\n",
              "      <td>54</td>\n",
              "      <td>1</td>\n",
              "      <td>57</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>68</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>40</td>\n",
              "      <td>1</td>\n",
              "      <td>78</td>\n",
              "      <td>1</td>\n",
              "      <td>68</td>\n",
              "      <td>1</td>\n",
              "      <td>55</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>54</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>154</td>\n",
              "      <td>1</td>\n",
              "      <td>18</td>\n",
              "      <td>1</td>\n",
              "      <td>227</td>\n",
              "      <td>1</td>\n",
              "      <td>45</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>20</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>169</td>\n",
              "      <td>1</td>\n",
              "      <td>19</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>1</td>\n",
              "      <td>45</td>\n",
              "      <td>1</td>\n",
              "      <td>21</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>61</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>124</td>\n",
              "      <td>0</td>\n",
              "      <td>67</td>\n",
              "      <td>1</td>\n",
              "      <td>229</td>\n",
              "      <td>1</td>\n",
              "      <td>69</td>\n",
              "      <td>1</td>\n",
              "      <td>203</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>35</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>107</td>\n",
              "      <td>1</td>\n",
              "      <td>19</td>\n",
              "      <td>1</td>\n",
              "      <td>199</td>\n",
              "      <td>1</td>\n",
              "      <td>40</td>\n",
              "      <td>1</td>\n",
              "      <td>197</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>59</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>110</td>\n",
              "      <td>1</td>\n",
              "      <td>12</td>\n",
              "      <td>1</td>\n",
              "      <td>19</td>\n",
              "      <td>1</td>\n",
              "      <td>51</td>\n",
              "      <td>1</td>\n",
              "      <td>23</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>78</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>220</td>\n",
              "      <td>1</td>\n",
              "      <td>17</td>\n",
              "      <td>1</td>\n",
              "      <td>229</td>\n",
              "      <td>1</td>\n",
              "      <td>50</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>20</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>147</td>\n",
              "      <td>1</td>\n",
              "      <td>28</td>\n",
              "      <td>1</td>\n",
              "      <td>24</td>\n",
              "      <td>1</td>\n",
              "      <td>48</td>\n",
              "      <td>1</td>\n",
              "      <td>33</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-99c9fcc0-5e3c-4e19-a660-ddf948c73c51')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-99c9fcc0-5e3c-4e19-a660-ddf948c73c51 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-99c9fcc0-5e3c-4e19-a660-ddf948c73c51');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "Data_Thyroid.head(20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfBk_DlASMOd"
      },
      "source": [
        "# **splitting the dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "M5hTwcKLGiiR"
      },
      "outputs": [],
      "source": [
        "#Splitting the database into supporting or non supporting variable \n",
        "x=Data_Thyroid.iloc[:,:-1]\n",
        "y= Data_Thyroid['binaryClass']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jk21nL2EGifR",
        "outputId": "19fbf744-21b1-4c5a-cb9c-0a6d6890b822"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3163, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "x.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_F-HQPo1GicR",
        "outputId": "cbf3ad94-e5cc-4a72-c352-e45adf5c4746"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3163,)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "y.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5qDuGK8m8g0"
      },
      "source": [
        "# **FEATURE SELECTION**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "9MW90f-KoWvr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94def110-fd35-4fe5-ad8c-400252acc255"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting skfeature-chappers\n",
            "  Downloading skfeature_chappers-1.1.0-py3-none-any.whl (66 kB)\n",
            "\u001b[?25l\r\u001b[K     |                           | 10 kB 28.9 MB/s eta 0:00:01\r\u001b[K     |                      | 20 kB 36.6 MB/s eta 0:00:01\r\u001b[K     |                 | 30 kB 46.6 MB/s eta 0:00:01\r\u001b[K     |            | 40 kB 22.3 MB/s eta 0:00:01\r\u001b[K     |       | 51 kB 24.0 MB/s eta 0:00:01\r\u001b[K     |  | 61 kB 27.1 MB/s eta 0:00:01\r\u001b[K     || 66 kB 3.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from skfeature-chappers) (1.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from skfeature-chappers) (1.3.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from skfeature-chappers) (1.21.6)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->skfeature-chappers) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->skfeature-chappers) (2022.2.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->skfeature-chappers) (1.15.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->skfeature-chappers) (1.1.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->skfeature-chappers) (1.7.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->skfeature-chappers) (3.1.0)\n",
            "Installing collected packages: skfeature-chappers\n",
            "Successfully installed skfeature-chappers-1.1.0\n"
          ]
        }
      ],
      "source": [
        "!pip install skfeature-chappers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "zaQ0fZvCGiZR"
      },
      "outputs": [],
      "source": [
        "# importing feature selection library\n",
        "from skfeature.function.information_theoretical_based import CMIM\n",
        "# importing the library of mutual info classifier\n",
        "from sklearn.feature_selection import mutual_info_classif"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "M5wMQF6QGiWJ"
      },
      "outputs": [],
      "source": [
        "# initializing the  feature selection technique\n",
        "features_cal_cmim = CMIM.cmim(x.values,y.values, n_selected_features=44)\n",
        "# performing feature selection \n",
        "ext_fcmim = Data_Thyroid.columns[features_cal_cmim]\n",
        "# to extract the most important feature\n",
        "x = Data_Thyroid[ext_fcmim]\n",
        "#to target the independent feature \n",
        "y = Data_Thyroid['binaryClass']\n",
        "# to target dependent column\n",
        "# draw out the x , y data \n",
        "importance = mutual_info_classif(x, y)\n",
        "#identifying dependency \n",
        "ig_importance = pd.Series(importance, index=x.columns)\n",
        "#examine the dependency"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 960
        },
        "id": "pv1uYpxtGiTJ",
        "outputId": "e44b0904-1c39-49ea-9e1a-8e8f0ca8716d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1080x1152 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# acquiring comparison chart\n",
        "fig,ax = plt.subplots()\n",
        "#acquiring different sub plots \n",
        "fig.set_size_inches(15,16)\n",
        "#fixing its size\n",
        "ax.barh(ig_importance.sort_values().index, ig_importance.sort_values().values)\n",
        "#fixing bar graph data \n",
        "plt.xticks(rotation=90)\n",
        "#to rotate the x axis labels by Xticks\n",
        "plt.ylabel('Features')\n",
        "plt.xlabel('Dependency')\n",
        "plt.title('Information Gain for each feature')\n",
        "#defining title name\n",
        "plt.show()#to draw the bar graph "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LO7QLj8_GiPh",
        "outputId": "1305eb67-bcaf-45c4-e165-173bc8a63c72"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['on thyroxine', 'on antithyroid medication', 'referral source', 'age',\n",
              "       'FTI', 'TT4', 'TSH', 'T3', 'TSH measured', 'psych', 'sick',\n",
              "       'query hypothyroid', 'lithium', 'query on thyroxine',\n",
              "       'thyroid surgery'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "t = pd.DataFrame(ig_importance.sort_values(), columns=['columns_name']).reset_index()#calling dataframe from pandas and converting feature importance in dataframe and sorting all values  and reseting index by function\n",
        "#making Data_Thyroid of important attribute\n",
        "p = t['index'].tolist()#changing in list by using tolist function\n",
        "# to list the index \n",
        "x = x.drop(p[0:13], axis=1)\n",
        "# to drop the  columns\n",
        "x.columns# to visualize columns data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "Rw6cNTJaMOln"
      },
      "outputs": [],
      "source": [
        "# splitting the arrival database into fitting and testing \n",
        "from sklearn.model_selection import train_test_split\n",
        "# assigning the attributes \n",
        "Thyroid_X_train, Thyroid_X_test, Thyroid_Y_train, Thyroid_Y_test = train_test_split(x,y, test_size=0.3, random_state =42,  stratify=y ,shuffle=True) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jrcSO93rMOiH",
        "outputId": "7cc3f469-76dc-40e1-8777-804e617b0e94"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2214, 15)"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "Thyroid_X_train.shape \n",
        "# to get the shape of xtrain variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V3dsTRz5MOe7",
        "outputId": "7a4027cb-650a-4deb-a8c0-a7ff6ef034cf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2214,)"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "Thyroid_Y_train.shape\n",
        "# to get the shape of ytrain variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f-OZKo37MObk",
        "outputId": "307ff5df-20f7-4474-b070-5ae9872463b1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(949, 15)"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "Thyroid_X_test.shape\n",
        "# to get the shape of xtest variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AdaOrDIQMOX3",
        "outputId": "45360b53-e740-482c-c0b2-8de2810dff19"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(949,)"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ],
      "source": [
        "Thyroid_Y_test.shape\n",
        "# to get the shape of ytest variable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIhWnmlDmNMU"
      },
      "source": [
        "# **KNN**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j1AYV5qXMOVQ",
        "outputId": "b5d61899-a49b-4118-8957-ff4bff223d56"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9272918861959958"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "source": [
        "# importing the algorithm\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "# initializing the algorithm \n",
        "modelk2 = KNeighborsClassifier(n_neighbors=2)\n",
        "# to trained the algorithm\n",
        "modelk2.fit(Thyroid_X_train,Thyroid_Y_train)\n",
        "# to get the accuracy \n",
        "modelk2.score(Thyroid_X_test, Thyroid_Y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "T35TrYt1MOSD"
      },
      "outputs": [],
      "source": [
        "# importing cr , cm \n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "# initialising the prediction variable\n",
        "pred1 = modelk2.predict(Thyroid_X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "9zmSncKUMOPJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04bcee64-e73d-4498-a2c8-f1703ed6dc5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.52      0.68      0.59        73\n",
            "           1       0.97      0.95      0.96       876\n",
            "\n",
            "    accuracy                           0.93       949\n",
            "   macro avg       0.75      0.82      0.78       949\n",
            "weighted avg       0.94      0.93      0.93       949\n",
            "\n",
            "\n",
            "Accuracy:  0.9272918861959958\n"
          ]
        }
      ],
      "source": [
        "print(classification_report(Thyroid_Y_test, pred1)) \n",
        "print()\n",
        "# reprinting the performance \n",
        "print('Accuracy: ', accuracy_score(Thyroid_Y_test, pred1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r2N345KJMOMN",
        "outputId": "4a7df79c-6001-458e-f557-0d929aa646d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision by knn of testing data is: 0.927\n",
            "Recall by knn of testing data is: 0.927\n",
            "F1 score by knn of testing data is: 0.927\n"
          ]
        }
      ],
      "source": [
        "# various libraries for report\n",
        "from sklearn.metrics import precision_score,recall_score,f1_score\n",
        "# printing the performance \n",
        "print('Precision by knn of testing data is: %.3f' % precision_score(Thyroid_Y_test, pred1,average='micro')) \n",
        "# checking the precision value\n",
        "print('Recall by knn of testing data is: %.3f' % recall_score(Thyroid_Y_test, pred1,average='micro')) \n",
        "# checking the recall value\n",
        "print('F1 score by knn of testing data is: %.3f' % f1_score(Thyroid_Y_test, pred1,average='micro')) \n",
        "# checking the f2 score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ENVvE0WMOJU",
        "outputId": "3a37be6e-3a59-4ac5-cb4a-353bdcd3e986"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 50,  23],\n",
              "       [ 46, 830]])"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ],
      "source": [
        "# importing c_m library\n",
        "from sklearn.metrics import confusion_matrix\n",
        "# initializing c_m\n",
        "cm = confusion_matrix(Thyroid_Y_test,pred1)\n",
        "cm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "XbRdPB-ZMOGD",
        "outputId": "b67c2f09-fc1a-4790-cca4-1c4e15b07cf9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(33.0, 0.5, 'Original Values')"
            ]
          },
          "metadata": {},
          "execution_count": 48
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# making heat map \n",
        "import seaborn as sns\n",
        "# assigning the color \n",
        "sns.heatmap(cm,annot=True, fmt='d' , cmap='Pastel1_r')\n",
        "# assigning the x-axis label\n",
        "plt.xlabel('Predicted values')\n",
        "# assigning the y-axis label\n",
        "plt.ylabel('Original Values')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MeR3za9xMODj",
        "outputId": "b0206f8b-41f9-4f2e-90bd-9b145e969975"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sensitivity :  0.9474885844748858\n",
            "Specificity :  0.684931506849315\n"
          ]
        }
      ],
      "source": [
        "# printing Sensitivity accuracy \n",
        "sensitivity = cm[1,1]/(cm[1,1]+cm[1,0])\n",
        "# to print the senstivity score \n",
        "print('Sensitivity : ', sensitivity)\n",
        "# printing Specificity accuracy \n",
        "specificity = cm[0,0]/(cm[0,1]+cm[0,0])\n",
        "# to print the score of specificity\n",
        "print('Specificity : ', specificity)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WeVnAHo8eEZ_",
        "outputId": "516fa491-1f9f-4967-afce-d41d469196ea"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9399367755532139"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ],
      "source": [
        "# importing the algorithm\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "# initislizing the algorithm\n",
        "modelk10 = KNeighborsClassifier(n_neighbors=10)\n",
        "# to trained the algorithm\n",
        "modelk10.fit(Thyroid_X_train,Thyroid_Y_train)\n",
        "# to print the score of the algorithm\n",
        "modelk10.score(Thyroid_X_test, Thyroid_Y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "wq0YZa9XfUoJ"
      },
      "outputs": [],
      "source": [
        "# importing cr , cm \n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "# initialising the prediction variable\n",
        "pred1 = modelk10.predict(Thyroid_X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "xo0AQFg0fUkn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb5f3223-a44b-4653-ad3f-bf816f71e54d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.69      0.40      0.50        73\n",
            "           1       0.95      0.99      0.97       876\n",
            "\n",
            "    accuracy                           0.94       949\n",
            "   macro avg       0.82      0.69      0.74       949\n",
            "weighted avg       0.93      0.94      0.93       949\n",
            "\n",
            "\n",
            "Accuracy:  0.9399367755532139\n"
          ]
        }
      ],
      "source": [
        "# to print the c_r\n",
        "print(classification_report(Thyroid_Y_test, pred1)) \n",
        "print()\n",
        "# reprinting the performance \n",
        "print('Accuracy: ', accuracy_score(Thyroid_Y_test, pred1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XwtQ4uSrfUbl",
        "outputId": "7b01ac26-6e33-4a5a-bb77-fc63cda4b926"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 29,  44],\n",
              "       [ 13, 863]])"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ],
      "source": [
        "# importing c_m\n",
        "from sklearn.metrics import confusion_matrix\n",
        "# initializing cm\n",
        "cm = confusion_matrix(Thyroid_Y_test,pred1)\n",
        "cm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5ByI7u-fUYJ",
        "outputId": "d0d44916-1aae-4277-826d-f72e09f02075"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sensitivity :  0.9851598173515982\n",
            "Specificity :  0.3972602739726027\n"
          ]
        }
      ],
      "source": [
        "# printing Sensitivity accuracy \n",
        "sensitivity = cm[1,1]/(cm[1,1]+cm[1,0])\n",
        "# to print the score of sentivity score\n",
        "print('Sensitivity : ', sensitivity)\n",
        "# printing Specificity accuracy \n",
        "specificity = cm[0,0]/(cm[0,1]+cm[0,0])\n",
        "# to print the specificity score\n",
        "print('Specificity : ', specificity)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6OZUPQ0neEWm",
        "outputId": "1771d4d6-b303-4891-b172-21b68f263f70"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9441517386722866"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ],
      "source": [
        "# importing the algorithm\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "# initializing the classifier\n",
        "modelk20 = KNeighborsClassifier(n_neighbors=20)\n",
        "# to trained the algorithm\n",
        "modelk20.fit(Thyroid_X_train,Thyroid_Y_train)\n",
        "# to get the score of the algorithm\n",
        "modelk20.score(Thyroid_X_test, Thyroid_Y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "iC0w0IkyfqL6"
      },
      "outputs": [],
      "source": [
        "# importing cr , cm \n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "# initialising the prediction variable\n",
        "pred1 = modelk20.predict(Thyroid_X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2jbCr6Z6fqCP",
        "outputId": "a86f634e-f317-4db4-ed62-339daa59fcd0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.37      0.50        73\n",
            "           1       0.95      0.99      0.97       876\n",
            "\n",
            "    accuracy                           0.94       949\n",
            "   macro avg       0.87      0.68      0.74       949\n",
            "weighted avg       0.94      0.94      0.93       949\n",
            "\n",
            "\n",
            "Accuracy:  0.9441517386722866\n"
          ]
        }
      ],
      "source": [
        "# to draw the c_r\n",
        "print(classification_report(Thyroid_Y_test, pred1)) \n",
        "print()\n",
        "# reprinting the performance \n",
        "print('Accuracy: ', accuracy_score(Thyroid_Y_test, pred1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dUZwJ0JGfp-u",
        "outputId": "bafb8bc6-af10-4975-8bd0-d9301c39a9b4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 27,  46],\n",
              "       [  7, 869]])"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ],
      "source": [
        "# importing c_m\n",
        "from sklearn.metrics import confusion_matrix\n",
        "# initializing cm\n",
        "cm = confusion_matrix(Thyroid_Y_test,pred1)\n",
        "cm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H4yU5e9Ofp70",
        "outputId": "5616977a-6d19-48e1-dc26-56d6389e64db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sensitivity :  0.9920091324200914\n",
            "Specificity :  0.3698630136986301\n"
          ]
        }
      ],
      "source": [
        "# printing Sensitivity accuracy \n",
        "sensitivity = cm[1,1]/(cm[1,1]+cm[1,0])\n",
        "# to get the score of senstivity\n",
        "print('Sensitivity : ', sensitivity)\n",
        "# printing Specificity accuracy \n",
        "specificity = cm[0,0]/(cm[0,1]+cm[0,0])\n",
        "# to get the score of specificity\n",
        "print('Specificity : ', specificity)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mMPcVeyVeETR",
        "outputId": "9ff74bda-99c1-4907-edd0-4e7eaf8d794d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9399367755532139"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ],
      "source": [
        "# importing the classifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "# initializing the classifier\n",
        "modelk25 = KNeighborsClassifier(n_neighbors=25)\n",
        "# to trained the algorithm\n",
        "modelk25.fit(Thyroid_X_train,Thyroid_Y_train)\n",
        "# to get the accuracy of the algorithm\n",
        "modelk25.score(Thyroid_X_test, Thyroid_Y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "ggOKR9utf4rf"
      },
      "outputs": [],
      "source": [
        "# importing cr , cm \n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "# initialising the prediction variable\n",
        "pred1 = modelk25.predict(Thyroid_X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "quet6u6Ef4oG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fc8ec92-504f-4ca5-8bf5-2b04caa6a586"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.30      0.44        73\n",
            "           1       0.94      0.99      0.97       876\n",
            "\n",
            "    accuracy                           0.94       949\n",
            "   macro avg       0.87      0.65      0.70       949\n",
            "weighted avg       0.93      0.94      0.93       949\n",
            "\n",
            "\n",
            "Accuracy:  0.9399367755532139\n"
          ]
        }
      ],
      "source": [
        "# to print the c_r\n",
        "print(classification_report(Thyroid_Y_test, pred1)) \n",
        "print()\n",
        "# reprinting the performance \n",
        "print('Accuracy: ', accuracy_score(Thyroid_Y_test, pred1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6-KkVneQf4lJ",
        "outputId": "67174237-5b1b-4804-d872-d67e9f959683"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 22,  51],\n",
              "       [  6, 870]])"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ],
      "source": [
        "# importing c_m \n",
        "from sklearn.metrics import confusion_matrix\n",
        "# initializing the c_m\n",
        "cm = confusion_matrix(Thyroid_Y_test,pred1)\n",
        "cm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eXY5vLsqf4Vz",
        "outputId": "b775bf64-9282-4510-af7c-4a5ddbca19bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sensitivity :  0.9931506849315068\n",
            "Specificity :  0.3013698630136986\n"
          ]
        }
      ],
      "source": [
        "# printing Sensitivity accuracy \n",
        "sensitivity = cm[1,1]/(cm[1,1]+cm[1,0])\n",
        "# to get the score of senstivity score\n",
        "print('Sensitivity : ', sensitivity)\n",
        "# printing Specificity accuracy \n",
        "specificity = cm[0,0]/(cm[0,1]+cm[0,0])\n",
        "# to get the specificity score\n",
        "print('Specificity : ', specificity)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "at7HVOSRQbOE"
      },
      "source": [
        "# **RandomForestClassifier**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lKKpRwP9P10H",
        "outputId": "ee5aba38-27fc-4d6e-d0b6-8252e27ab8e2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9747102212855637"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ],
      "source": [
        "# to import the algorithm\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "# initialize the algo\n",
        "model_RF = RandomForestClassifier(max_depth=10, n_estimators=8 )\n",
        "# to trained the algorithm\n",
        "model_RF.fit(Thyroid_X_train,Thyroid_Y_train)\n",
        "# to get the score of the algorithm\n",
        "model_RF.score(Thyroid_X_test,Thyroid_Y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "OoF0QbKyP1xJ"
      },
      "outputs": [],
      "source": [
        "# importing cr , cm \n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "# initialising the predict variable\n",
        "pred2 = model_RF.predict(Thyroid_X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "quBqZkYVP1uZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41eb1e3a-f05f-4b29-f320-f4fedfd700db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.71      0.81        73\n",
            "           1       0.98      1.00      0.99       876\n",
            "\n",
            "    accuracy                           0.97       949\n",
            "   macro avg       0.96      0.85      0.90       949\n",
            "weighted avg       0.97      0.97      0.97       949\n",
            "\n",
            "\n",
            "Accuracy:  0.9747102212855637\n"
          ]
        }
      ],
      "source": [
        "# to print the C_R\n",
        "print(classification_report(Thyroid_Y_test, pred2)) \n",
        "print()\n",
        "# reprinting the performance \n",
        "print('Accuracy: ', accuracy_score(Thyroid_Y_test, pred2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQ95qChXP1rr",
        "outputId": "5101517d-ff00-4886-c451-473036f328f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision by RandomForest of testing data is: 0.975\n",
            "Recall by RandomForest of testing data is: 0.975\n",
            "F1 score by RandomForest of testing data is: 0.975\n"
          ]
        }
      ],
      "source": [
        "# various libraries for report\n",
        "from sklearn.metrics import precision_score,recall_score,f1_score\n",
        "# printing the performance \n",
        "print('Precision by RandomForest of testing data is: %.3f' % precision_score(Thyroid_Y_test, pred2,average='micro')) \n",
        "# checking the precision value\n",
        "print('Recall by RandomForest of testing data is: %.3f' % recall_score(Thyroid_Y_test, pred2,average='micro')) \n",
        "# checking the recall value\n",
        "print('F1 score by RandomForest of testing data is: %.3f' % f1_score(Thyroid_Y_test, pred2,average='micro')) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "q6VwcdYhMN6a"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "# for cm\n",
        "cm1 = confusion_matrix(Thyroid_Y_test,pred2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "ifvgyet3MN3a",
        "outputId": "a9235d44-8aee-456e-c59f-42628168704f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(33.0, 0.5, 'Original Values')"
            ]
          },
          "metadata": {},
          "execution_count": 70
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# making heat map \n",
        "import seaborn as sns\n",
        "# assigning the color \n",
        "sns.heatmap(cm,annot=True, fmt='d' , cmap='Pastel2_r')\n",
        "# assigning the x-axis label\n",
        "plt.xlabel('Predicted values')\n",
        "# assigning the y-axis label\n",
        "plt.ylabel('Original Values')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z-u3QQPIMN0G",
        "outputId": "5c13894c-0103-4b3e-935b-a790f2f98b83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sensitivity :  0.9965753424657534\n",
            "Specificity :  0.7123287671232876\n"
          ]
        }
      ],
      "source": [
        "# printing Sensitivity accuracy \n",
        "sensitivity = cm1[1,1]/(cm1[1,1]+cm1[1,0])\n",
        "# to get the sentivity score\n",
        "print('Sensitivity : ', sensitivity)\n",
        "# printing Specificity accuracy \n",
        "specificity = cm1[0,0]/(cm1[0,1]+cm1[0,0])\n",
        "# to get the specoficity score\n",
        "print('Specificity : ', specificity)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QM552fL3kjI"
      },
      "source": [
        "# **ANN_Classifier**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "ERYZX5-u3kjI"
      },
      "outputs": [],
      "source": [
        "# importing library to the conersion of data into different categories \n",
        "from tensorflow.keras.utils import to_categorical, plot_model\n",
        "# to categorize the ytrain variable \n",
        "cat_Thyroid_Y_train = to_categorical(Thyroid_Y_train)\n",
        "# to categorize the ytest variable \n",
        "cat_Thyroid_Y_test = to_categorical(Thyroid_Y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c1216dd-1f42-4b7a-a143-08535255f308",
        "id": "bFZUe7PK3kjI"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2214, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ],
      "source": [
        "cat_Thyroid_Y_train.shape # to get the shape "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e97723c-a05f-4e88-c2e7-9e68a2e66d6f",
        "id": "FjiCsVVB3kjI"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(949, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ],
      "source": [
        "cat_Thyroid_Y_test.shape # to get the shape "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "0DwX4fCu3kjI"
      },
      "outputs": [],
      "source": [
        "# importing model\n",
        "from keras.models import Sequential\n",
        "# importing layers \n",
        "from keras.layers import Dense\n",
        "# initializing the model\n",
        "model_ann = Sequential()\n",
        "# adding the hidden layer\n",
        "model_ann.add(Dense(12, activation='relu'))\n",
        "# adding the deep layer\n",
        "model_ann.add(Dense(8,activation='relu'))\n",
        "# defining the resulting layer\n",
        "model_ann.add(Dense(2,activation='sigmoid'))\n",
        "# to debug the ANN algorithm\n",
        "model_ann.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "rrQLxJyy3kjI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00b575a5-b72b-455f-addd-ca7af466a9d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 4.1932 - accuracy: 0.7773 - val_loss: 0.9733 - val_accuracy: 0.9146\n",
            "Epoch 2/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.6685 - accuracy: 0.9110 - val_loss: 0.6122 - val_accuracy: 0.8946\n",
            "Epoch 3/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.4729 - accuracy: 0.9178 - val_loss: 0.5021 - val_accuracy: 0.9210\n",
            "Epoch 4/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.4059 - accuracy: 0.9205 - val_loss: 0.4488 - val_accuracy: 0.9220\n",
            "Epoch 5/1000\n",
            "222/222 [==============================] - 0s 1ms/step - loss: 0.3624 - accuracy: 0.9205 - val_loss: 0.4055 - val_accuracy: 0.9220\n",
            "Epoch 6/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.3320 - accuracy: 0.9223 - val_loss: 0.3739 - val_accuracy: 0.9231\n",
            "Epoch 7/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.3155 - accuracy: 0.9187 - val_loss: 0.4228 - val_accuracy: 0.8672\n",
            "Epoch 8/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.2993 - accuracy: 0.9178 - val_loss: 0.3235 - val_accuracy: 0.9210\n",
            "Epoch 9/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.2893 - accuracy: 0.9232 - val_loss: 0.3052 - val_accuracy: 0.9220\n",
            "Epoch 10/1000\n",
            "222/222 [==============================] - 0s 1ms/step - loss: 0.2843 - accuracy: 0.9196 - val_loss: 0.2934 - val_accuracy: 0.9210\n",
            "Epoch 11/1000\n",
            "222/222 [==============================] - 0s 1ms/step - loss: 0.2664 - accuracy: 0.9205 - val_loss: 0.3008 - val_accuracy: 0.9231\n",
            "Epoch 12/1000\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.2684 - accuracy: 0.9237 - val_loss: 0.3161 - val_accuracy: 0.8978\n",
            "Epoch 13/1000\n",
            "222/222 [==============================] - 1s 3ms/step - loss: 0.2779 - accuracy: 0.9201 - val_loss: 0.2787 - val_accuracy: 0.9210\n",
            "Epoch 14/1000\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.2654 - accuracy: 0.9232 - val_loss: 0.2969 - val_accuracy: 0.9220\n",
            "Epoch 15/1000\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.2531 - accuracy: 0.9237 - val_loss: 0.2800 - val_accuracy: 0.9220\n",
            "Epoch 16/1000\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.2522 - accuracy: 0.9241 - val_loss: 0.2694 - val_accuracy: 0.9189\n",
            "Epoch 17/1000\n",
            "222/222 [==============================] - 1s 3ms/step - loss: 0.2509 - accuracy: 0.9241 - val_loss: 0.2630 - val_accuracy: 0.9252\n",
            "Epoch 18/1000\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.2438 - accuracy: 0.9246 - val_loss: 0.2845 - val_accuracy: 0.9241\n",
            "Epoch 19/1000\n",
            "222/222 [==============================] - 1s 3ms/step - loss: 0.2468 - accuracy: 0.9273 - val_loss: 0.4284 - val_accuracy: 0.8462\n",
            "Epoch 20/1000\n",
            "222/222 [==============================] - 1s 3ms/step - loss: 0.2524 - accuracy: 0.9214 - val_loss: 0.2641 - val_accuracy: 0.9283\n",
            "Epoch 21/1000\n",
            "222/222 [==============================] - 1s 3ms/step - loss: 0.2384 - accuracy: 0.9268 - val_loss: 0.2532 - val_accuracy: 0.9283\n",
            "Epoch 22/1000\n",
            "222/222 [==============================] - 1s 3ms/step - loss: 0.2323 - accuracy: 0.9259 - val_loss: 0.2566 - val_accuracy: 0.9294\n",
            "Epoch 23/1000\n",
            "222/222 [==============================] - 1s 3ms/step - loss: 0.2350 - accuracy: 0.9219 - val_loss: 0.2614 - val_accuracy: 0.9262\n",
            "Epoch 24/1000\n",
            "222/222 [==============================] - 1s 3ms/step - loss: 0.2353 - accuracy: 0.9241 - val_loss: 0.2541 - val_accuracy: 0.9210\n",
            "Epoch 25/1000\n",
            "222/222 [==============================] - 1s 3ms/step - loss: 0.2365 - accuracy: 0.9246 - val_loss: 0.2930 - val_accuracy: 0.9210\n",
            "Epoch 26/1000\n",
            "222/222 [==============================] - 1s 3ms/step - loss: 0.2478 - accuracy: 0.9255 - val_loss: 0.2526 - val_accuracy: 0.9241\n",
            "Epoch 27/1000\n",
            "222/222 [==============================] - 1s 3ms/step - loss: 0.2226 - accuracy: 0.9286 - val_loss: 0.2505 - val_accuracy: 0.9199\n",
            "Epoch 28/1000\n",
            "222/222 [==============================] - 1s 3ms/step - loss: 0.2282 - accuracy: 0.9205 - val_loss: 0.2461 - val_accuracy: 0.9210\n",
            "Epoch 29/1000\n",
            "222/222 [==============================] - 1s 3ms/step - loss: 0.2292 - accuracy: 0.9250 - val_loss: 0.2406 - val_accuracy: 0.9241\n",
            "Epoch 30/1000\n",
            "222/222 [==============================] - 1s 3ms/step - loss: 0.2238 - accuracy: 0.9246 - val_loss: 0.2460 - val_accuracy: 0.9241\n",
            "Epoch 31/1000\n",
            "222/222 [==============================] - 1s 4ms/step - loss: 0.2301 - accuracy: 0.9246 - val_loss: 0.2601 - val_accuracy: 0.9241\n",
            "Epoch 32/1000\n",
            "222/222 [==============================] - 1s 3ms/step - loss: 0.2225 - accuracy: 0.9246 - val_loss: 0.2404 - val_accuracy: 0.9252\n",
            "Epoch 33/1000\n",
            "222/222 [==============================] - 1s 5ms/step - loss: 0.2219 - accuracy: 0.9255 - val_loss: 0.2490 - val_accuracy: 0.9231\n",
            "Epoch 34/1000\n",
            "222/222 [==============================] - 1s 4ms/step - loss: 0.2164 - accuracy: 0.9250 - val_loss: 0.2535 - val_accuracy: 0.9210\n",
            "Epoch 35/1000\n",
            "222/222 [==============================] - 1s 3ms/step - loss: 0.2161 - accuracy: 0.9277 - val_loss: 0.2783 - val_accuracy: 0.9231\n",
            "Epoch 36/1000\n",
            "222/222 [==============================] - 1s 4ms/step - loss: 0.2332 - accuracy: 0.9250 - val_loss: 0.2487 - val_accuracy: 0.9252\n",
            "Epoch 37/1000\n",
            "222/222 [==============================] - 1s 3ms/step - loss: 0.2256 - accuracy: 0.9246 - val_loss: 0.2359 - val_accuracy: 0.9262\n",
            "Epoch 38/1000\n",
            "222/222 [==============================] - 1s 3ms/step - loss: 0.2235 - accuracy: 0.9277 - val_loss: 0.2455 - val_accuracy: 0.9262\n",
            "Epoch 39/1000\n",
            "222/222 [==============================] - 1s 3ms/step - loss: 0.2222 - accuracy: 0.9273 - val_loss: 0.2374 - val_accuracy: 0.9262\n",
            "Epoch 40/1000\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.2239 - accuracy: 0.9228 - val_loss: 0.2526 - val_accuracy: 0.9305\n",
            "Epoch 41/1000\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.2131 - accuracy: 0.9282 - val_loss: 0.2341 - val_accuracy: 0.9273\n",
            "Epoch 42/1000\n",
            "222/222 [==============================] - 1s 3ms/step - loss: 0.2146 - accuracy: 0.9268 - val_loss: 0.2371 - val_accuracy: 0.9294\n",
            "Epoch 43/1000\n",
            "222/222 [==============================] - 1s 3ms/step - loss: 0.2178 - accuracy: 0.9246 - val_loss: 0.2463 - val_accuracy: 0.9262\n",
            "Epoch 44/1000\n",
            "222/222 [==============================] - 0s 1ms/step - loss: 0.2213 - accuracy: 0.9286 - val_loss: 0.2342 - val_accuracy: 0.9283\n",
            "Epoch 45/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.2199 - accuracy: 0.9291 - val_loss: 0.2846 - val_accuracy: 0.9252\n",
            "Epoch 46/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.2195 - accuracy: 0.9250 - val_loss: 0.2351 - val_accuracy: 0.9294\n",
            "Epoch 47/1000\n",
            "222/222 [==============================] - 0s 1ms/step - loss: 0.2098 - accuracy: 0.9286 - val_loss: 0.2332 - val_accuracy: 0.9294\n",
            "Epoch 48/1000\n",
            "222/222 [==============================] - 0s 1ms/step - loss: 0.2126 - accuracy: 0.9295 - val_loss: 0.2856 - val_accuracy: 0.9252\n",
            "Epoch 49/1000\n",
            "222/222 [==============================] - 0s 1ms/step - loss: 0.2139 - accuracy: 0.9282 - val_loss: 0.2305 - val_accuracy: 0.9294\n",
            "Epoch 50/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.2244 - accuracy: 0.9259 - val_loss: 0.2278 - val_accuracy: 0.9283\n",
            "Epoch 51/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.2146 - accuracy: 0.9286 - val_loss: 0.2343 - val_accuracy: 0.9262\n",
            "Epoch 52/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.2112 - accuracy: 0.9268 - val_loss: 0.2212 - val_accuracy: 0.9294\n",
            "Epoch 53/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.2094 - accuracy: 0.9309 - val_loss: 0.2388 - val_accuracy: 0.9336\n",
            "Epoch 54/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.2037 - accuracy: 0.9313 - val_loss: 0.2248 - val_accuracy: 0.9305\n",
            "Epoch 55/1000\n",
            "222/222 [==============================] - 0s 1ms/step - loss: 0.2032 - accuracy: 0.9286 - val_loss: 0.2185 - val_accuracy: 0.9368\n",
            "Epoch 56/1000\n",
            "222/222 [==============================] - 0s 1ms/step - loss: 0.2037 - accuracy: 0.9336 - val_loss: 0.2250 - val_accuracy: 0.9305\n",
            "Epoch 57/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.2092 - accuracy: 0.9291 - val_loss: 0.2420 - val_accuracy: 0.9241\n",
            "Epoch 58/1000\n",
            "222/222 [==============================] - 0s 1ms/step - loss: 0.2083 - accuracy: 0.9250 - val_loss: 0.2195 - val_accuracy: 0.9315\n",
            "Epoch 59/1000\n",
            "222/222 [==============================] - 0s 1ms/step - loss: 0.2061 - accuracy: 0.9282 - val_loss: 0.2205 - val_accuracy: 0.9347\n",
            "Epoch 60/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.2123 - accuracy: 0.9277 - val_loss: 0.2827 - val_accuracy: 0.9125\n",
            "Epoch 61/1000\n",
            "222/222 [==============================] - 0s 1ms/step - loss: 0.2021 - accuracy: 0.9309 - val_loss: 0.2173 - val_accuracy: 0.9315\n",
            "Epoch 62/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.2012 - accuracy: 0.9282 - val_loss: 0.2115 - val_accuracy: 0.9326\n",
            "Epoch 63/1000\n",
            "222/222 [==============================] - 0s 1ms/step - loss: 0.1991 - accuracy: 0.9295 - val_loss: 0.2199 - val_accuracy: 0.9283\n",
            "Epoch 64/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1975 - accuracy: 0.9300 - val_loss: 0.2195 - val_accuracy: 0.9326\n",
            "Epoch 65/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1996 - accuracy: 0.9309 - val_loss: 0.2292 - val_accuracy: 0.9283\n",
            "Epoch 66/1000\n",
            "222/222 [==============================] - 0s 1ms/step - loss: 0.2039 - accuracy: 0.9322 - val_loss: 0.2176 - val_accuracy: 0.9336\n",
            "Epoch 67/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1981 - accuracy: 0.9304 - val_loss: 0.2259 - val_accuracy: 0.9262\n",
            "Epoch 68/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1997 - accuracy: 0.9309 - val_loss: 0.2281 - val_accuracy: 0.9262\n",
            "Epoch 69/1000\n",
            "222/222 [==============================] - 0s 1ms/step - loss: 0.2020 - accuracy: 0.9295 - val_loss: 0.2137 - val_accuracy: 0.9357\n",
            "Epoch 70/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.2022 - accuracy: 0.9304 - val_loss: 0.2118 - val_accuracy: 0.9399\n",
            "Epoch 71/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1932 - accuracy: 0.9345 - val_loss: 0.2762 - val_accuracy: 0.8915\n",
            "Epoch 72/1000\n",
            "222/222 [==============================] - 0s 1ms/step - loss: 0.1988 - accuracy: 0.9304 - val_loss: 0.2566 - val_accuracy: 0.9062\n",
            "Epoch 73/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.2014 - accuracy: 0.9264 - val_loss: 0.2277 - val_accuracy: 0.9262\n",
            "Epoch 74/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.2006 - accuracy: 0.9277 - val_loss: 0.2223 - val_accuracy: 0.9283\n",
            "Epoch 75/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1988 - accuracy: 0.9304 - val_loss: 0.2172 - val_accuracy: 0.9347\n",
            "Epoch 76/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1973 - accuracy: 0.9291 - val_loss: 0.2164 - val_accuracy: 0.9389\n",
            "Epoch 77/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1961 - accuracy: 0.9345 - val_loss: 0.2089 - val_accuracy: 0.9368\n",
            "Epoch 78/1000\n",
            "222/222 [==============================] - 0s 1ms/step - loss: 0.1960 - accuracy: 0.9304 - val_loss: 0.2461 - val_accuracy: 0.9157\n",
            "Epoch 79/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1882 - accuracy: 0.9309 - val_loss: 0.2072 - val_accuracy: 0.9305\n",
            "Epoch 80/1000\n",
            "222/222 [==============================] - 0s 1ms/step - loss: 0.1921 - accuracy: 0.9318 - val_loss: 0.2095 - val_accuracy: 0.9368\n",
            "Epoch 81/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1931 - accuracy: 0.9286 - val_loss: 0.2102 - val_accuracy: 0.9347\n",
            "Epoch 82/1000\n",
            "222/222 [==============================] - 0s 1ms/step - loss: 0.1900 - accuracy: 0.9291 - val_loss: 0.2067 - val_accuracy: 0.9305\n",
            "Epoch 83/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1861 - accuracy: 0.9341 - val_loss: 0.2176 - val_accuracy: 0.9305\n",
            "Epoch 84/1000\n",
            "222/222 [==============================] - 0s 1ms/step - loss: 0.1948 - accuracy: 0.9282 - val_loss: 0.2580 - val_accuracy: 0.8999\n",
            "Epoch 85/1000\n",
            "222/222 [==============================] - 0s 1ms/step - loss: 0.1918 - accuracy: 0.9322 - val_loss: 0.2128 - val_accuracy: 0.9305\n",
            "Epoch 86/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1891 - accuracy: 0.9309 - val_loss: 0.2143 - val_accuracy: 0.9399\n",
            "Epoch 87/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1921 - accuracy: 0.9282 - val_loss: 0.2062 - val_accuracy: 0.9368\n",
            "Epoch 88/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1911 - accuracy: 0.9309 - val_loss: 0.2160 - val_accuracy: 0.9336\n",
            "Epoch 89/1000\n",
            "222/222 [==============================] - 0s 1ms/step - loss: 0.1999 - accuracy: 0.9282 - val_loss: 0.2062 - val_accuracy: 0.9420\n",
            "Epoch 90/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1843 - accuracy: 0.9322 - val_loss: 0.2696 - val_accuracy: 0.8936\n",
            "Epoch 91/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1931 - accuracy: 0.9291 - val_loss: 0.2059 - val_accuracy: 0.9431\n",
            "Epoch 92/1000\n",
            "222/222 [==============================] - 0s 1ms/step - loss: 0.1845 - accuracy: 0.9327 - val_loss: 0.2307 - val_accuracy: 0.9273\n",
            "Epoch 93/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1886 - accuracy: 0.9354 - val_loss: 0.2101 - val_accuracy: 0.9336\n",
            "Epoch 94/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1840 - accuracy: 0.9322 - val_loss: 0.2060 - val_accuracy: 0.9357\n",
            "Epoch 95/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1845 - accuracy: 0.9282 - val_loss: 0.2032 - val_accuracy: 0.9378\n",
            "Epoch 96/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1872 - accuracy: 0.9345 - val_loss: 0.2474 - val_accuracy: 0.9241\n",
            "Epoch 97/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1934 - accuracy: 0.9300 - val_loss: 0.2145 - val_accuracy: 0.9326\n",
            "Epoch 98/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1849 - accuracy: 0.9336 - val_loss: 0.2070 - val_accuracy: 0.9442\n",
            "Epoch 99/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1877 - accuracy: 0.9313 - val_loss: 0.2137 - val_accuracy: 0.9378\n",
            "Epoch 100/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1804 - accuracy: 0.9291 - val_loss: 0.2040 - val_accuracy: 0.9378\n",
            "Epoch 101/1000\n",
            "222/222 [==============================] - 0s 1ms/step - loss: 0.1817 - accuracy: 0.9332 - val_loss: 0.2240 - val_accuracy: 0.9262\n",
            "Epoch 102/1000\n",
            "222/222 [==============================] - 0s 1ms/step - loss: 0.1850 - accuracy: 0.9304 - val_loss: 0.2051 - val_accuracy: 0.9368\n",
            "Epoch 103/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1806 - accuracy: 0.9336 - val_loss: 0.2198 - val_accuracy: 0.9262\n",
            "Epoch 104/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1992 - accuracy: 0.9322 - val_loss: 0.2019 - val_accuracy: 0.9410\n",
            "Epoch 105/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1964 - accuracy: 0.9273 - val_loss: 0.2091 - val_accuracy: 0.9368\n",
            "Epoch 106/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1831 - accuracy: 0.9341 - val_loss: 0.2069 - val_accuracy: 0.9431\n",
            "Epoch 107/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1827 - accuracy: 0.9327 - val_loss: 0.2146 - val_accuracy: 0.9305\n",
            "Epoch 108/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1826 - accuracy: 0.9336 - val_loss: 0.2213 - val_accuracy: 0.9294\n",
            "Epoch 109/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1852 - accuracy: 0.9295 - val_loss: 0.2057 - val_accuracy: 0.9378\n",
            "Epoch 110/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1807 - accuracy: 0.9318 - val_loss: 0.2105 - val_accuracy: 0.9410\n",
            "Epoch 111/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1780 - accuracy: 0.9300 - val_loss: 0.2092 - val_accuracy: 0.9420\n",
            "Epoch 112/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1865 - accuracy: 0.9332 - val_loss: 0.2068 - val_accuracy: 0.9399\n",
            "Epoch 113/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1804 - accuracy: 0.9318 - val_loss: 0.2125 - val_accuracy: 0.9305\n",
            "Epoch 114/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1751 - accuracy: 0.9377 - val_loss: 0.1980 - val_accuracy: 0.9389\n",
            "Epoch 115/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1750 - accuracy: 0.9377 - val_loss: 0.2187 - val_accuracy: 0.9252\n",
            "Epoch 116/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1791 - accuracy: 0.9350 - val_loss: 0.1986 - val_accuracy: 0.9442\n",
            "Epoch 117/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1739 - accuracy: 0.9363 - val_loss: 0.2068 - val_accuracy: 0.9420\n",
            "Epoch 118/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1724 - accuracy: 0.9341 - val_loss: 0.2047 - val_accuracy: 0.9326\n",
            "Epoch 119/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1767 - accuracy: 0.9345 - val_loss: 0.1972 - val_accuracy: 0.9442\n",
            "Epoch 120/1000\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.1800 - accuracy: 0.9304 - val_loss: 0.2012 - val_accuracy: 0.9326\n",
            "Epoch 121/1000\n",
            "222/222 [==============================] - 1s 4ms/step - loss: 0.1765 - accuracy: 0.9354 - val_loss: 0.2033 - val_accuracy: 0.9399\n",
            "Epoch 122/1000\n",
            "222/222 [==============================] - 1s 3ms/step - loss: 0.1751 - accuracy: 0.9341 - val_loss: 0.2044 - val_accuracy: 0.9326\n",
            "Epoch 123/1000\n",
            "222/222 [==============================] - 1s 3ms/step - loss: 0.1733 - accuracy: 0.9327 - val_loss: 0.1988 - val_accuracy: 0.9420\n",
            "Epoch 124/1000\n",
            "222/222 [==============================] - 1s 4ms/step - loss: 0.1829 - accuracy: 0.9332 - val_loss: 0.1990 - val_accuracy: 0.9442\n",
            "Epoch 125/1000\n",
            "222/222 [==============================] - 1s 3ms/step - loss: 0.1759 - accuracy: 0.9304 - val_loss: 0.1982 - val_accuracy: 0.9389\n",
            "Epoch 126/1000\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.1793 - accuracy: 0.9322 - val_loss: 0.1953 - val_accuracy: 0.9420\n",
            "Epoch 127/1000\n",
            "222/222 [==============================] - 0s 1ms/step - loss: 0.1740 - accuracy: 0.9345 - val_loss: 0.1908 - val_accuracy: 0.9420\n",
            "Epoch 128/1000\n",
            "222/222 [==============================] - 0s 1ms/step - loss: 0.1691 - accuracy: 0.9372 - val_loss: 0.1980 - val_accuracy: 0.9473\n",
            "Epoch 129/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1784 - accuracy: 0.9318 - val_loss: 0.1973 - val_accuracy: 0.9357\n",
            "Epoch 130/1000\n",
            "222/222 [==============================] - 0s 1ms/step - loss: 0.1714 - accuracy: 0.9341 - val_loss: 0.2018 - val_accuracy: 0.9420\n",
            "Epoch 131/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1833 - accuracy: 0.9336 - val_loss: 0.1968 - val_accuracy: 0.9473\n",
            "Epoch 132/1000\n",
            "222/222 [==============================] - 0s 1ms/step - loss: 0.1794 - accuracy: 0.9350 - val_loss: 0.2023 - val_accuracy: 0.9368\n",
            "Epoch 133/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1770 - accuracy: 0.9295 - val_loss: 0.2001 - val_accuracy: 0.9473\n",
            "Epoch 134/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1711 - accuracy: 0.9359 - val_loss: 0.1923 - val_accuracy: 0.9399\n",
            "Epoch 135/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1653 - accuracy: 0.9363 - val_loss: 0.1906 - val_accuracy: 0.9484\n",
            "Epoch 136/1000\n",
            "222/222 [==============================] - 0s 1ms/step - loss: 0.1719 - accuracy: 0.9368 - val_loss: 0.1863 - val_accuracy: 0.9442\n",
            "Epoch 137/1000\n",
            "222/222 [==============================] - 0s 1ms/step - loss: 0.1696 - accuracy: 0.9363 - val_loss: 0.1900 - val_accuracy: 0.9505\n",
            "Epoch 138/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1700 - accuracy: 0.9354 - val_loss: 0.1993 - val_accuracy: 0.9389\n",
            "Epoch 139/1000\n",
            "222/222 [==============================] - 0s 1ms/step - loss: 0.1734 - accuracy: 0.9354 - val_loss: 0.1905 - val_accuracy: 0.9494\n",
            "Epoch 140/1000\n",
            "222/222 [==============================] - 0s 1ms/step - loss: 0.1666 - accuracy: 0.9363 - val_loss: 0.1962 - val_accuracy: 0.9357\n",
            "Epoch 141/1000\n",
            "222/222 [==============================] - 0s 1ms/step - loss: 0.1658 - accuracy: 0.9368 - val_loss: 0.1996 - val_accuracy: 0.9420\n",
            "Epoch 142/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1764 - accuracy: 0.9318 - val_loss: 0.2028 - val_accuracy: 0.9431\n",
            "Epoch 143/1000\n",
            "222/222 [==============================] - 0s 1ms/step - loss: 0.1694 - accuracy: 0.9386 - val_loss: 0.2081 - val_accuracy: 0.9378\n",
            "Epoch 144/1000\n",
            "222/222 [==============================] - 0s 1ms/step - loss: 0.1688 - accuracy: 0.9341 - val_loss: 0.1879 - val_accuracy: 0.9494\n",
            "Epoch 145/1000\n",
            "222/222 [==============================] - 0s 1ms/step - loss: 0.1663 - accuracy: 0.9363 - val_loss: 0.1928 - val_accuracy: 0.9420\n",
            "Epoch 146/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1681 - accuracy: 0.9332 - val_loss: 0.2100 - val_accuracy: 0.9305\n",
            "Epoch 147/1000\n",
            "222/222 [==============================] - 0s 1ms/step - loss: 0.1649 - accuracy: 0.9377 - val_loss: 0.1949 - val_accuracy: 0.9410\n",
            "Epoch 148/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1730 - accuracy: 0.9300 - val_loss: 0.2098 - val_accuracy: 0.9368\n",
            "Epoch 149/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1646 - accuracy: 0.9354 - val_loss: 0.1859 - val_accuracy: 0.9442\n",
            "Epoch 150/1000\n",
            "222/222 [==============================] - 0s 1ms/step - loss: 0.1694 - accuracy: 0.9372 - val_loss: 0.1978 - val_accuracy: 0.9431\n",
            "Epoch 151/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1633 - accuracy: 0.9354 - val_loss: 0.1921 - val_accuracy: 0.9442\n",
            "Epoch 152/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1778 - accuracy: 0.9332 - val_loss: 0.1960 - val_accuracy: 0.9463\n",
            "Epoch 153/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1745 - accuracy: 0.9399 - val_loss: 0.1968 - val_accuracy: 0.9336\n",
            "Epoch 154/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1707 - accuracy: 0.9341 - val_loss: 0.1927 - val_accuracy: 0.9484\n",
            "Epoch 155/1000\n",
            "222/222 [==============================] - 0s 1ms/step - loss: 0.1652 - accuracy: 0.9386 - val_loss: 0.1933 - val_accuracy: 0.9410\n",
            "Epoch 156/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1665 - accuracy: 0.9354 - val_loss: 0.1908 - val_accuracy: 0.9442\n",
            "Epoch 157/1000\n",
            "222/222 [==============================] - 0s 1ms/step - loss: 0.1639 - accuracy: 0.9368 - val_loss: 0.2115 - val_accuracy: 0.9420\n",
            "Epoch 158/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1630 - accuracy: 0.9422 - val_loss: 0.1962 - val_accuracy: 0.9399\n",
            "Epoch 159/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1732 - accuracy: 0.9377 - val_loss: 0.2137 - val_accuracy: 0.9178\n",
            "Epoch 160/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1726 - accuracy: 0.9368 - val_loss: 0.1940 - val_accuracy: 0.9410\n",
            "Epoch 161/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1663 - accuracy: 0.9359 - val_loss: 0.1986 - val_accuracy: 0.9452\n",
            "Epoch 162/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1680 - accuracy: 0.9363 - val_loss: 0.1924 - val_accuracy: 0.9452\n",
            "Epoch 163/1000\n",
            "222/222 [==============================] - 0s 1ms/step - loss: 0.1644 - accuracy: 0.9390 - val_loss: 0.1929 - val_accuracy: 0.9484\n",
            "Epoch 164/1000\n",
            "222/222 [==============================] - 0s 1ms/step - loss: 0.1686 - accuracy: 0.9377 - val_loss: 0.2080 - val_accuracy: 0.9273\n",
            "Epoch 165/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1644 - accuracy: 0.9372 - val_loss: 0.2067 - val_accuracy: 0.9336\n",
            "Epoch 166/1000\n",
            "222/222 [==============================] - 0s 1ms/step - loss: 0.1674 - accuracy: 0.9363 - val_loss: 0.2172 - val_accuracy: 0.9347\n",
            "Epoch 167/1000\n",
            "222/222 [==============================] - 0s 1ms/step - loss: 0.1670 - accuracy: 0.9368 - val_loss: 0.1907 - val_accuracy: 0.9452\n",
            "Epoch 168/1000\n",
            "222/222 [==============================] - 0s 1ms/step - loss: 0.1696 - accuracy: 0.9345 - val_loss: 0.2144 - val_accuracy: 0.9273\n",
            "Epoch 169/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1711 - accuracy: 0.9332 - val_loss: 0.1844 - val_accuracy: 0.9452\n",
            "Epoch 170/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1624 - accuracy: 0.9377 - val_loss: 0.1908 - val_accuracy: 0.9431\n",
            "Epoch 171/1000\n",
            "222/222 [==============================] - 0s 1ms/step - loss: 0.1585 - accuracy: 0.9368 - val_loss: 0.1904 - val_accuracy: 0.9442\n",
            "Epoch 172/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1631 - accuracy: 0.9363 - val_loss: 0.1833 - val_accuracy: 0.9463\n",
            "Epoch 173/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1746 - accuracy: 0.9336 - val_loss: 0.2153 - val_accuracy: 0.9347\n",
            "Epoch 174/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1623 - accuracy: 0.9341 - val_loss: 0.2065 - val_accuracy: 0.9347\n",
            "Epoch 175/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1649 - accuracy: 0.9390 - val_loss: 0.2193 - val_accuracy: 0.9357\n",
            "Epoch 176/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1613 - accuracy: 0.9426 - val_loss: 0.1790 - val_accuracy: 0.9515\n",
            "Epoch 177/1000\n",
            "222/222 [==============================] - 0s 1ms/step - loss: 0.1622 - accuracy: 0.9345 - val_loss: 0.1889 - val_accuracy: 0.9473\n",
            "Epoch 178/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1626 - accuracy: 0.9332 - val_loss: 0.1854 - val_accuracy: 0.9473\n",
            "Epoch 179/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1678 - accuracy: 0.9350 - val_loss: 0.1881 - val_accuracy: 0.9515\n",
            "Epoch 180/1000\n",
            "222/222 [==============================] - 0s 1ms/step - loss: 0.1780 - accuracy: 0.9332 - val_loss: 0.1922 - val_accuracy: 0.9452\n",
            "Epoch 181/1000\n",
            "222/222 [==============================] - 0s 1ms/step - loss: 0.1634 - accuracy: 0.9381 - val_loss: 0.1949 - val_accuracy: 0.9473\n",
            "Epoch 182/1000\n",
            "222/222 [==============================] - 0s 1ms/step - loss: 0.1644 - accuracy: 0.9368 - val_loss: 0.1987 - val_accuracy: 0.9389\n",
            "Epoch 183/1000\n",
            "222/222 [==============================] - 0s 1ms/step - loss: 0.1596 - accuracy: 0.9399 - val_loss: 0.2068 - val_accuracy: 0.9305\n",
            "Epoch 184/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1645 - accuracy: 0.9345 - val_loss: 0.1809 - val_accuracy: 0.9463\n",
            "Epoch 185/1000\n",
            "222/222 [==============================] - 0s 1ms/step - loss: 0.1647 - accuracy: 0.9354 - val_loss: 0.1759 - val_accuracy: 0.9505\n",
            "Epoch 186/1000\n",
            "222/222 [==============================] - 0s 1ms/step - loss: 0.1589 - accuracy: 0.9404 - val_loss: 0.2200 - val_accuracy: 0.9294\n",
            "Epoch 187/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1651 - accuracy: 0.9354 - val_loss: 0.1914 - val_accuracy: 0.9473\n",
            "Epoch 188/1000\n",
            "222/222 [==============================] - 0s 1ms/step - loss: 0.1571 - accuracy: 0.9395 - val_loss: 0.1889 - val_accuracy: 0.9473\n",
            "Epoch 189/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1605 - accuracy: 0.9408 - val_loss: 0.2270 - val_accuracy: 0.9178\n",
            "Epoch 190/1000\n",
            "222/222 [==============================] - 1s 4ms/step - loss: 0.1583 - accuracy: 0.9363 - val_loss: 0.1884 - val_accuracy: 0.9494\n",
            "Epoch 191/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1616 - accuracy: 0.9359 - val_loss: 0.1951 - val_accuracy: 0.9494\n",
            "Epoch 192/1000\n",
            "222/222 [==============================] - 1s 3ms/step - loss: 0.1663 - accuracy: 0.9354 - val_loss: 0.1825 - val_accuracy: 0.9515\n",
            "Epoch 193/1000\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.1577 - accuracy: 0.9413 - val_loss: 0.2042 - val_accuracy: 0.9231\n",
            "Epoch 194/1000\n",
            "222/222 [==============================] - 1s 3ms/step - loss: 0.1596 - accuracy: 0.9395 - val_loss: 0.1867 - val_accuracy: 0.9420\n",
            "Epoch 195/1000\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.1592 - accuracy: 0.9368 - val_loss: 0.1868 - val_accuracy: 0.9420\n",
            "Epoch 196/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1613 - accuracy: 0.9381 - val_loss: 0.1986 - val_accuracy: 0.9357\n",
            "Epoch 197/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1593 - accuracy: 0.9386 - val_loss: 0.2066 - val_accuracy: 0.9399\n",
            "Epoch 198/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1603 - accuracy: 0.9390 - val_loss: 0.1930 - val_accuracy: 0.9473\n",
            "Epoch 199/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1541 - accuracy: 0.9413 - val_loss: 0.2057 - val_accuracy: 0.9294\n",
            "Epoch 200/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1577 - accuracy: 0.9359 - val_loss: 0.2245 - val_accuracy: 0.9136\n",
            "Epoch 201/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1539 - accuracy: 0.9399 - val_loss: 0.1905 - val_accuracy: 0.9494\n",
            "Epoch 202/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1603 - accuracy: 0.9386 - val_loss: 0.2040 - val_accuracy: 0.9336\n",
            "Epoch 203/1000\n",
            "222/222 [==============================] - 0s 1ms/step - loss: 0.1596 - accuracy: 0.9381 - val_loss: 0.2219 - val_accuracy: 0.9146\n",
            "Epoch 204/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1621 - accuracy: 0.9363 - val_loss: 0.1768 - val_accuracy: 0.9515\n",
            "Epoch 205/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1608 - accuracy: 0.9359 - val_loss: 0.1880 - val_accuracy: 0.9420\n",
            "Epoch 206/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1597 - accuracy: 0.9417 - val_loss: 0.2047 - val_accuracy: 0.9357\n",
            "Epoch 207/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1580 - accuracy: 0.9372 - val_loss: 0.1980 - val_accuracy: 0.9505\n",
            "Epoch 208/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1521 - accuracy: 0.9422 - val_loss: 0.1838 - val_accuracy: 0.9420\n",
            "Epoch 209/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1551 - accuracy: 0.9386 - val_loss: 0.1894 - val_accuracy: 0.9347\n",
            "Epoch 210/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1576 - accuracy: 0.9377 - val_loss: 0.2277 - val_accuracy: 0.9052\n",
            "Epoch 211/1000\n",
            "222/222 [==============================] - 0s 1ms/step - loss: 0.1606 - accuracy: 0.9350 - val_loss: 0.1944 - val_accuracy: 0.9494\n",
            "Epoch 212/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1483 - accuracy: 0.9395 - val_loss: 0.1929 - val_accuracy: 0.9378\n",
            "Epoch 213/1000\n",
            "222/222 [==============================] - 0s 1ms/step - loss: 0.1489 - accuracy: 0.9426 - val_loss: 0.1796 - val_accuracy: 0.9526\n",
            "Epoch 214/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1592 - accuracy: 0.9341 - val_loss: 0.1830 - val_accuracy: 0.9473\n",
            "Epoch 215/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1607 - accuracy: 0.9417 - val_loss: 0.1852 - val_accuracy: 0.9473\n",
            "Epoch 216/1000\n",
            "222/222 [==============================] - 0s 1ms/step - loss: 0.1543 - accuracy: 0.9390 - val_loss: 0.1850 - val_accuracy: 0.9357\n",
            "Epoch 217/1000\n",
            "222/222 [==============================] - 0s 1ms/step - loss: 0.1457 - accuracy: 0.9444 - val_loss: 0.1929 - val_accuracy: 0.9399\n",
            "Epoch 218/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1632 - accuracy: 0.9377 - val_loss: 0.1879 - val_accuracy: 0.9389\n",
            "Epoch 219/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1582 - accuracy: 0.9359 - val_loss: 0.1801 - val_accuracy: 0.9452\n",
            "Epoch 220/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1612 - accuracy: 0.9368 - val_loss: 0.2087 - val_accuracy: 0.9378\n",
            "Epoch 221/1000\n",
            "222/222 [==============================] - 0s 1ms/step - loss: 0.1587 - accuracy: 0.9404 - val_loss: 0.1809 - val_accuracy: 0.9473\n",
            "Epoch 222/1000\n",
            "222/222 [==============================] - 0s 1ms/step - loss: 0.1615 - accuracy: 0.9354 - val_loss: 0.1824 - val_accuracy: 0.9431\n",
            "Epoch 223/1000\n",
            "222/222 [==============================] - 0s 1ms/step - loss: 0.1464 - accuracy: 0.9386 - val_loss: 0.1886 - val_accuracy: 0.9536\n",
            "Epoch 224/1000\n",
            "222/222 [==============================] - 0s 1ms/step - loss: 0.1579 - accuracy: 0.9368 - val_loss: 0.1946 - val_accuracy: 0.9420\n",
            "Epoch 225/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1616 - accuracy: 0.9386 - val_loss: 0.1767 - val_accuracy: 0.9505\n",
            "Epoch 226/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1505 - accuracy: 0.9390 - val_loss: 0.1958 - val_accuracy: 0.9442\n",
            "Epoch 227/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1612 - accuracy: 0.9386 - val_loss: 0.1865 - val_accuracy: 0.9484\n",
            "Epoch 228/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1586 - accuracy: 0.9404 - val_loss: 0.1865 - val_accuracy: 0.9420\n",
            "Epoch 229/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1566 - accuracy: 0.9350 - val_loss: 0.1778 - val_accuracy: 0.9473\n",
            "Epoch 230/1000\n",
            "222/222 [==============================] - 0s 1ms/step - loss: 0.1510 - accuracy: 0.9386 - val_loss: 0.1784 - val_accuracy: 0.9452\n",
            "Epoch 231/1000\n",
            "222/222 [==============================] - 0s 1ms/step - loss: 0.1597 - accuracy: 0.9372 - val_loss: 0.1779 - val_accuracy: 0.9463\n",
            "Epoch 232/1000\n",
            "222/222 [==============================] - 0s 1ms/step - loss: 0.1545 - accuracy: 0.9368 - val_loss: 0.1816 - val_accuracy: 0.9473\n",
            "Epoch 233/1000\n",
            "222/222 [==============================] - 0s 1ms/step - loss: 0.1573 - accuracy: 0.9386 - val_loss: 0.1881 - val_accuracy: 0.9463\n",
            "Epoch 234/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1469 - accuracy: 0.9426 - val_loss: 0.2031 - val_accuracy: 0.9378\n",
            "Epoch 235/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1550 - accuracy: 0.9417 - val_loss: 0.1917 - val_accuracy: 0.9368\n",
            "Epoch 236/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1599 - accuracy: 0.9350 - val_loss: 0.1913 - val_accuracy: 0.9494\n",
            "Epoch 237/1000\n",
            "222/222 [==============================] - 0s 1ms/step - loss: 0.1530 - accuracy: 0.9368 - val_loss: 0.2025 - val_accuracy: 0.9389\n",
            "Epoch 238/1000\n",
            "222/222 [==============================] - 0s 1ms/step - loss: 0.1505 - accuracy: 0.9395 - val_loss: 0.1989 - val_accuracy: 0.9336\n",
            "Epoch 239/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1550 - accuracy: 0.9413 - val_loss: 0.1915 - val_accuracy: 0.9389\n",
            "Epoch 240/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1566 - accuracy: 0.9395 - val_loss: 0.1862 - val_accuracy: 0.9473\n",
            "Epoch 241/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1491 - accuracy: 0.9386 - val_loss: 0.1847 - val_accuracy: 0.9431\n",
            "Epoch 242/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1463 - accuracy: 0.9435 - val_loss: 0.1943 - val_accuracy: 0.9420\n",
            "Epoch 243/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1473 - accuracy: 0.9399 - val_loss: 0.1773 - val_accuracy: 0.9484\n",
            "Epoch 244/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1504 - accuracy: 0.9399 - val_loss: 0.1816 - val_accuracy: 0.9463\n",
            "Epoch 245/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1539 - accuracy: 0.9444 - val_loss: 0.2051 - val_accuracy: 0.9389\n",
            "Epoch 246/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1549 - accuracy: 0.9413 - val_loss: 0.1844 - val_accuracy: 0.9452\n",
            "Epoch 247/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1551 - accuracy: 0.9413 - val_loss: 0.1844 - val_accuracy: 0.9442\n",
            "Epoch 248/1000\n",
            "222/222 [==============================] - 1s 3ms/step - loss: 0.1506 - accuracy: 0.9404 - val_loss: 0.1775 - val_accuracy: 0.9494\n",
            "Epoch 249/1000\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.1468 - accuracy: 0.9399 - val_loss: 0.1815 - val_accuracy: 0.9484\n",
            "Epoch 250/1000\n",
            "222/222 [==============================] - 0s 1ms/step - loss: 0.1527 - accuracy: 0.9386 - val_loss: 0.1951 - val_accuracy: 0.9347\n",
            "Epoch 251/1000\n",
            "222/222 [==============================] - 0s 1ms/step - loss: 0.1425 - accuracy: 0.9431 - val_loss: 0.1839 - val_accuracy: 0.9536\n",
            "Epoch 252/1000\n",
            "222/222 [==============================] - 0s 1ms/step - loss: 0.1646 - accuracy: 0.9313 - val_loss: 0.1892 - val_accuracy: 0.9357\n",
            "Epoch 253/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1609 - accuracy: 0.9408 - val_loss: 0.2014 - val_accuracy: 0.9420\n",
            "Epoch 254/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1454 - accuracy: 0.9404 - val_loss: 0.2041 - val_accuracy: 0.9484\n",
            "Epoch 255/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1532 - accuracy: 0.9386 - val_loss: 0.1810 - val_accuracy: 0.9452\n",
            "Epoch 256/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1440 - accuracy: 0.9426 - val_loss: 0.2125 - val_accuracy: 0.9294\n",
            "Epoch 257/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1620 - accuracy: 0.9395 - val_loss: 0.1740 - val_accuracy: 0.9452\n",
            "Epoch 258/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1557 - accuracy: 0.9359 - val_loss: 0.1800 - val_accuracy: 0.9420\n",
            "Epoch 259/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1431 - accuracy: 0.9476 - val_loss: 0.1805 - val_accuracy: 0.9494\n",
            "Epoch 260/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1465 - accuracy: 0.9417 - val_loss: 0.1797 - val_accuracy: 0.9420\n",
            "Epoch 261/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1487 - accuracy: 0.9449 - val_loss: 0.1800 - val_accuracy: 0.9484\n",
            "Epoch 262/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1583 - accuracy: 0.9368 - val_loss: 0.1803 - val_accuracy: 0.9452\n",
            "Epoch 263/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1486 - accuracy: 0.9426 - val_loss: 0.1838 - val_accuracy: 0.9494\n",
            "Epoch 264/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1418 - accuracy: 0.9431 - val_loss: 0.1779 - val_accuracy: 0.9484\n",
            "Epoch 265/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1520 - accuracy: 0.9422 - val_loss: 0.1736 - val_accuracy: 0.9536\n",
            "Epoch 266/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1499 - accuracy: 0.9453 - val_loss: 0.1849 - val_accuracy: 0.9357\n",
            "Epoch 267/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1544 - accuracy: 0.9404 - val_loss: 0.1760 - val_accuracy: 0.9515\n",
            "Epoch 268/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1439 - accuracy: 0.9435 - val_loss: 0.2113 - val_accuracy: 0.9231\n",
            "Epoch 269/1000\n",
            "222/222 [==============================] - 0s 1ms/step - loss: 0.1508 - accuracy: 0.9390 - val_loss: 0.1861 - val_accuracy: 0.9452\n",
            "Epoch 270/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1608 - accuracy: 0.9354 - val_loss: 0.1889 - val_accuracy: 0.9410\n",
            "Epoch 271/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1451 - accuracy: 0.9395 - val_loss: 0.1806 - val_accuracy: 0.9442\n",
            "Epoch 272/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1435 - accuracy: 0.9404 - val_loss: 0.1769 - val_accuracy: 0.9473\n",
            "Epoch 273/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1421 - accuracy: 0.9440 - val_loss: 0.1732 - val_accuracy: 0.9515\n",
            "Epoch 274/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1509 - accuracy: 0.9372 - val_loss: 0.1894 - val_accuracy: 0.9378\n",
            "Epoch 275/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1449 - accuracy: 0.9417 - val_loss: 0.1810 - val_accuracy: 0.9473\n",
            "Epoch 276/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1435 - accuracy: 0.9444 - val_loss: 0.1689 - val_accuracy: 0.9494\n",
            "Epoch 277/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1437 - accuracy: 0.9426 - val_loss: 0.1777 - val_accuracy: 0.9473\n",
            "Epoch 278/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1454 - accuracy: 0.9404 - val_loss: 0.1719 - val_accuracy: 0.9484\n",
            "Epoch 279/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1510 - accuracy: 0.9440 - val_loss: 0.1831 - val_accuracy: 0.9410\n",
            "Epoch 280/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1452 - accuracy: 0.9453 - val_loss: 0.1738 - val_accuracy: 0.9452\n",
            "Epoch 281/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1409 - accuracy: 0.9449 - val_loss: 0.2180 - val_accuracy: 0.9357\n",
            "Epoch 282/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1477 - accuracy: 0.9408 - val_loss: 0.1939 - val_accuracy: 0.9326\n",
            "Epoch 283/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1467 - accuracy: 0.9399 - val_loss: 0.1755 - val_accuracy: 0.9410\n",
            "Epoch 284/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1394 - accuracy: 0.9449 - val_loss: 0.1797 - val_accuracy: 0.9463\n",
            "Epoch 285/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1395 - accuracy: 0.9426 - val_loss: 0.1900 - val_accuracy: 0.9410\n",
            "Epoch 286/1000\n",
            "222/222 [==============================] - 1s 4ms/step - loss: 0.1408 - accuracy: 0.9390 - val_loss: 0.1761 - val_accuracy: 0.9431\n",
            "Epoch 287/1000\n",
            "222/222 [==============================] - 1s 4ms/step - loss: 0.1521 - accuracy: 0.9399 - val_loss: 0.1743 - val_accuracy: 0.9515\n",
            "Epoch 288/1000\n",
            "222/222 [==============================] - 1s 3ms/step - loss: 0.1446 - accuracy: 0.9399 - val_loss: 0.1988 - val_accuracy: 0.9378\n",
            "Epoch 289/1000\n",
            "222/222 [==============================] - 1s 3ms/step - loss: 0.1478 - accuracy: 0.9449 - val_loss: 0.1766 - val_accuracy: 0.9526\n",
            "Epoch 290/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1486 - accuracy: 0.9408 - val_loss: 0.1772 - val_accuracy: 0.9452\n",
            "Epoch 291/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1429 - accuracy: 0.9422 - val_loss: 0.1907 - val_accuracy: 0.9368\n",
            "Epoch 292/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1427 - accuracy: 0.9408 - val_loss: 0.2068 - val_accuracy: 0.9336\n",
            "Epoch 293/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1448 - accuracy: 0.9463 - val_loss: 0.2148 - val_accuracy: 0.9389\n",
            "Epoch 294/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1470 - accuracy: 0.9472 - val_loss: 0.2075 - val_accuracy: 0.9368\n",
            "Epoch 295/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1421 - accuracy: 0.9413 - val_loss: 0.1799 - val_accuracy: 0.9442\n",
            "Epoch 296/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1449 - accuracy: 0.9444 - val_loss: 0.1710 - val_accuracy: 0.9536\n",
            "Epoch 297/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1412 - accuracy: 0.9490 - val_loss: 0.1863 - val_accuracy: 0.9463\n",
            "Epoch 298/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1395 - accuracy: 0.9390 - val_loss: 0.1825 - val_accuracy: 0.9442\n",
            "Epoch 299/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1582 - accuracy: 0.9408 - val_loss: 0.1834 - val_accuracy: 0.9399\n",
            "Epoch 300/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1402 - accuracy: 0.9449 - val_loss: 0.1881 - val_accuracy: 0.9431\n",
            "Epoch 301/1000\n",
            "222/222 [==============================] - 0s 1ms/step - loss: 0.1421 - accuracy: 0.9404 - val_loss: 0.1872 - val_accuracy: 0.9431\n",
            "Epoch 302/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1433 - accuracy: 0.9444 - val_loss: 0.1882 - val_accuracy: 0.9442\n",
            "Epoch 303/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1400 - accuracy: 0.9453 - val_loss: 0.1688 - val_accuracy: 0.9473\n",
            "Epoch 304/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1584 - accuracy: 0.9381 - val_loss: 0.1883 - val_accuracy: 0.9442\n",
            "Epoch 305/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1472 - accuracy: 0.9381 - val_loss: 0.1693 - val_accuracy: 0.9526\n",
            "Epoch 306/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1391 - accuracy: 0.9453 - val_loss: 0.1879 - val_accuracy: 0.9442\n",
            "Epoch 307/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1398 - accuracy: 0.9422 - val_loss: 0.1841 - val_accuracy: 0.9463\n",
            "Epoch 308/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1431 - accuracy: 0.9381 - val_loss: 0.2110 - val_accuracy: 0.9410\n",
            "Epoch 309/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1476 - accuracy: 0.9431 - val_loss: 0.2083 - val_accuracy: 0.9420\n",
            "Epoch 310/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1401 - accuracy: 0.9440 - val_loss: 0.1938 - val_accuracy: 0.9378\n",
            "Epoch 311/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1614 - accuracy: 0.9408 - val_loss: 0.1982 - val_accuracy: 0.9452\n",
            "Epoch 312/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1441 - accuracy: 0.9435 - val_loss: 0.1677 - val_accuracy: 0.9536\n",
            "Epoch 313/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1432 - accuracy: 0.9426 - val_loss: 0.1919 - val_accuracy: 0.9431\n",
            "Epoch 314/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1432 - accuracy: 0.9390 - val_loss: 0.1694 - val_accuracy: 0.9494\n",
            "Epoch 315/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1402 - accuracy: 0.9413 - val_loss: 0.1830 - val_accuracy: 0.9442\n",
            "Epoch 316/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1391 - accuracy: 0.9435 - val_loss: 0.1894 - val_accuracy: 0.9399\n",
            "Epoch 317/1000\n",
            "222/222 [==============================] - 0s 1ms/step - loss: 0.1386 - accuracy: 0.9417 - val_loss: 0.1993 - val_accuracy: 0.9399\n",
            "Epoch 318/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1331 - accuracy: 0.9485 - val_loss: 0.2044 - val_accuracy: 0.9431\n",
            "Epoch 319/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1475 - accuracy: 0.9417 - val_loss: 0.1875 - val_accuracy: 0.9420\n",
            "Epoch 320/1000\n",
            "222/222 [==============================] - 0s 1ms/step - loss: 0.1381 - accuracy: 0.9440 - val_loss: 0.1679 - val_accuracy: 0.9505\n",
            "Epoch 321/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1415 - accuracy: 0.9426 - val_loss: 0.1750 - val_accuracy: 0.9442\n",
            "Epoch 322/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1382 - accuracy: 0.9449 - val_loss: 0.1721 - val_accuracy: 0.9515\n",
            "Epoch 323/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1347 - accuracy: 0.9494 - val_loss: 0.1658 - val_accuracy: 0.9526\n",
            "Epoch 324/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1388 - accuracy: 0.9476 - val_loss: 0.1773 - val_accuracy: 0.9431\n",
            "Epoch 325/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1384 - accuracy: 0.9440 - val_loss: 0.1974 - val_accuracy: 0.9452\n",
            "Epoch 326/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1388 - accuracy: 0.9485 - val_loss: 0.1769 - val_accuracy: 0.9420\n",
            "Epoch 327/1000\n",
            "222/222 [==============================] - 0s 1ms/step - loss: 0.1428 - accuracy: 0.9440 - val_loss: 0.1934 - val_accuracy: 0.9399\n",
            "Epoch 328/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1476 - accuracy: 0.9399 - val_loss: 0.1865 - val_accuracy: 0.9378\n",
            "Epoch 329/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1393 - accuracy: 0.9431 - val_loss: 0.2070 - val_accuracy: 0.9189\n",
            "Epoch 330/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1363 - accuracy: 0.9408 - val_loss: 0.2184 - val_accuracy: 0.9041\n",
            "Epoch 331/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1416 - accuracy: 0.9458 - val_loss: 0.1732 - val_accuracy: 0.9484\n",
            "Epoch 332/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1424 - accuracy: 0.9417 - val_loss: 0.1769 - val_accuracy: 0.9484\n",
            "Epoch 333/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1381 - accuracy: 0.9458 - val_loss: 0.1912 - val_accuracy: 0.9389\n",
            "Epoch 334/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1414 - accuracy: 0.9463 - val_loss: 0.1722 - val_accuracy: 0.9515\n",
            "Epoch 335/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1401 - accuracy: 0.9426 - val_loss: 0.1756 - val_accuracy: 0.9463\n",
            "Epoch 336/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1389 - accuracy: 0.9463 - val_loss: 0.1821 - val_accuracy: 0.9410\n",
            "Epoch 337/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1350 - accuracy: 0.9472 - val_loss: 0.1784 - val_accuracy: 0.9505\n",
            "Epoch 338/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1471 - accuracy: 0.9395 - val_loss: 0.1766 - val_accuracy: 0.9505\n",
            "Epoch 339/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1334 - accuracy: 0.9463 - val_loss: 0.2085 - val_accuracy: 0.9262\n",
            "Epoch 340/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1427 - accuracy: 0.9435 - val_loss: 0.1806 - val_accuracy: 0.9399\n",
            "Epoch 341/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1484 - accuracy: 0.9444 - val_loss: 0.1700 - val_accuracy: 0.9452\n",
            "Epoch 342/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1394 - accuracy: 0.9449 - val_loss: 0.1907 - val_accuracy: 0.9389\n",
            "Epoch 343/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1431 - accuracy: 0.9444 - val_loss: 0.1728 - val_accuracy: 0.9442\n",
            "Epoch 344/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1385 - accuracy: 0.9453 - val_loss: 0.1795 - val_accuracy: 0.9463\n",
            "Epoch 345/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1447 - accuracy: 0.9449 - val_loss: 0.1856 - val_accuracy: 0.9326\n",
            "Epoch 346/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1385 - accuracy: 0.9408 - val_loss: 0.1680 - val_accuracy: 0.9484\n",
            "Epoch 347/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1482 - accuracy: 0.9390 - val_loss: 0.1729 - val_accuracy: 0.9399\n",
            "Epoch 348/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1333 - accuracy: 0.9472 - val_loss: 0.1678 - val_accuracy: 0.9484\n",
            "Epoch 349/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1477 - accuracy: 0.9377 - val_loss: 0.1737 - val_accuracy: 0.9463\n",
            "Epoch 350/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1364 - accuracy: 0.9444 - val_loss: 0.1701 - val_accuracy: 0.9484\n",
            "Epoch 351/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1415 - accuracy: 0.9426 - val_loss: 0.1723 - val_accuracy: 0.9463\n",
            "Epoch 352/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1350 - accuracy: 0.9453 - val_loss: 0.1707 - val_accuracy: 0.9442\n",
            "Epoch 353/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1377 - accuracy: 0.9476 - val_loss: 0.1691 - val_accuracy: 0.9442\n",
            "Epoch 354/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1436 - accuracy: 0.9404 - val_loss: 0.1775 - val_accuracy: 0.9410\n",
            "Epoch 355/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1366 - accuracy: 0.9449 - val_loss: 0.1688 - val_accuracy: 0.9452\n",
            "Epoch 356/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1370 - accuracy: 0.9413 - val_loss: 0.1636 - val_accuracy: 0.9515\n",
            "Epoch 357/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1332 - accuracy: 0.9453 - val_loss: 0.1688 - val_accuracy: 0.9484\n",
            "Epoch 358/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1365 - accuracy: 0.9467 - val_loss: 0.1643 - val_accuracy: 0.9473\n",
            "Epoch 359/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1353 - accuracy: 0.9490 - val_loss: 0.1788 - val_accuracy: 0.9399\n",
            "Epoch 360/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1502 - accuracy: 0.9417 - val_loss: 0.1921 - val_accuracy: 0.9389\n",
            "Epoch 361/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1333 - accuracy: 0.9503 - val_loss: 0.1773 - val_accuracy: 0.9473\n",
            "Epoch 362/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1508 - accuracy: 0.9399 - val_loss: 0.1743 - val_accuracy: 0.9452\n",
            "Epoch 363/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1337 - accuracy: 0.9453 - val_loss: 0.2052 - val_accuracy: 0.9452\n",
            "Epoch 364/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1361 - accuracy: 0.9449 - val_loss: 0.1767 - val_accuracy: 0.9431\n",
            "Epoch 365/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1340 - accuracy: 0.9435 - val_loss: 0.1626 - val_accuracy: 0.9515\n",
            "Epoch 366/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1350 - accuracy: 0.9476 - val_loss: 0.1647 - val_accuracy: 0.9484\n",
            "Epoch 367/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1414 - accuracy: 0.9408 - val_loss: 0.1693 - val_accuracy: 0.9473\n",
            "Epoch 368/1000\n",
            "222/222 [==============================] - 0s 1ms/step - loss: 0.1342 - accuracy: 0.9485 - val_loss: 0.1928 - val_accuracy: 0.9410\n",
            "Epoch 369/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1326 - accuracy: 0.9463 - val_loss: 0.1830 - val_accuracy: 0.9484\n",
            "Epoch 370/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1315 - accuracy: 0.9458 - val_loss: 0.1844 - val_accuracy: 0.9378\n",
            "Epoch 371/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1324 - accuracy: 0.9499 - val_loss: 0.1797 - val_accuracy: 0.9420\n",
            "Epoch 372/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1378 - accuracy: 0.9467 - val_loss: 0.1749 - val_accuracy: 0.9420\n",
            "Epoch 373/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1348 - accuracy: 0.9458 - val_loss: 0.1836 - val_accuracy: 0.9484\n",
            "Epoch 374/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1444 - accuracy: 0.9431 - val_loss: 0.1663 - val_accuracy: 0.9494\n",
            "Epoch 375/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1355 - accuracy: 0.9444 - val_loss: 0.1638 - val_accuracy: 0.9505\n",
            "Epoch 376/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1424 - accuracy: 0.9431 - val_loss: 0.1677 - val_accuracy: 0.9515\n",
            "Epoch 377/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1373 - accuracy: 0.9449 - val_loss: 0.1631 - val_accuracy: 0.9452\n",
            "Epoch 378/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1383 - accuracy: 0.9449 - val_loss: 0.1909 - val_accuracy: 0.9283\n",
            "Epoch 379/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1281 - accuracy: 0.9463 - val_loss: 0.1721 - val_accuracy: 0.9452\n",
            "Epoch 380/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1363 - accuracy: 0.9458 - val_loss: 0.1818 - val_accuracy: 0.9452\n",
            "Epoch 381/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1338 - accuracy: 0.9449 - val_loss: 0.1961 - val_accuracy: 0.9389\n",
            "Epoch 382/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1330 - accuracy: 0.9444 - val_loss: 0.1773 - val_accuracy: 0.9431\n",
            "Epoch 383/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1372 - accuracy: 0.9453 - val_loss: 0.2054 - val_accuracy: 0.9442\n",
            "Epoch 384/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1389 - accuracy: 0.9404 - val_loss: 0.1709 - val_accuracy: 0.9442\n",
            "Epoch 385/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1345 - accuracy: 0.9435 - val_loss: 0.1848 - val_accuracy: 0.9305\n",
            "Epoch 386/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1356 - accuracy: 0.9467 - val_loss: 0.1701 - val_accuracy: 0.9484\n",
            "Epoch 387/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1348 - accuracy: 0.9467 - val_loss: 0.1755 - val_accuracy: 0.9442\n",
            "Epoch 388/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1430 - accuracy: 0.9444 - val_loss: 0.1886 - val_accuracy: 0.9399\n",
            "Epoch 389/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1369 - accuracy: 0.9481 - val_loss: 0.1710 - val_accuracy: 0.9515\n",
            "Epoch 390/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1343 - accuracy: 0.9449 - val_loss: 0.1977 - val_accuracy: 0.9305\n",
            "Epoch 391/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1352 - accuracy: 0.9458 - val_loss: 0.1660 - val_accuracy: 0.9494\n",
            "Epoch 392/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1358 - accuracy: 0.9426 - val_loss: 0.1756 - val_accuracy: 0.9526\n",
            "Epoch 393/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1398 - accuracy: 0.9413 - val_loss: 0.1756 - val_accuracy: 0.9431\n",
            "Epoch 394/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1360 - accuracy: 0.9431 - val_loss: 0.1663 - val_accuracy: 0.9463\n",
            "Epoch 395/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1330 - accuracy: 0.9481 - val_loss: 0.1754 - val_accuracy: 0.9410\n",
            "Epoch 396/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1288 - accuracy: 0.9481 - val_loss: 0.1662 - val_accuracy: 0.9526\n",
            "Epoch 397/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1333 - accuracy: 0.9494 - val_loss: 0.1811 - val_accuracy: 0.9452\n",
            "Epoch 398/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1320 - accuracy: 0.9499 - val_loss: 0.1776 - val_accuracy: 0.9431\n",
            "Epoch 399/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1349 - accuracy: 0.9449 - val_loss: 0.1668 - val_accuracy: 0.9526\n",
            "Epoch 400/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1431 - accuracy: 0.9417 - val_loss: 0.1705 - val_accuracy: 0.9494\n",
            "Epoch 401/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1320 - accuracy: 0.9467 - val_loss: 0.1910 - val_accuracy: 0.9494\n",
            "Epoch 402/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1374 - accuracy: 0.9426 - val_loss: 0.1815 - val_accuracy: 0.9463\n",
            "Epoch 403/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1388 - accuracy: 0.9422 - val_loss: 0.1909 - val_accuracy: 0.9410\n",
            "Epoch 404/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1334 - accuracy: 0.9463 - val_loss: 0.1747 - val_accuracy: 0.9484\n",
            "Epoch 405/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1324 - accuracy: 0.9467 - val_loss: 0.2012 - val_accuracy: 0.9410\n",
            "Epoch 406/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1380 - accuracy: 0.9499 - val_loss: 0.1835 - val_accuracy: 0.9410\n",
            "Epoch 407/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1259 - accuracy: 0.9476 - val_loss: 0.1769 - val_accuracy: 0.9484\n",
            "Epoch 408/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1381 - accuracy: 0.9435 - val_loss: 0.1649 - val_accuracy: 0.9452\n",
            "Epoch 409/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1353 - accuracy: 0.9440 - val_loss: 0.1647 - val_accuracy: 0.9505\n",
            "Epoch 410/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1336 - accuracy: 0.9476 - val_loss: 0.1662 - val_accuracy: 0.9505\n",
            "Epoch 411/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1317 - accuracy: 0.9472 - val_loss: 0.1703 - val_accuracy: 0.9463\n",
            "Epoch 412/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1339 - accuracy: 0.9472 - val_loss: 0.1771 - val_accuracy: 0.9463\n",
            "Epoch 413/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1329 - accuracy: 0.9517 - val_loss: 0.1748 - val_accuracy: 0.9420\n",
            "Epoch 414/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1391 - accuracy: 0.9413 - val_loss: 0.1834 - val_accuracy: 0.9452\n",
            "Epoch 415/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1285 - accuracy: 0.9485 - val_loss: 0.1759 - val_accuracy: 0.9484\n",
            "Epoch 416/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1326 - accuracy: 0.9472 - val_loss: 0.1687 - val_accuracy: 0.9515\n",
            "Epoch 417/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1424 - accuracy: 0.9413 - val_loss: 0.1739 - val_accuracy: 0.9484\n",
            "Epoch 418/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1537 - accuracy: 0.9422 - val_loss: 0.1838 - val_accuracy: 0.9399\n",
            "Epoch 419/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1424 - accuracy: 0.9422 - val_loss: 0.1701 - val_accuracy: 0.9442\n",
            "Epoch 420/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1344 - accuracy: 0.9458 - val_loss: 0.1728 - val_accuracy: 0.9463\n",
            "Epoch 421/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1289 - accuracy: 0.9494 - val_loss: 0.1734 - val_accuracy: 0.9399\n",
            "Epoch 422/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1266 - accuracy: 0.9499 - val_loss: 0.1820 - val_accuracy: 0.9463\n",
            "Epoch 423/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1367 - accuracy: 0.9453 - val_loss: 0.1887 - val_accuracy: 0.9336\n",
            "Epoch 424/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1440 - accuracy: 0.9426 - val_loss: 0.1908 - val_accuracy: 0.9420\n",
            "Epoch 425/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1335 - accuracy: 0.9490 - val_loss: 0.1742 - val_accuracy: 0.9547\n",
            "Epoch 426/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1373 - accuracy: 0.9426 - val_loss: 0.1719 - val_accuracy: 0.9526\n",
            "Epoch 427/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1374 - accuracy: 0.9476 - val_loss: 0.1625 - val_accuracy: 0.9536\n",
            "Epoch 428/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1306 - accuracy: 0.9472 - val_loss: 0.1905 - val_accuracy: 0.9336\n",
            "Epoch 429/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1279 - accuracy: 0.9476 - val_loss: 0.1687 - val_accuracy: 0.9473\n",
            "Epoch 430/1000\n",
            "222/222 [==============================] - 1s 3ms/step - loss: 0.1449 - accuracy: 0.9453 - val_loss: 0.1699 - val_accuracy: 0.9515\n",
            "Epoch 431/1000\n",
            "222/222 [==============================] - 1s 3ms/step - loss: 0.1316 - accuracy: 0.9512 - val_loss: 0.1800 - val_accuracy: 0.9484\n",
            "Epoch 432/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1355 - accuracy: 0.9444 - val_loss: 0.1766 - val_accuracy: 0.9494\n",
            "Epoch 433/1000\n",
            "222/222 [==============================] - 1s 3ms/step - loss: 0.1318 - accuracy: 0.9512 - val_loss: 0.1796 - val_accuracy: 0.9515\n",
            "Epoch 434/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1293 - accuracy: 0.9467 - val_loss: 0.1907 - val_accuracy: 0.9368\n",
            "Epoch 435/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1468 - accuracy: 0.9431 - val_loss: 0.1751 - val_accuracy: 0.9463\n",
            "Epoch 436/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1314 - accuracy: 0.9490 - val_loss: 0.1892 - val_accuracy: 0.9336\n",
            "Epoch 437/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1364 - accuracy: 0.9476 - val_loss: 0.1593 - val_accuracy: 0.9515\n",
            "Epoch 438/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1354 - accuracy: 0.9472 - val_loss: 0.1724 - val_accuracy: 0.9484\n",
            "Epoch 439/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1402 - accuracy: 0.9417 - val_loss: 0.1750 - val_accuracy: 0.9442\n",
            "Epoch 440/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1344 - accuracy: 0.9467 - val_loss: 0.1832 - val_accuracy: 0.9420\n",
            "Epoch 441/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1349 - accuracy: 0.9458 - val_loss: 0.1822 - val_accuracy: 0.9410\n",
            "Epoch 442/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1333 - accuracy: 0.9472 - val_loss: 0.1796 - val_accuracy: 0.9484\n",
            "Epoch 443/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1432 - accuracy: 0.9458 - val_loss: 0.1801 - val_accuracy: 0.9463\n",
            "Epoch 444/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1329 - accuracy: 0.9431 - val_loss: 0.1786 - val_accuracy: 0.9484\n",
            "Epoch 445/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1361 - accuracy: 0.9463 - val_loss: 0.1778 - val_accuracy: 0.9505\n",
            "Epoch 446/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1481 - accuracy: 0.9404 - val_loss: 0.2320 - val_accuracy: 0.8999\n",
            "Epoch 447/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1313 - accuracy: 0.9449 - val_loss: 0.1642 - val_accuracy: 0.9494\n",
            "Epoch 448/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1294 - accuracy: 0.9499 - val_loss: 0.1827 - val_accuracy: 0.9473\n",
            "Epoch 449/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1366 - accuracy: 0.9453 - val_loss: 0.1935 - val_accuracy: 0.9399\n",
            "Epoch 450/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1371 - accuracy: 0.9453 - val_loss: 0.1802 - val_accuracy: 0.9431\n",
            "Epoch 451/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1307 - accuracy: 0.9512 - val_loss: 0.1668 - val_accuracy: 0.9473\n",
            "Epoch 452/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1268 - accuracy: 0.9485 - val_loss: 0.1727 - val_accuracy: 0.9505\n",
            "Epoch 453/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1298 - accuracy: 0.9467 - val_loss: 0.1839 - val_accuracy: 0.9368\n",
            "Epoch 454/1000\n",
            "222/222 [==============================] - 1s 3ms/step - loss: 0.1335 - accuracy: 0.9449 - val_loss: 0.1697 - val_accuracy: 0.9420\n",
            "Epoch 455/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1351 - accuracy: 0.9422 - val_loss: 0.1870 - val_accuracy: 0.9389\n",
            "Epoch 456/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1328 - accuracy: 0.9517 - val_loss: 0.1667 - val_accuracy: 0.9494\n",
            "Epoch 457/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1459 - accuracy: 0.9435 - val_loss: 0.1627 - val_accuracy: 0.9536\n",
            "Epoch 458/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1358 - accuracy: 0.9458 - val_loss: 0.1753 - val_accuracy: 0.9452\n",
            "Epoch 459/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1367 - accuracy: 0.9449 - val_loss: 0.1716 - val_accuracy: 0.9505\n",
            "Epoch 460/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1327 - accuracy: 0.9476 - val_loss: 0.1946 - val_accuracy: 0.9410\n",
            "Epoch 461/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1242 - accuracy: 0.9499 - val_loss: 0.1745 - val_accuracy: 0.9505\n",
            "Epoch 462/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1287 - accuracy: 0.9444 - val_loss: 0.1671 - val_accuracy: 0.9484\n",
            "Epoch 463/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1317 - accuracy: 0.9431 - val_loss: 0.1934 - val_accuracy: 0.9368\n",
            "Epoch 464/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1289 - accuracy: 0.9485 - val_loss: 0.1887 - val_accuracy: 0.9494\n",
            "Epoch 465/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1313 - accuracy: 0.9499 - val_loss: 0.1709 - val_accuracy: 0.9484\n",
            "Epoch 466/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1355 - accuracy: 0.9431 - val_loss: 0.1940 - val_accuracy: 0.9326\n",
            "Epoch 467/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1427 - accuracy: 0.9422 - val_loss: 0.1751 - val_accuracy: 0.9484\n",
            "Epoch 468/1000\n",
            "222/222 [==============================] - 0s 1ms/step - loss: 0.1278 - accuracy: 0.9485 - val_loss: 0.1824 - val_accuracy: 0.9473\n",
            "Epoch 469/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1262 - accuracy: 0.9449 - val_loss: 0.1995 - val_accuracy: 0.9241\n",
            "Epoch 470/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1252 - accuracy: 0.9517 - val_loss: 0.2286 - val_accuracy: 0.9073\n",
            "Epoch 471/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1481 - accuracy: 0.9417 - val_loss: 0.1931 - val_accuracy: 0.9431\n",
            "Epoch 472/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1319 - accuracy: 0.9467 - val_loss: 0.1783 - val_accuracy: 0.9431\n",
            "Epoch 473/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1329 - accuracy: 0.9458 - val_loss: 0.2433 - val_accuracy: 0.8915\n",
            "Epoch 474/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1287 - accuracy: 0.9467 - val_loss: 0.1744 - val_accuracy: 0.9420\n",
            "Epoch 475/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1297 - accuracy: 0.9490 - val_loss: 0.1711 - val_accuracy: 0.9442\n",
            "Epoch 476/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1287 - accuracy: 0.9463 - val_loss: 0.1703 - val_accuracy: 0.9494\n",
            "Epoch 477/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1274 - accuracy: 0.9490 - val_loss: 0.1753 - val_accuracy: 0.9494\n",
            "Epoch 478/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1360 - accuracy: 0.9485 - val_loss: 0.1905 - val_accuracy: 0.9294\n",
            "Epoch 479/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1307 - accuracy: 0.9449 - val_loss: 0.1775 - val_accuracy: 0.9442\n",
            "Epoch 480/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1285 - accuracy: 0.9472 - val_loss: 0.1884 - val_accuracy: 0.9463\n",
            "Epoch 481/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1317 - accuracy: 0.9490 - val_loss: 0.1667 - val_accuracy: 0.9494\n",
            "Epoch 482/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1307 - accuracy: 0.9453 - val_loss: 0.1801 - val_accuracy: 0.9452\n",
            "Epoch 483/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1347 - accuracy: 0.9426 - val_loss: 0.1736 - val_accuracy: 0.9463\n",
            "Epoch 484/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1343 - accuracy: 0.9472 - val_loss: 0.1880 - val_accuracy: 0.9505\n",
            "Epoch 485/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1363 - accuracy: 0.9417 - val_loss: 0.1656 - val_accuracy: 0.9505\n",
            "Epoch 486/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1329 - accuracy: 0.9463 - val_loss: 0.1784 - val_accuracy: 0.9442\n",
            "Epoch 487/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1373 - accuracy: 0.9458 - val_loss: 0.1893 - val_accuracy: 0.9420\n",
            "Epoch 488/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1346 - accuracy: 0.9453 - val_loss: 0.1730 - val_accuracy: 0.9526\n",
            "Epoch 489/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1274 - accuracy: 0.9458 - val_loss: 0.1822 - val_accuracy: 0.9431\n",
            "Epoch 490/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1254 - accuracy: 0.9476 - val_loss: 0.1656 - val_accuracy: 0.9463\n",
            "Epoch 491/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1317 - accuracy: 0.9476 - val_loss: 0.1642 - val_accuracy: 0.9484\n",
            "Epoch 492/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1354 - accuracy: 0.9463 - val_loss: 0.1793 - val_accuracy: 0.9463\n",
            "Epoch 493/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1252 - accuracy: 0.9485 - val_loss: 0.1841 - val_accuracy: 0.9484\n",
            "Epoch 494/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1347 - accuracy: 0.9485 - val_loss: 0.2150 - val_accuracy: 0.9347\n",
            "Epoch 495/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1301 - accuracy: 0.9490 - val_loss: 0.1853 - val_accuracy: 0.9431\n",
            "Epoch 496/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1301 - accuracy: 0.9449 - val_loss: 0.1682 - val_accuracy: 0.9526\n",
            "Epoch 497/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1305 - accuracy: 0.9485 - val_loss: 0.1652 - val_accuracy: 0.9452\n",
            "Epoch 498/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1297 - accuracy: 0.9444 - val_loss: 0.1867 - val_accuracy: 0.9473\n",
            "Epoch 499/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1315 - accuracy: 0.9512 - val_loss: 0.2152 - val_accuracy: 0.9368\n",
            "Epoch 500/1000\n",
            "222/222 [==============================] - 1s 5ms/step - loss: 0.1287 - accuracy: 0.9494 - val_loss: 0.2075 - val_accuracy: 0.9357\n",
            "Epoch 501/1000\n",
            "222/222 [==============================] - 1s 5ms/step - loss: 0.1357 - accuracy: 0.9422 - val_loss: 0.1662 - val_accuracy: 0.9452\n",
            "Epoch 502/1000\n",
            "222/222 [==============================] - 1s 4ms/step - loss: 0.1246 - accuracy: 0.9508 - val_loss: 0.1871 - val_accuracy: 0.9442\n",
            "Epoch 503/1000\n",
            "222/222 [==============================] - 1s 3ms/step - loss: 0.1306 - accuracy: 0.9435 - val_loss: 0.2038 - val_accuracy: 0.9315\n",
            "Epoch 504/1000\n",
            "222/222 [==============================] - 1s 4ms/step - loss: 0.1258 - accuracy: 0.9508 - val_loss: 0.1646 - val_accuracy: 0.9505\n",
            "Epoch 505/1000\n",
            "222/222 [==============================] - 1s 5ms/step - loss: 0.1450 - accuracy: 0.9408 - val_loss: 0.1743 - val_accuracy: 0.9431\n",
            "Epoch 506/1000\n",
            "222/222 [==============================] - 1s 3ms/step - loss: 0.1294 - accuracy: 0.9494 - val_loss: 0.1691 - val_accuracy: 0.9452\n",
            "Epoch 507/1000\n",
            "222/222 [==============================] - 1s 3ms/step - loss: 0.1326 - accuracy: 0.9463 - val_loss: 0.1659 - val_accuracy: 0.9505\n",
            "Epoch 508/1000\n",
            "222/222 [==============================] - 1s 3ms/step - loss: 0.1291 - accuracy: 0.9485 - val_loss: 0.1740 - val_accuracy: 0.9505\n",
            "Epoch 509/1000\n",
            "222/222 [==============================] - 1s 3ms/step - loss: 0.1287 - accuracy: 0.9472 - val_loss: 0.1697 - val_accuracy: 0.9452\n",
            "Epoch 510/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1352 - accuracy: 0.9481 - val_loss: 0.1803 - val_accuracy: 0.9420\n",
            "Epoch 511/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1335 - accuracy: 0.9458 - val_loss: 0.1702 - val_accuracy: 0.9484\n",
            "Epoch 512/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1326 - accuracy: 0.9458 - val_loss: 0.1699 - val_accuracy: 0.9547\n",
            "Epoch 513/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1283 - accuracy: 0.9490 - val_loss: 0.1624 - val_accuracy: 0.9494\n",
            "Epoch 514/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1377 - accuracy: 0.9426 - val_loss: 0.1733 - val_accuracy: 0.9463\n",
            "Epoch 515/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1322 - accuracy: 0.9476 - val_loss: 0.1681 - val_accuracy: 0.9494\n",
            "Epoch 516/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1299 - accuracy: 0.9444 - val_loss: 0.1630 - val_accuracy: 0.9547\n",
            "Epoch 517/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1272 - accuracy: 0.9521 - val_loss: 0.1911 - val_accuracy: 0.9336\n",
            "Epoch 518/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1407 - accuracy: 0.9431 - val_loss: 0.1795 - val_accuracy: 0.9515\n",
            "Epoch 519/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1334 - accuracy: 0.9467 - val_loss: 0.1861 - val_accuracy: 0.9357\n",
            "Epoch 520/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1262 - accuracy: 0.9494 - val_loss: 0.1806 - val_accuracy: 0.9463\n",
            "Epoch 521/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1270 - accuracy: 0.9535 - val_loss: 0.2102 - val_accuracy: 0.9168\n",
            "Epoch 522/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1351 - accuracy: 0.9458 - val_loss: 0.1686 - val_accuracy: 0.9568\n",
            "Epoch 523/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1478 - accuracy: 0.9426 - val_loss: 0.1840 - val_accuracy: 0.9505\n",
            "Epoch 524/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1311 - accuracy: 0.9476 - val_loss: 0.1792 - val_accuracy: 0.9473\n",
            "Epoch 525/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1295 - accuracy: 0.9530 - val_loss: 0.1679 - val_accuracy: 0.9463\n",
            "Epoch 526/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1334 - accuracy: 0.9467 - val_loss: 0.1734 - val_accuracy: 0.9463\n",
            "Epoch 527/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1289 - accuracy: 0.9453 - val_loss: 0.1823 - val_accuracy: 0.9420\n",
            "Epoch 528/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1298 - accuracy: 0.9508 - val_loss: 0.1763 - val_accuracy: 0.9410\n",
            "Epoch 529/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1309 - accuracy: 0.9472 - val_loss: 0.1768 - val_accuracy: 0.9473\n",
            "Epoch 530/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1265 - accuracy: 0.9472 - val_loss: 0.1727 - val_accuracy: 0.9484\n",
            "Epoch 531/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1308 - accuracy: 0.9472 - val_loss: 0.1751 - val_accuracy: 0.9547\n",
            "Epoch 532/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1264 - accuracy: 0.9485 - val_loss: 0.1884 - val_accuracy: 0.9326\n",
            "Epoch 533/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1289 - accuracy: 0.9463 - val_loss: 0.1681 - val_accuracy: 0.9515\n",
            "Epoch 534/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1332 - accuracy: 0.9463 - val_loss: 0.1666 - val_accuracy: 0.9484\n",
            "Epoch 535/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1413 - accuracy: 0.9458 - val_loss: 0.1764 - val_accuracy: 0.9473\n",
            "Epoch 536/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1408 - accuracy: 0.9463 - val_loss: 0.1720 - val_accuracy: 0.9494\n",
            "Epoch 537/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1271 - accuracy: 0.9490 - val_loss: 0.1707 - val_accuracy: 0.9494\n",
            "Epoch 538/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1296 - accuracy: 0.9503 - val_loss: 0.1787 - val_accuracy: 0.9378\n",
            "Epoch 539/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1273 - accuracy: 0.9449 - val_loss: 0.2065 - val_accuracy: 0.9210\n",
            "Epoch 540/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1254 - accuracy: 0.9499 - val_loss: 0.1931 - val_accuracy: 0.9431\n",
            "Epoch 541/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1340 - accuracy: 0.9440 - val_loss: 0.1896 - val_accuracy: 0.9473\n",
            "Epoch 542/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1272 - accuracy: 0.9476 - val_loss: 0.1702 - val_accuracy: 0.9494\n",
            "Epoch 543/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1288 - accuracy: 0.9508 - val_loss: 0.1958 - val_accuracy: 0.9399\n",
            "Epoch 544/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1319 - accuracy: 0.9485 - val_loss: 0.1695 - val_accuracy: 0.9515\n",
            "Epoch 545/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1393 - accuracy: 0.9463 - val_loss: 0.1807 - val_accuracy: 0.9473\n",
            "Epoch 546/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1301 - accuracy: 0.9449 - val_loss: 0.1832 - val_accuracy: 0.9389\n",
            "Epoch 547/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1268 - accuracy: 0.9490 - val_loss: 0.1625 - val_accuracy: 0.9505\n",
            "Epoch 548/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1339 - accuracy: 0.9458 - val_loss: 0.1762 - val_accuracy: 0.9452\n",
            "Epoch 549/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1283 - accuracy: 0.9485 - val_loss: 0.1825 - val_accuracy: 0.9420\n",
            "Epoch 550/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1318 - accuracy: 0.9485 - val_loss: 0.1753 - val_accuracy: 0.9473\n",
            "Epoch 551/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1347 - accuracy: 0.9481 - val_loss: 0.1732 - val_accuracy: 0.9505\n",
            "Epoch 552/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1229 - accuracy: 0.9503 - val_loss: 0.1768 - val_accuracy: 0.9452\n",
            "Epoch 553/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1315 - accuracy: 0.9431 - val_loss: 0.1750 - val_accuracy: 0.9442\n",
            "Epoch 554/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1359 - accuracy: 0.9458 - val_loss: 0.1831 - val_accuracy: 0.9452\n",
            "Epoch 555/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1258 - accuracy: 0.9458 - val_loss: 0.2169 - val_accuracy: 0.9442\n",
            "Epoch 556/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1332 - accuracy: 0.9490 - val_loss: 0.1648 - val_accuracy: 0.9494\n",
            "Epoch 557/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1220 - accuracy: 0.9526 - val_loss: 0.1964 - val_accuracy: 0.9399\n",
            "Epoch 558/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1307 - accuracy: 0.9481 - val_loss: 0.1646 - val_accuracy: 0.9515\n",
            "Epoch 559/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1252 - accuracy: 0.9458 - val_loss: 0.1722 - val_accuracy: 0.9526\n",
            "Epoch 560/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1256 - accuracy: 0.9526 - val_loss: 0.1691 - val_accuracy: 0.9463\n",
            "Epoch 561/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1326 - accuracy: 0.9467 - val_loss: 0.1736 - val_accuracy: 0.9442\n",
            "Epoch 562/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1318 - accuracy: 0.9472 - val_loss: 0.1640 - val_accuracy: 0.9463\n",
            "Epoch 563/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1302 - accuracy: 0.9472 - val_loss: 0.1664 - val_accuracy: 0.9452\n",
            "Epoch 564/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1293 - accuracy: 0.9481 - val_loss: 0.1797 - val_accuracy: 0.9452\n",
            "Epoch 565/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1369 - accuracy: 0.9458 - val_loss: 0.1643 - val_accuracy: 0.9557\n",
            "Epoch 566/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1254 - accuracy: 0.9494 - val_loss: 0.1689 - val_accuracy: 0.9473\n",
            "Epoch 567/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1418 - accuracy: 0.9458 - val_loss: 0.1737 - val_accuracy: 0.9452\n",
            "Epoch 568/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1333 - accuracy: 0.9472 - val_loss: 0.1872 - val_accuracy: 0.9431\n",
            "Epoch 569/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1279 - accuracy: 0.9508 - val_loss: 0.1686 - val_accuracy: 0.9484\n",
            "Epoch 570/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1281 - accuracy: 0.9481 - val_loss: 0.1748 - val_accuracy: 0.9442\n",
            "Epoch 571/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1275 - accuracy: 0.9453 - val_loss: 0.1666 - val_accuracy: 0.9557\n",
            "Epoch 572/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1276 - accuracy: 0.9463 - val_loss: 0.1771 - val_accuracy: 0.9463\n",
            "Epoch 573/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1336 - accuracy: 0.9467 - val_loss: 0.1857 - val_accuracy: 0.9431\n",
            "Epoch 574/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1392 - accuracy: 0.9449 - val_loss: 0.1973 - val_accuracy: 0.9410\n",
            "Epoch 575/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1266 - accuracy: 0.9440 - val_loss: 0.1836 - val_accuracy: 0.9420\n",
            "Epoch 576/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1256 - accuracy: 0.9503 - val_loss: 0.1862 - val_accuracy: 0.9484\n",
            "Epoch 577/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1347 - accuracy: 0.9463 - val_loss: 0.1844 - val_accuracy: 0.9452\n",
            "Epoch 578/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1297 - accuracy: 0.9435 - val_loss: 0.1840 - val_accuracy: 0.9410\n",
            "Epoch 579/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1268 - accuracy: 0.9499 - val_loss: 0.1998 - val_accuracy: 0.9378\n",
            "Epoch 580/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1312 - accuracy: 0.9449 - val_loss: 0.1748 - val_accuracy: 0.9547\n",
            "Epoch 581/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1301 - accuracy: 0.9453 - val_loss: 0.1732 - val_accuracy: 0.9452\n",
            "Epoch 582/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1337 - accuracy: 0.9431 - val_loss: 0.2049 - val_accuracy: 0.9452\n",
            "Epoch 583/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1300 - accuracy: 0.9490 - val_loss: 0.1828 - val_accuracy: 0.9431\n",
            "Epoch 584/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1233 - accuracy: 0.9490 - val_loss: 0.1831 - val_accuracy: 0.9484\n",
            "Epoch 585/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1344 - accuracy: 0.9463 - val_loss: 0.1740 - val_accuracy: 0.9526\n",
            "Epoch 586/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1339 - accuracy: 0.9435 - val_loss: 0.1884 - val_accuracy: 0.9515\n",
            "Epoch 587/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1242 - accuracy: 0.9530 - val_loss: 0.1872 - val_accuracy: 0.9389\n",
            "Epoch 588/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1356 - accuracy: 0.9476 - val_loss: 0.2114 - val_accuracy: 0.9210\n",
            "Epoch 589/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1307 - accuracy: 0.9490 - val_loss: 0.1761 - val_accuracy: 0.9505\n",
            "Epoch 590/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1265 - accuracy: 0.9449 - val_loss: 0.1776 - val_accuracy: 0.9431\n",
            "Epoch 591/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1253 - accuracy: 0.9521 - val_loss: 0.1790 - val_accuracy: 0.9463\n",
            "Epoch 592/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1248 - accuracy: 0.9508 - val_loss: 0.1735 - val_accuracy: 0.9484\n",
            "Epoch 593/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1232 - accuracy: 0.9503 - val_loss: 0.1939 - val_accuracy: 0.9378\n",
            "Epoch 594/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1251 - accuracy: 0.9490 - val_loss: 0.1786 - val_accuracy: 0.9452\n",
            "Epoch 595/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1406 - accuracy: 0.9444 - val_loss: 0.1732 - val_accuracy: 0.9463\n",
            "Epoch 596/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1246 - accuracy: 0.9485 - val_loss: 0.1683 - val_accuracy: 0.9484\n",
            "Epoch 597/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1438 - accuracy: 0.9417 - val_loss: 0.1776 - val_accuracy: 0.9463\n",
            "Epoch 598/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1329 - accuracy: 0.9490 - val_loss: 0.2023 - val_accuracy: 0.9378\n",
            "Epoch 599/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1247 - accuracy: 0.9503 - val_loss: 0.1895 - val_accuracy: 0.9452\n",
            "Epoch 600/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1213 - accuracy: 0.9490 - val_loss: 0.2161 - val_accuracy: 0.9431\n",
            "Epoch 601/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1214 - accuracy: 0.9494 - val_loss: 0.1762 - val_accuracy: 0.9515\n",
            "Epoch 602/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1288 - accuracy: 0.9467 - val_loss: 0.2541 - val_accuracy: 0.9326\n",
            "Epoch 603/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1390 - accuracy: 0.9472 - val_loss: 0.1963 - val_accuracy: 0.9420\n",
            "Epoch 604/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1283 - accuracy: 0.9476 - val_loss: 0.1869 - val_accuracy: 0.9410\n",
            "Epoch 605/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1317 - accuracy: 0.9435 - val_loss: 0.2077 - val_accuracy: 0.9399\n",
            "Epoch 606/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1341 - accuracy: 0.9490 - val_loss: 0.1870 - val_accuracy: 0.9378\n",
            "Epoch 607/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1254 - accuracy: 0.9494 - val_loss: 0.1781 - val_accuracy: 0.9494\n",
            "Epoch 608/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1261 - accuracy: 0.9458 - val_loss: 0.1682 - val_accuracy: 0.9452\n",
            "Epoch 609/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1222 - accuracy: 0.9548 - val_loss: 0.1779 - val_accuracy: 0.9389\n",
            "Epoch 610/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1376 - accuracy: 0.9431 - val_loss: 0.1780 - val_accuracy: 0.9442\n",
            "Epoch 611/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1271 - accuracy: 0.9485 - val_loss: 0.1796 - val_accuracy: 0.9505\n",
            "Epoch 612/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1311 - accuracy: 0.9476 - val_loss: 0.1865 - val_accuracy: 0.9515\n",
            "Epoch 613/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1289 - accuracy: 0.9512 - val_loss: 0.1750 - val_accuracy: 0.9463\n",
            "Epoch 614/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1283 - accuracy: 0.9485 - val_loss: 0.1740 - val_accuracy: 0.9484\n",
            "Epoch 615/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1362 - accuracy: 0.9426 - val_loss: 0.1804 - val_accuracy: 0.9452\n",
            "Epoch 616/1000\n",
            "222/222 [==============================] - 1s 3ms/step - loss: 0.1264 - accuracy: 0.9512 - val_loss: 0.2056 - val_accuracy: 0.9442\n",
            "Epoch 617/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1325 - accuracy: 0.9485 - val_loss: 0.1875 - val_accuracy: 0.9410\n",
            "Epoch 618/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1234 - accuracy: 0.9476 - val_loss: 0.1651 - val_accuracy: 0.9484\n",
            "Epoch 619/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1272 - accuracy: 0.9463 - val_loss: 0.1737 - val_accuracy: 0.9515\n",
            "Epoch 620/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1272 - accuracy: 0.9490 - val_loss: 0.1853 - val_accuracy: 0.9399\n",
            "Epoch 621/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1253 - accuracy: 0.9481 - val_loss: 0.1755 - val_accuracy: 0.9473\n",
            "Epoch 622/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1262 - accuracy: 0.9476 - val_loss: 0.1759 - val_accuracy: 0.9431\n",
            "Epoch 623/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1202 - accuracy: 0.9539 - val_loss: 0.1927 - val_accuracy: 0.9305\n",
            "Epoch 624/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1259 - accuracy: 0.9481 - val_loss: 0.1791 - val_accuracy: 0.9536\n",
            "Epoch 625/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1260 - accuracy: 0.9508 - val_loss: 0.3194 - val_accuracy: 0.9210\n",
            "Epoch 626/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1330 - accuracy: 0.9508 - val_loss: 0.1735 - val_accuracy: 0.9494\n",
            "Epoch 627/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1333 - accuracy: 0.9485 - val_loss: 0.1994 - val_accuracy: 0.9389\n",
            "Epoch 628/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1274 - accuracy: 0.9490 - val_loss: 0.1811 - val_accuracy: 0.9442\n",
            "Epoch 629/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1334 - accuracy: 0.9453 - val_loss: 0.1726 - val_accuracy: 0.9526\n",
            "Epoch 630/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1275 - accuracy: 0.9458 - val_loss: 0.2496 - val_accuracy: 0.9347\n",
            "Epoch 631/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1337 - accuracy: 0.9431 - val_loss: 0.1823 - val_accuracy: 0.9494\n",
            "Epoch 632/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1295 - accuracy: 0.9472 - val_loss: 0.1934 - val_accuracy: 0.9305\n",
            "Epoch 633/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1273 - accuracy: 0.9490 - val_loss: 0.1733 - val_accuracy: 0.9494\n",
            "Epoch 634/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1291 - accuracy: 0.9499 - val_loss: 0.1741 - val_accuracy: 0.9452\n",
            "Epoch 635/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1309 - accuracy: 0.9508 - val_loss: 0.1813 - val_accuracy: 0.9452\n",
            "Epoch 636/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1246 - accuracy: 0.9485 - val_loss: 0.1852 - val_accuracy: 0.9442\n",
            "Epoch 637/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1248 - accuracy: 0.9503 - val_loss: 0.1910 - val_accuracy: 0.9399\n",
            "Epoch 638/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1239 - accuracy: 0.9481 - val_loss: 0.1686 - val_accuracy: 0.9494\n",
            "Epoch 639/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1234 - accuracy: 0.9508 - val_loss: 0.1804 - val_accuracy: 0.9368\n",
            "Epoch 640/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1277 - accuracy: 0.9485 - val_loss: 0.1820 - val_accuracy: 0.9484\n",
            "Epoch 641/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1242 - accuracy: 0.9503 - val_loss: 0.1724 - val_accuracy: 0.9399\n",
            "Epoch 642/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1280 - accuracy: 0.9444 - val_loss: 0.1903 - val_accuracy: 0.9399\n",
            "Epoch 643/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1299 - accuracy: 0.9463 - val_loss: 0.1718 - val_accuracy: 0.9463\n",
            "Epoch 644/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1245 - accuracy: 0.9503 - val_loss: 0.1730 - val_accuracy: 0.9452\n",
            "Epoch 645/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1222 - accuracy: 0.9521 - val_loss: 0.1876 - val_accuracy: 0.9378\n",
            "Epoch 646/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1303 - accuracy: 0.9458 - val_loss: 0.1739 - val_accuracy: 0.9484\n",
            "Epoch 647/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1356 - accuracy: 0.9472 - val_loss: 0.2839 - val_accuracy: 0.8778\n",
            "Epoch 648/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1245 - accuracy: 0.9485 - val_loss: 0.1741 - val_accuracy: 0.9431\n",
            "Epoch 649/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1258 - accuracy: 0.9530 - val_loss: 0.1731 - val_accuracy: 0.9484\n",
            "Epoch 650/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1284 - accuracy: 0.9481 - val_loss: 0.1675 - val_accuracy: 0.9473\n",
            "Epoch 651/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1250 - accuracy: 0.9467 - val_loss: 0.1752 - val_accuracy: 0.9420\n",
            "Epoch 652/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1273 - accuracy: 0.9508 - val_loss: 0.2034 - val_accuracy: 0.9399\n",
            "Epoch 653/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1275 - accuracy: 0.9453 - val_loss: 0.1741 - val_accuracy: 0.9505\n",
            "Epoch 654/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1239 - accuracy: 0.9508 - val_loss: 0.1728 - val_accuracy: 0.9442\n",
            "Epoch 655/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1274 - accuracy: 0.9472 - val_loss: 0.1771 - val_accuracy: 0.9452\n",
            "Epoch 656/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1258 - accuracy: 0.9467 - val_loss: 0.1806 - val_accuracy: 0.9442\n",
            "Epoch 657/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1301 - accuracy: 0.9476 - val_loss: 0.1856 - val_accuracy: 0.9431\n",
            "Epoch 658/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1353 - accuracy: 0.9463 - val_loss: 0.1791 - val_accuracy: 0.9536\n",
            "Epoch 659/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1345 - accuracy: 0.9490 - val_loss: 0.2006 - val_accuracy: 0.9420\n",
            "Epoch 660/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1291 - accuracy: 0.9453 - val_loss: 0.1706 - val_accuracy: 0.9452\n",
            "Epoch 661/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1265 - accuracy: 0.9485 - val_loss: 0.1866 - val_accuracy: 0.9431\n",
            "Epoch 662/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1297 - accuracy: 0.9503 - val_loss: 0.1689 - val_accuracy: 0.9473\n",
            "Epoch 663/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1270 - accuracy: 0.9485 - val_loss: 0.1730 - val_accuracy: 0.9473\n",
            "Epoch 664/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1288 - accuracy: 0.9481 - val_loss: 0.2034 - val_accuracy: 0.9368\n",
            "Epoch 665/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1296 - accuracy: 0.9453 - val_loss: 0.1639 - val_accuracy: 0.9442\n",
            "Epoch 666/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1215 - accuracy: 0.9503 - val_loss: 0.1862 - val_accuracy: 0.9357\n",
            "Epoch 667/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1291 - accuracy: 0.9435 - val_loss: 0.1983 - val_accuracy: 0.9326\n",
            "Epoch 668/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1232 - accuracy: 0.9521 - val_loss: 0.1734 - val_accuracy: 0.9442\n",
            "Epoch 669/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1235 - accuracy: 0.9490 - val_loss: 0.1835 - val_accuracy: 0.9452\n",
            "Epoch 670/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1271 - accuracy: 0.9481 - val_loss: 0.1831 - val_accuracy: 0.9399\n",
            "Epoch 671/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1264 - accuracy: 0.9499 - val_loss: 0.1758 - val_accuracy: 0.9505\n",
            "Epoch 672/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1310 - accuracy: 0.9472 - val_loss: 0.1780 - val_accuracy: 0.9442\n",
            "Epoch 673/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1265 - accuracy: 0.9472 - val_loss: 0.1752 - val_accuracy: 0.9473\n",
            "Epoch 674/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1195 - accuracy: 0.9526 - val_loss: 0.1947 - val_accuracy: 0.9484\n",
            "Epoch 675/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1301 - accuracy: 0.9485 - val_loss: 0.2369 - val_accuracy: 0.9336\n",
            "Epoch 676/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1330 - accuracy: 0.9490 - val_loss: 0.2041 - val_accuracy: 0.9431\n",
            "Epoch 677/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1234 - accuracy: 0.9494 - val_loss: 0.2088 - val_accuracy: 0.9136\n",
            "Epoch 678/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1374 - accuracy: 0.9444 - val_loss: 0.1707 - val_accuracy: 0.9515\n",
            "Epoch 679/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1261 - accuracy: 0.9476 - val_loss: 0.1737 - val_accuracy: 0.9515\n",
            "Epoch 680/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1227 - accuracy: 0.9472 - val_loss: 0.1927 - val_accuracy: 0.9357\n",
            "Epoch 681/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1303 - accuracy: 0.9494 - val_loss: 0.1775 - val_accuracy: 0.9484\n",
            "Epoch 682/1000\n",
            "222/222 [==============================] - 1s 4ms/step - loss: 0.1314 - accuracy: 0.9440 - val_loss: 0.1683 - val_accuracy: 0.9452\n",
            "Epoch 683/1000\n",
            "222/222 [==============================] - 1s 3ms/step - loss: 0.1216 - accuracy: 0.9476 - val_loss: 0.1738 - val_accuracy: 0.9526\n",
            "Epoch 684/1000\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.1316 - accuracy: 0.9431 - val_loss: 0.1886 - val_accuracy: 0.9452\n",
            "Epoch 685/1000\n",
            "222/222 [==============================] - 1s 4ms/step - loss: 0.1228 - accuracy: 0.9490 - val_loss: 0.1815 - val_accuracy: 0.9494\n",
            "Epoch 686/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1165 - accuracy: 0.9512 - val_loss: 0.1714 - val_accuracy: 0.9442\n",
            "Epoch 687/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1411 - accuracy: 0.9440 - val_loss: 0.1650 - val_accuracy: 0.9494\n",
            "Epoch 688/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1203 - accuracy: 0.9499 - val_loss: 0.1796 - val_accuracy: 0.9526\n",
            "Epoch 689/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1233 - accuracy: 0.9485 - val_loss: 0.1676 - val_accuracy: 0.9494\n",
            "Epoch 690/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1259 - accuracy: 0.9494 - val_loss: 0.1907 - val_accuracy: 0.9420\n",
            "Epoch 691/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1386 - accuracy: 0.9467 - val_loss: 0.1873 - val_accuracy: 0.9473\n",
            "Epoch 692/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1233 - accuracy: 0.9490 - val_loss: 0.1870 - val_accuracy: 0.9494\n",
            "Epoch 693/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1201 - accuracy: 0.9530 - val_loss: 0.1733 - val_accuracy: 0.9494\n",
            "Epoch 694/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1211 - accuracy: 0.9512 - val_loss: 0.1762 - val_accuracy: 0.9494\n",
            "Epoch 695/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1209 - accuracy: 0.9521 - val_loss: 0.1763 - val_accuracy: 0.9526\n",
            "Epoch 696/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1244 - accuracy: 0.9472 - val_loss: 0.1816 - val_accuracy: 0.9410\n",
            "Epoch 697/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1223 - accuracy: 0.9485 - val_loss: 0.1813 - val_accuracy: 0.9505\n",
            "Epoch 698/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1287 - accuracy: 0.9467 - val_loss: 0.2232 - val_accuracy: 0.9347\n",
            "Epoch 699/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1284 - accuracy: 0.9467 - val_loss: 0.1814 - val_accuracy: 0.9442\n",
            "Epoch 700/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1264 - accuracy: 0.9508 - val_loss: 0.1782 - val_accuracy: 0.9515\n",
            "Epoch 701/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1299 - accuracy: 0.9481 - val_loss: 0.1843 - val_accuracy: 0.9526\n",
            "Epoch 702/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1273 - accuracy: 0.9476 - val_loss: 0.1640 - val_accuracy: 0.9505\n",
            "Epoch 703/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1239 - accuracy: 0.9472 - val_loss: 0.1775 - val_accuracy: 0.9473\n",
            "Epoch 704/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1231 - accuracy: 0.9526 - val_loss: 0.1645 - val_accuracy: 0.9515\n",
            "Epoch 705/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1267 - accuracy: 0.9472 - val_loss: 0.1943 - val_accuracy: 0.9410\n",
            "Epoch 706/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1225 - accuracy: 0.9490 - val_loss: 0.1714 - val_accuracy: 0.9484\n",
            "Epoch 707/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1340 - accuracy: 0.9490 - val_loss: 0.1795 - val_accuracy: 0.9484\n",
            "Epoch 708/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1247 - accuracy: 0.9481 - val_loss: 0.1674 - val_accuracy: 0.9494\n",
            "Epoch 709/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1222 - accuracy: 0.9512 - val_loss: 0.1985 - val_accuracy: 0.9220\n",
            "Epoch 710/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1294 - accuracy: 0.9431 - val_loss: 0.1833 - val_accuracy: 0.9420\n",
            "Epoch 711/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1228 - accuracy: 0.9485 - val_loss: 0.1961 - val_accuracy: 0.9505\n",
            "Epoch 712/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1232 - accuracy: 0.9508 - val_loss: 0.1708 - val_accuracy: 0.9505\n",
            "Epoch 713/1000\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.1210 - accuracy: 0.9508 - val_loss: 0.2002 - val_accuracy: 0.9378\n",
            "Epoch 714/1000\n",
            "222/222 [==============================] - 1s 5ms/step - loss: 0.1239 - accuracy: 0.9472 - val_loss: 0.1742 - val_accuracy: 0.9505\n",
            "Epoch 715/1000\n",
            "222/222 [==============================] - 1s 4ms/step - loss: 0.1330 - accuracy: 0.9485 - val_loss: 0.1690 - val_accuracy: 0.9431\n",
            "Epoch 716/1000\n",
            "222/222 [==============================] - 1s 5ms/step - loss: 0.1267 - accuracy: 0.9517 - val_loss: 0.1668 - val_accuracy: 0.9515\n",
            "Epoch 717/1000\n",
            "222/222 [==============================] - 1s 3ms/step - loss: 0.1325 - accuracy: 0.9458 - val_loss: 0.1990 - val_accuracy: 0.9357\n",
            "Epoch 718/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1228 - accuracy: 0.9539 - val_loss: 0.1862 - val_accuracy: 0.9494\n",
            "Epoch 719/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1240 - accuracy: 0.9526 - val_loss: 0.1820 - val_accuracy: 0.9463\n",
            "Epoch 720/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1244 - accuracy: 0.9485 - val_loss: 0.1691 - val_accuracy: 0.9442\n",
            "Epoch 721/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1322 - accuracy: 0.9490 - val_loss: 0.1890 - val_accuracy: 0.9473\n",
            "Epoch 722/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1401 - accuracy: 0.9435 - val_loss: 0.1991 - val_accuracy: 0.9378\n",
            "Epoch 723/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1238 - accuracy: 0.9485 - val_loss: 0.1729 - val_accuracy: 0.9463\n",
            "Epoch 724/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1205 - accuracy: 0.9476 - val_loss: 0.1878 - val_accuracy: 0.9378\n",
            "Epoch 725/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1151 - accuracy: 0.9526 - val_loss: 0.1772 - val_accuracy: 0.9536\n",
            "Epoch 726/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1240 - accuracy: 0.9521 - val_loss: 0.1784 - val_accuracy: 0.9505\n",
            "Epoch 727/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1286 - accuracy: 0.9485 - val_loss: 0.2236 - val_accuracy: 0.9315\n",
            "Epoch 728/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1285 - accuracy: 0.9472 - val_loss: 0.1776 - val_accuracy: 0.9463\n",
            "Epoch 729/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1206 - accuracy: 0.9503 - val_loss: 0.1652 - val_accuracy: 0.9494\n",
            "Epoch 730/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1242 - accuracy: 0.9472 - val_loss: 0.1768 - val_accuracy: 0.9452\n",
            "Epoch 731/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1236 - accuracy: 0.9481 - val_loss: 0.1906 - val_accuracy: 0.9452\n",
            "Epoch 732/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1333 - accuracy: 0.9467 - val_loss: 0.1768 - val_accuracy: 0.9557\n",
            "Epoch 733/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1239 - accuracy: 0.9530 - val_loss: 0.1742 - val_accuracy: 0.9452\n",
            "Epoch 734/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1263 - accuracy: 0.9490 - val_loss: 0.1798 - val_accuracy: 0.9463\n",
            "Epoch 735/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1248 - accuracy: 0.9499 - val_loss: 0.1704 - val_accuracy: 0.9484\n",
            "Epoch 736/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1154 - accuracy: 0.9503 - val_loss: 0.1793 - val_accuracy: 0.9536\n",
            "Epoch 737/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1285 - accuracy: 0.9485 - val_loss: 0.2011 - val_accuracy: 0.9410\n",
            "Epoch 738/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1244 - accuracy: 0.9485 - val_loss: 0.1860 - val_accuracy: 0.9431\n",
            "Epoch 739/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1231 - accuracy: 0.9481 - val_loss: 0.1862 - val_accuracy: 0.9431\n",
            "Epoch 740/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1267 - accuracy: 0.9485 - val_loss: 0.1843 - val_accuracy: 0.9473\n",
            "Epoch 741/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1213 - accuracy: 0.9508 - val_loss: 0.1909 - val_accuracy: 0.9410\n",
            "Epoch 742/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1212 - accuracy: 0.9535 - val_loss: 0.1776 - val_accuracy: 0.9399\n",
            "Epoch 743/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1236 - accuracy: 0.9521 - val_loss: 0.1724 - val_accuracy: 0.9452\n",
            "Epoch 744/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1228 - accuracy: 0.9481 - val_loss: 0.3084 - val_accuracy: 0.8714\n",
            "Epoch 745/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1268 - accuracy: 0.9512 - val_loss: 0.1726 - val_accuracy: 0.9463\n",
            "Epoch 746/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1206 - accuracy: 0.9530 - val_loss: 0.1856 - val_accuracy: 0.9463\n",
            "Epoch 747/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1219 - accuracy: 0.9503 - val_loss: 0.1898 - val_accuracy: 0.9463\n",
            "Epoch 748/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1229 - accuracy: 0.9512 - val_loss: 0.1710 - val_accuracy: 0.9452\n",
            "Epoch 749/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1228 - accuracy: 0.9458 - val_loss: 0.2025 - val_accuracy: 0.9368\n",
            "Epoch 750/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1222 - accuracy: 0.9508 - val_loss: 0.1775 - val_accuracy: 0.9473\n",
            "Epoch 751/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1218 - accuracy: 0.9512 - val_loss: 0.1788 - val_accuracy: 0.9526\n",
            "Epoch 752/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1220 - accuracy: 0.9467 - val_loss: 0.1905 - val_accuracy: 0.9484\n",
            "Epoch 753/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1234 - accuracy: 0.9508 - val_loss: 0.1774 - val_accuracy: 0.9515\n",
            "Epoch 754/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1196 - accuracy: 0.9503 - val_loss: 0.1820 - val_accuracy: 0.9547\n",
            "Epoch 755/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1310 - accuracy: 0.9458 - val_loss: 0.1745 - val_accuracy: 0.9547\n",
            "Epoch 756/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1258 - accuracy: 0.9503 - val_loss: 0.1901 - val_accuracy: 0.9484\n",
            "Epoch 757/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1251 - accuracy: 0.9503 - val_loss: 0.1922 - val_accuracy: 0.9494\n",
            "Epoch 758/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1288 - accuracy: 0.9476 - val_loss: 0.2001 - val_accuracy: 0.9399\n",
            "Epoch 759/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1201 - accuracy: 0.9508 - val_loss: 0.2121 - val_accuracy: 0.9305\n",
            "Epoch 760/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1276 - accuracy: 0.9453 - val_loss: 0.1794 - val_accuracy: 0.9463\n",
            "Epoch 761/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1212 - accuracy: 0.9499 - val_loss: 0.1747 - val_accuracy: 0.9505\n",
            "Epoch 762/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1249 - accuracy: 0.9508 - val_loss: 0.1887 - val_accuracy: 0.9452\n",
            "Epoch 763/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1192 - accuracy: 0.9521 - val_loss: 0.1907 - val_accuracy: 0.9473\n",
            "Epoch 764/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1254 - accuracy: 0.9535 - val_loss: 0.2038 - val_accuracy: 0.9399\n",
            "Epoch 765/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1232 - accuracy: 0.9494 - val_loss: 0.2002 - val_accuracy: 0.9431\n",
            "Epoch 766/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1212 - accuracy: 0.9485 - val_loss: 0.1975 - val_accuracy: 0.9442\n",
            "Epoch 767/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1217 - accuracy: 0.9494 - val_loss: 0.2089 - val_accuracy: 0.9484\n",
            "Epoch 768/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1269 - accuracy: 0.9463 - val_loss: 0.2420 - val_accuracy: 0.9347\n",
            "Epoch 769/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1234 - accuracy: 0.9481 - val_loss: 0.2603 - val_accuracy: 0.9431\n",
            "Epoch 770/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1343 - accuracy: 0.9453 - val_loss: 0.2122 - val_accuracy: 0.9262\n",
            "Epoch 771/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1361 - accuracy: 0.9485 - val_loss: 0.1830 - val_accuracy: 0.9505\n",
            "Epoch 772/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1225 - accuracy: 0.9508 - val_loss: 0.1898 - val_accuracy: 0.9526\n",
            "Epoch 773/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1192 - accuracy: 0.9530 - val_loss: 0.1898 - val_accuracy: 0.9463\n",
            "Epoch 774/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1219 - accuracy: 0.9557 - val_loss: 0.2104 - val_accuracy: 0.9410\n",
            "Epoch 775/1000\n",
            "222/222 [==============================] - 1s 4ms/step - loss: 0.1298 - accuracy: 0.9449 - val_loss: 0.2155 - val_accuracy: 0.9347\n",
            "Epoch 776/1000\n",
            "222/222 [==============================] - 1s 5ms/step - loss: 0.1234 - accuracy: 0.9485 - val_loss: 0.2027 - val_accuracy: 0.9452\n",
            "Epoch 777/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1187 - accuracy: 0.9499 - val_loss: 0.2114 - val_accuracy: 0.9410\n",
            "Epoch 778/1000\n",
            "222/222 [==============================] - 1s 4ms/step - loss: 0.1234 - accuracy: 0.9481 - val_loss: 0.2115 - val_accuracy: 0.9494\n",
            "Epoch 779/1000\n",
            "222/222 [==============================] - 1s 5ms/step - loss: 0.1201 - accuracy: 0.9485 - val_loss: 0.2072 - val_accuracy: 0.9431\n",
            "Epoch 780/1000\n",
            "222/222 [==============================] - 1s 4ms/step - loss: 0.1184 - accuracy: 0.9530 - val_loss: 0.2127 - val_accuracy: 0.9463\n",
            "Epoch 781/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1268 - accuracy: 0.9472 - val_loss: 0.2290 - val_accuracy: 0.9231\n",
            "Epoch 782/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1208 - accuracy: 0.9526 - val_loss: 0.2063 - val_accuracy: 0.9484\n",
            "Epoch 783/1000\n",
            "222/222 [==============================] - 1s 3ms/step - loss: 0.1180 - accuracy: 0.9530 - val_loss: 0.2072 - val_accuracy: 0.9515\n",
            "Epoch 784/1000\n",
            "222/222 [==============================] - 1s 3ms/step - loss: 0.1222 - accuracy: 0.9499 - val_loss: 0.2224 - val_accuracy: 0.9505\n",
            "Epoch 785/1000\n",
            "222/222 [==============================] - 1s 3ms/step - loss: 0.1201 - accuracy: 0.9521 - val_loss: 0.2162 - val_accuracy: 0.9463\n",
            "Epoch 786/1000\n",
            "222/222 [==============================] - 1s 3ms/step - loss: 0.1232 - accuracy: 0.9517 - val_loss: 0.2171 - val_accuracy: 0.9484\n",
            "Epoch 787/1000\n",
            "222/222 [==============================] - 1s 3ms/step - loss: 0.1195 - accuracy: 0.9517 - val_loss: 0.2161 - val_accuracy: 0.9494\n",
            "Epoch 788/1000\n",
            "222/222 [==============================] - 1s 3ms/step - loss: 0.1158 - accuracy: 0.9562 - val_loss: 0.2356 - val_accuracy: 0.9420\n",
            "Epoch 789/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1186 - accuracy: 0.9544 - val_loss: 0.2658 - val_accuracy: 0.9262\n",
            "Epoch 790/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1352 - accuracy: 0.9458 - val_loss: 0.2194 - val_accuracy: 0.9452\n",
            "Epoch 791/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1305 - accuracy: 0.9508 - val_loss: 0.2266 - val_accuracy: 0.9473\n",
            "Epoch 792/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1227 - accuracy: 0.9499 - val_loss: 0.2784 - val_accuracy: 0.9420\n",
            "Epoch 793/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1327 - accuracy: 0.9526 - val_loss: 0.2285 - val_accuracy: 0.9431\n",
            "Epoch 794/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1277 - accuracy: 0.9453 - val_loss: 0.2327 - val_accuracy: 0.9515\n",
            "Epoch 795/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1195 - accuracy: 0.9490 - val_loss: 0.2480 - val_accuracy: 0.9494\n",
            "Epoch 796/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1206 - accuracy: 0.9503 - val_loss: 0.2711 - val_accuracy: 0.9473\n",
            "Epoch 797/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1222 - accuracy: 0.9503 - val_loss: 0.2496 - val_accuracy: 0.9484\n",
            "Epoch 798/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1182 - accuracy: 0.9526 - val_loss: 0.2568 - val_accuracy: 0.9526\n",
            "Epoch 799/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1367 - accuracy: 0.9453 - val_loss: 0.2405 - val_accuracy: 0.9378\n",
            "Epoch 800/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1240 - accuracy: 0.9476 - val_loss: 0.2167 - val_accuracy: 0.9336\n",
            "Epoch 801/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1330 - accuracy: 0.9485 - val_loss: 0.2180 - val_accuracy: 0.9494\n",
            "Epoch 802/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1206 - accuracy: 0.9544 - val_loss: 0.2386 - val_accuracy: 0.9305\n",
            "Epoch 803/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1219 - accuracy: 0.9494 - val_loss: 0.2373 - val_accuracy: 0.9484\n",
            "Epoch 804/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1206 - accuracy: 0.9517 - val_loss: 0.2189 - val_accuracy: 0.9536\n",
            "Epoch 805/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1207 - accuracy: 0.9517 - val_loss: 0.2325 - val_accuracy: 0.9431\n",
            "Epoch 806/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1270 - accuracy: 0.9494 - val_loss: 0.2671 - val_accuracy: 0.9452\n",
            "Epoch 807/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1259 - accuracy: 0.9476 - val_loss: 0.2442 - val_accuracy: 0.9515\n",
            "Epoch 808/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1207 - accuracy: 0.9517 - val_loss: 0.2474 - val_accuracy: 0.9442\n",
            "Epoch 809/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1188 - accuracy: 0.9508 - val_loss: 0.2314 - val_accuracy: 0.9389\n",
            "Epoch 810/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1230 - accuracy: 0.9503 - val_loss: 0.2426 - val_accuracy: 0.9463\n",
            "Epoch 811/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1181 - accuracy: 0.9508 - val_loss: 0.2362 - val_accuracy: 0.9410\n",
            "Epoch 812/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1229 - accuracy: 0.9481 - val_loss: 0.2287 - val_accuracy: 0.9473\n",
            "Epoch 813/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1195 - accuracy: 0.9490 - val_loss: 0.2583 - val_accuracy: 0.9357\n",
            "Epoch 814/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1199 - accuracy: 0.9490 - val_loss: 0.2591 - val_accuracy: 0.9473\n",
            "Epoch 815/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1243 - accuracy: 0.9512 - val_loss: 0.2813 - val_accuracy: 0.9262\n",
            "Epoch 816/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1271 - accuracy: 0.9512 - val_loss: 0.2319 - val_accuracy: 0.9526\n",
            "Epoch 817/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1239 - accuracy: 0.9508 - val_loss: 0.2296 - val_accuracy: 0.9463\n",
            "Epoch 818/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1222 - accuracy: 0.9485 - val_loss: 0.2633 - val_accuracy: 0.9494\n",
            "Epoch 819/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1221 - accuracy: 0.9494 - val_loss: 0.2474 - val_accuracy: 0.9494\n",
            "Epoch 820/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1227 - accuracy: 0.9512 - val_loss: 0.2135 - val_accuracy: 0.9420\n",
            "Epoch 821/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1264 - accuracy: 0.9526 - val_loss: 0.2072 - val_accuracy: 0.9431\n",
            "Epoch 822/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1209 - accuracy: 0.9544 - val_loss: 0.2378 - val_accuracy: 0.9410\n",
            "Epoch 823/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1255 - accuracy: 0.9490 - val_loss: 0.2430 - val_accuracy: 0.9357\n",
            "Epoch 824/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1168 - accuracy: 0.9521 - val_loss: 0.2545 - val_accuracy: 0.9536\n",
            "Epoch 825/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1255 - accuracy: 0.9472 - val_loss: 0.2128 - val_accuracy: 0.9484\n",
            "Epoch 826/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1270 - accuracy: 0.9494 - val_loss: 0.2380 - val_accuracy: 0.9484\n",
            "Epoch 827/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1222 - accuracy: 0.9526 - val_loss: 0.2288 - val_accuracy: 0.9505\n",
            "Epoch 828/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1169 - accuracy: 0.9539 - val_loss: 0.2196 - val_accuracy: 0.9442\n",
            "Epoch 829/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1284 - accuracy: 0.9440 - val_loss: 0.2342 - val_accuracy: 0.9547\n",
            "Epoch 830/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1182 - accuracy: 0.9494 - val_loss: 0.2307 - val_accuracy: 0.9452\n",
            "Epoch 831/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1254 - accuracy: 0.9467 - val_loss: 0.2191 - val_accuracy: 0.9452\n",
            "Epoch 832/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1204 - accuracy: 0.9535 - val_loss: 0.2238 - val_accuracy: 0.9515\n",
            "Epoch 833/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1268 - accuracy: 0.9499 - val_loss: 0.2499 - val_accuracy: 0.9463\n",
            "Epoch 834/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1248 - accuracy: 0.9494 - val_loss: 0.2597 - val_accuracy: 0.9452\n",
            "Epoch 835/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1306 - accuracy: 0.9503 - val_loss: 0.2304 - val_accuracy: 0.9389\n",
            "Epoch 836/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1200 - accuracy: 0.9526 - val_loss: 0.2476 - val_accuracy: 0.9452\n",
            "Epoch 837/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1262 - accuracy: 0.9472 - val_loss: 0.2608 - val_accuracy: 0.9452\n",
            "Epoch 838/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1241 - accuracy: 0.9485 - val_loss: 0.2563 - val_accuracy: 0.9484\n",
            "Epoch 839/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1255 - accuracy: 0.9453 - val_loss: 0.2327 - val_accuracy: 0.9463\n",
            "Epoch 840/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1260 - accuracy: 0.9526 - val_loss: 0.2475 - val_accuracy: 0.9494\n",
            "Epoch 841/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1177 - accuracy: 0.9553 - val_loss: 0.2516 - val_accuracy: 0.9452\n",
            "Epoch 842/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1220 - accuracy: 0.9517 - val_loss: 0.2685 - val_accuracy: 0.9473\n",
            "Epoch 843/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1204 - accuracy: 0.9526 - val_loss: 0.2778 - val_accuracy: 0.9452\n",
            "Epoch 844/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1186 - accuracy: 0.9521 - val_loss: 0.2264 - val_accuracy: 0.9463\n",
            "Epoch 845/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1157 - accuracy: 0.9512 - val_loss: 0.2751 - val_accuracy: 0.9536\n",
            "Epoch 846/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1194 - accuracy: 0.9494 - val_loss: 0.2938 - val_accuracy: 0.9031\n",
            "Epoch 847/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1241 - accuracy: 0.9458 - val_loss: 0.2576 - val_accuracy: 0.9442\n",
            "Epoch 848/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1241 - accuracy: 0.9490 - val_loss: 0.2126 - val_accuracy: 0.9505\n",
            "Epoch 849/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1173 - accuracy: 0.9499 - val_loss: 0.2289 - val_accuracy: 0.9473\n",
            "Epoch 850/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1268 - accuracy: 0.9494 - val_loss: 0.2579 - val_accuracy: 0.9494\n",
            "Epoch 851/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1166 - accuracy: 0.9526 - val_loss: 0.2699 - val_accuracy: 0.9526\n",
            "Epoch 852/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1193 - accuracy: 0.9530 - val_loss: 0.2696 - val_accuracy: 0.9547\n",
            "Epoch 853/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1257 - accuracy: 0.9508 - val_loss: 0.2865 - val_accuracy: 0.9220\n",
            "Epoch 854/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1166 - accuracy: 0.9512 - val_loss: 0.2469 - val_accuracy: 0.9452\n",
            "Epoch 855/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1168 - accuracy: 0.9499 - val_loss: 0.2710 - val_accuracy: 0.9442\n",
            "Epoch 856/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1193 - accuracy: 0.9467 - val_loss: 0.2700 - val_accuracy: 0.9442\n",
            "Epoch 857/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1166 - accuracy: 0.9530 - val_loss: 0.2686 - val_accuracy: 0.9494\n",
            "Epoch 858/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1165 - accuracy: 0.9521 - val_loss: 0.2858 - val_accuracy: 0.9515\n",
            "Epoch 859/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1231 - accuracy: 0.9499 - val_loss: 0.3043 - val_accuracy: 0.9305\n",
            "Epoch 860/1000\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.1641 - accuracy: 0.9408 - val_loss: 0.2197 - val_accuracy: 0.9357\n",
            "Epoch 861/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1219 - accuracy: 0.9539 - val_loss: 0.2168 - val_accuracy: 0.9547\n",
            "Epoch 862/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1229 - accuracy: 0.9517 - val_loss: 0.2451 - val_accuracy: 0.9515\n",
            "Epoch 863/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1178 - accuracy: 0.9494 - val_loss: 0.2519 - val_accuracy: 0.9494\n",
            "Epoch 864/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1150 - accuracy: 0.9535 - val_loss: 0.2344 - val_accuracy: 0.9442\n",
            "Epoch 865/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1216 - accuracy: 0.9481 - val_loss: 0.2828 - val_accuracy: 0.9368\n",
            "Epoch 866/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1186 - accuracy: 0.9490 - val_loss: 0.2493 - val_accuracy: 0.9515\n",
            "Epoch 867/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1163 - accuracy: 0.9526 - val_loss: 0.2557 - val_accuracy: 0.9389\n",
            "Epoch 868/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1204 - accuracy: 0.9490 - val_loss: 0.2616 - val_accuracy: 0.9442\n",
            "Epoch 869/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1124 - accuracy: 0.9517 - val_loss: 0.2656 - val_accuracy: 0.9463\n",
            "Epoch 870/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1186 - accuracy: 0.9472 - val_loss: 0.2903 - val_accuracy: 0.9336\n",
            "Epoch 871/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1181 - accuracy: 0.9530 - val_loss: 0.2457 - val_accuracy: 0.9410\n",
            "Epoch 872/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1200 - accuracy: 0.9521 - val_loss: 0.2575 - val_accuracy: 0.9484\n",
            "Epoch 873/1000\n",
            "222/222 [==============================] - 1s 4ms/step - loss: 0.1254 - accuracy: 0.9485 - val_loss: 0.2570 - val_accuracy: 0.9494\n",
            "Epoch 874/1000\n",
            "222/222 [==============================] - 1s 3ms/step - loss: 0.1254 - accuracy: 0.9490 - val_loss: 0.3009 - val_accuracy: 0.9389\n",
            "Epoch 875/1000\n",
            "222/222 [==============================] - 1s 3ms/step - loss: 0.1230 - accuracy: 0.9503 - val_loss: 0.2111 - val_accuracy: 0.9452\n",
            "Epoch 876/1000\n",
            "222/222 [==============================] - 1s 4ms/step - loss: 0.1150 - accuracy: 0.9544 - val_loss: 0.2245 - val_accuracy: 0.9484\n",
            "Epoch 877/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1199 - accuracy: 0.9499 - val_loss: 0.2347 - val_accuracy: 0.9505\n",
            "Epoch 878/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1253 - accuracy: 0.9526 - val_loss: 0.2489 - val_accuracy: 0.9463\n",
            "Epoch 879/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1190 - accuracy: 0.9544 - val_loss: 0.2500 - val_accuracy: 0.9526\n",
            "Epoch 880/1000\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.1259 - accuracy: 0.9449 - val_loss: 0.2344 - val_accuracy: 0.9420\n",
            "Epoch 881/1000\n",
            "222/222 [==============================] - 1s 3ms/step - loss: 0.1228 - accuracy: 0.9512 - val_loss: 0.2335 - val_accuracy: 0.9463\n",
            "Epoch 882/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1225 - accuracy: 0.9499 - val_loss: 0.2642 - val_accuracy: 0.9484\n",
            "Epoch 883/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1178 - accuracy: 0.9494 - val_loss: 0.2665 - val_accuracy: 0.9494\n",
            "Epoch 884/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1229 - accuracy: 0.9494 - val_loss: 0.2211 - val_accuracy: 0.9473\n",
            "Epoch 885/1000\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.1177 - accuracy: 0.9499 - val_loss: 0.2360 - val_accuracy: 0.9389\n",
            "Epoch 886/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1226 - accuracy: 0.9476 - val_loss: 0.2445 - val_accuracy: 0.9410\n",
            "Epoch 887/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1130 - accuracy: 0.9539 - val_loss: 0.2529 - val_accuracy: 0.9473\n",
            "Epoch 888/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1220 - accuracy: 0.9503 - val_loss: 0.2450 - val_accuracy: 0.9494\n",
            "Epoch 889/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1241 - accuracy: 0.9526 - val_loss: 0.2333 - val_accuracy: 0.9484\n",
            "Epoch 890/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1215 - accuracy: 0.9494 - val_loss: 0.2696 - val_accuracy: 0.9357\n",
            "Epoch 891/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1177 - accuracy: 0.9539 - val_loss: 0.2463 - val_accuracy: 0.9515\n",
            "Epoch 892/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1165 - accuracy: 0.9557 - val_loss: 0.2995 - val_accuracy: 0.9378\n",
            "Epoch 893/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1259 - accuracy: 0.9463 - val_loss: 0.2561 - val_accuracy: 0.9442\n",
            "Epoch 894/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1162 - accuracy: 0.9512 - val_loss: 0.2454 - val_accuracy: 0.9494\n",
            "Epoch 895/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1178 - accuracy: 0.9544 - val_loss: 0.2682 - val_accuracy: 0.9452\n",
            "Epoch 896/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1215 - accuracy: 0.9548 - val_loss: 0.2003 - val_accuracy: 0.9463\n",
            "Epoch 897/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1359 - accuracy: 0.9467 - val_loss: 0.2327 - val_accuracy: 0.9452\n",
            "Epoch 898/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1195 - accuracy: 0.9512 - val_loss: 0.2270 - val_accuracy: 0.9494\n",
            "Epoch 899/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1164 - accuracy: 0.9535 - val_loss: 0.2333 - val_accuracy: 0.9431\n",
            "Epoch 900/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1251 - accuracy: 0.9463 - val_loss: 0.2593 - val_accuracy: 0.9494\n",
            "Epoch 901/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1191 - accuracy: 0.9535 - val_loss: 0.2679 - val_accuracy: 0.9357\n",
            "Epoch 902/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1176 - accuracy: 0.9512 - val_loss: 0.2746 - val_accuracy: 0.9442\n",
            "Epoch 903/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1167 - accuracy: 0.9499 - val_loss: 0.2799 - val_accuracy: 0.9442\n",
            "Epoch 904/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1298 - accuracy: 0.9481 - val_loss: 0.2168 - val_accuracy: 0.9484\n",
            "Epoch 905/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1213 - accuracy: 0.9548 - val_loss: 0.2707 - val_accuracy: 0.9484\n",
            "Epoch 906/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1194 - accuracy: 0.9512 - val_loss: 0.2602 - val_accuracy: 0.9452\n",
            "Epoch 907/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1178 - accuracy: 0.9490 - val_loss: 0.2521 - val_accuracy: 0.9505\n",
            "Epoch 908/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1162 - accuracy: 0.9526 - val_loss: 0.2491 - val_accuracy: 0.9494\n",
            "Epoch 909/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1145 - accuracy: 0.9512 - val_loss: 0.3216 - val_accuracy: 0.9252\n",
            "Epoch 910/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1211 - accuracy: 0.9494 - val_loss: 0.2221 - val_accuracy: 0.9463\n",
            "Epoch 911/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1177 - accuracy: 0.9530 - val_loss: 0.2076 - val_accuracy: 0.9452\n",
            "Epoch 912/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1193 - accuracy: 0.9521 - val_loss: 0.2772 - val_accuracy: 0.9494\n",
            "Epoch 913/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1251 - accuracy: 0.9499 - val_loss: 0.2672 - val_accuracy: 0.9526\n",
            "Epoch 914/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1158 - accuracy: 0.9544 - val_loss: 0.2764 - val_accuracy: 0.9505\n",
            "Epoch 915/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1227 - accuracy: 0.9490 - val_loss: 0.2735 - val_accuracy: 0.9452\n",
            "Epoch 916/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1208 - accuracy: 0.9490 - val_loss: 0.2274 - val_accuracy: 0.9463\n",
            "Epoch 917/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1261 - accuracy: 0.9494 - val_loss: 0.2355 - val_accuracy: 0.9515\n",
            "Epoch 918/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1183 - accuracy: 0.9494 - val_loss: 0.2417 - val_accuracy: 0.9505\n",
            "Epoch 919/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1179 - accuracy: 0.9499 - val_loss: 0.2754 - val_accuracy: 0.9526\n",
            "Epoch 920/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1216 - accuracy: 0.9485 - val_loss: 0.2754 - val_accuracy: 0.9526\n",
            "Epoch 921/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1215 - accuracy: 0.9512 - val_loss: 0.2389 - val_accuracy: 0.9484\n",
            "Epoch 922/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1226 - accuracy: 0.9512 - val_loss: 0.2521 - val_accuracy: 0.9283\n",
            "Epoch 923/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1157 - accuracy: 0.9499 - val_loss: 0.2598 - val_accuracy: 0.9442\n",
            "Epoch 924/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1166 - accuracy: 0.9521 - val_loss: 0.2503 - val_accuracy: 0.9442\n",
            "Epoch 925/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1120 - accuracy: 0.9503 - val_loss: 0.2702 - val_accuracy: 0.9526\n",
            "Epoch 926/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1157 - accuracy: 0.9490 - val_loss: 0.2247 - val_accuracy: 0.9452\n",
            "Epoch 927/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1222 - accuracy: 0.9512 - val_loss: 0.2905 - val_accuracy: 0.9505\n",
            "Epoch 928/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1175 - accuracy: 0.9508 - val_loss: 0.2873 - val_accuracy: 0.9484\n",
            "Epoch 929/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1193 - accuracy: 0.9503 - val_loss: 0.2445 - val_accuracy: 0.9326\n",
            "Epoch 930/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1259 - accuracy: 0.9508 - val_loss: 0.2615 - val_accuracy: 0.9357\n",
            "Epoch 931/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1262 - accuracy: 0.9512 - val_loss: 0.2512 - val_accuracy: 0.9484\n",
            "Epoch 932/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1225 - accuracy: 0.9485 - val_loss: 0.2645 - val_accuracy: 0.9115\n",
            "Epoch 933/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1218 - accuracy: 0.9458 - val_loss: 0.2655 - val_accuracy: 0.9505\n",
            "Epoch 934/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1131 - accuracy: 0.9544 - val_loss: 0.2941 - val_accuracy: 0.9420\n",
            "Epoch 935/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1246 - accuracy: 0.9490 - val_loss: 0.2831 - val_accuracy: 0.9505\n",
            "Epoch 936/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1215 - accuracy: 0.9512 - val_loss: 0.2791 - val_accuracy: 0.9399\n",
            "Epoch 937/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1170 - accuracy: 0.9499 - val_loss: 0.2935 - val_accuracy: 0.9399\n",
            "Epoch 938/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1231 - accuracy: 0.9517 - val_loss: 0.2719 - val_accuracy: 0.9526\n",
            "Epoch 939/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1170 - accuracy: 0.9517 - val_loss: 0.2658 - val_accuracy: 0.9473\n",
            "Epoch 940/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1158 - accuracy: 0.9539 - val_loss: 0.2681 - val_accuracy: 0.9484\n",
            "Epoch 941/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1192 - accuracy: 0.9499 - val_loss: 0.2903 - val_accuracy: 0.9536\n",
            "Epoch 942/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1305 - accuracy: 0.9472 - val_loss: 0.2615 - val_accuracy: 0.9368\n",
            "Epoch 943/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1234 - accuracy: 0.9508 - val_loss: 0.2250 - val_accuracy: 0.9463\n",
            "Epoch 944/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1152 - accuracy: 0.9580 - val_loss: 0.2526 - val_accuracy: 0.9431\n",
            "Epoch 945/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1209 - accuracy: 0.9481 - val_loss: 0.2497 - val_accuracy: 0.9473\n",
            "Epoch 946/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1213 - accuracy: 0.9476 - val_loss: 0.3510 - val_accuracy: 0.9009\n",
            "Epoch 947/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1379 - accuracy: 0.9444 - val_loss: 0.2433 - val_accuracy: 0.9484\n",
            "Epoch 948/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1179 - accuracy: 0.9526 - val_loss: 0.2463 - val_accuracy: 0.9473\n",
            "Epoch 949/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1120 - accuracy: 0.9566 - val_loss: 0.2924 - val_accuracy: 0.9505\n",
            "Epoch 950/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1151 - accuracy: 0.9526 - val_loss: 0.2770 - val_accuracy: 0.8999\n",
            "Epoch 951/1000\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.1162 - accuracy: 0.9503 - val_loss: 0.2808 - val_accuracy: 0.9547\n",
            "Epoch 952/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1113 - accuracy: 0.9521 - val_loss: 0.2901 - val_accuracy: 0.9368\n",
            "Epoch 953/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1168 - accuracy: 0.9544 - val_loss: 0.2477 - val_accuracy: 0.9484\n",
            "Epoch 954/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1164 - accuracy: 0.9512 - val_loss: 0.2518 - val_accuracy: 0.9526\n",
            "Epoch 955/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1155 - accuracy: 0.9535 - val_loss: 0.2785 - val_accuracy: 0.9389\n",
            "Epoch 956/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1212 - accuracy: 0.9512 - val_loss: 0.3216 - val_accuracy: 0.9326\n",
            "Epoch 957/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1348 - accuracy: 0.9485 - val_loss: 0.2793 - val_accuracy: 0.9452\n",
            "Epoch 958/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1181 - accuracy: 0.9503 - val_loss: 0.2733 - val_accuracy: 0.9463\n",
            "Epoch 959/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1271 - accuracy: 0.9485 - val_loss: 0.2766 - val_accuracy: 0.9473\n",
            "Epoch 960/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1213 - accuracy: 0.9481 - val_loss: 0.2837 - val_accuracy: 0.9494\n",
            "Epoch 961/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1196 - accuracy: 0.9463 - val_loss: 0.3106 - val_accuracy: 0.9368\n",
            "Epoch 962/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1172 - accuracy: 0.9512 - val_loss: 0.3156 - val_accuracy: 0.9526\n",
            "Epoch 963/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1138 - accuracy: 0.9508 - val_loss: 0.3114 - val_accuracy: 0.9347\n",
            "Epoch 964/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1240 - accuracy: 0.9458 - val_loss: 0.3288 - val_accuracy: 0.9452\n",
            "Epoch 965/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1248 - accuracy: 0.9490 - val_loss: 0.2750 - val_accuracy: 0.9494\n",
            "Epoch 966/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1161 - accuracy: 0.9526 - val_loss: 0.3083 - val_accuracy: 0.9336\n",
            "Epoch 967/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1154 - accuracy: 0.9503 - val_loss: 0.2837 - val_accuracy: 0.9420\n",
            "Epoch 968/1000\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.1142 - accuracy: 0.9539 - val_loss: 0.3069 - val_accuracy: 0.9494\n",
            "Epoch 969/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1174 - accuracy: 0.9535 - val_loss: 0.3024 - val_accuracy: 0.9484\n",
            "Epoch 970/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1203 - accuracy: 0.9503 - val_loss: 0.2222 - val_accuracy: 0.9526\n",
            "Epoch 971/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1209 - accuracy: 0.9481 - val_loss: 0.2844 - val_accuracy: 0.9494\n",
            "Epoch 972/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1275 - accuracy: 0.9467 - val_loss: 0.2906 - val_accuracy: 0.9526\n",
            "Epoch 973/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1115 - accuracy: 0.9503 - val_loss: 0.2981 - val_accuracy: 0.9536\n",
            "Epoch 974/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1177 - accuracy: 0.9535 - val_loss: 0.2890 - val_accuracy: 0.9505\n",
            "Epoch 975/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1155 - accuracy: 0.9481 - val_loss: 0.3144 - val_accuracy: 0.9410\n",
            "Epoch 976/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1149 - accuracy: 0.9512 - val_loss: 0.2990 - val_accuracy: 0.9484\n",
            "Epoch 977/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1218 - accuracy: 0.9503 - val_loss: 0.2443 - val_accuracy: 0.9431\n",
            "Epoch 978/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1277 - accuracy: 0.9481 - val_loss: 0.2564 - val_accuracy: 0.9484\n",
            "Epoch 979/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1157 - accuracy: 0.9526 - val_loss: 0.2840 - val_accuracy: 0.9452\n",
            "Epoch 980/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1175 - accuracy: 0.9503 - val_loss: 0.2949 - val_accuracy: 0.9431\n",
            "Epoch 981/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1128 - accuracy: 0.9530 - val_loss: 0.2722 - val_accuracy: 0.9463\n",
            "Epoch 982/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1162 - accuracy: 0.9499 - val_loss: 0.2900 - val_accuracy: 0.9494\n",
            "Epoch 983/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1177 - accuracy: 0.9508 - val_loss: 0.2727 - val_accuracy: 0.9357\n",
            "Epoch 984/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1196 - accuracy: 0.9485 - val_loss: 0.2694 - val_accuracy: 0.9515\n",
            "Epoch 985/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1202 - accuracy: 0.9503 - val_loss: 0.2926 - val_accuracy: 0.9252\n",
            "Epoch 986/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1136 - accuracy: 0.9562 - val_loss: 0.3076 - val_accuracy: 0.9452\n",
            "Epoch 987/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1191 - accuracy: 0.9490 - val_loss: 0.2910 - val_accuracy: 0.9515\n",
            "Epoch 988/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1104 - accuracy: 0.9512 - val_loss: 0.3234 - val_accuracy: 0.9368\n",
            "Epoch 989/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1208 - accuracy: 0.9490 - val_loss: 0.3424 - val_accuracy: 0.8988\n",
            "Epoch 990/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1185 - accuracy: 0.9481 - val_loss: 0.3155 - val_accuracy: 0.9536\n",
            "Epoch 991/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1194 - accuracy: 0.9557 - val_loss: 0.3351 - val_accuracy: 0.9399\n",
            "Epoch 992/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1145 - accuracy: 0.9499 - val_loss: 0.3184 - val_accuracy: 0.9336\n",
            "Epoch 993/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1202 - accuracy: 0.9490 - val_loss: 0.3093 - val_accuracy: 0.9484\n",
            "Epoch 994/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1092 - accuracy: 0.9517 - val_loss: 0.3131 - val_accuracy: 0.9410\n",
            "Epoch 995/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1159 - accuracy: 0.9521 - val_loss: 0.3277 - val_accuracy: 0.9463\n",
            "Epoch 996/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1143 - accuracy: 0.9521 - val_loss: 0.2577 - val_accuracy: 0.9536\n",
            "Epoch 997/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1223 - accuracy: 0.9472 - val_loss: 0.2729 - val_accuracy: 0.9305\n",
            "Epoch 998/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1174 - accuracy: 0.9562 - val_loss: 0.2860 - val_accuracy: 0.9494\n",
            "Epoch 999/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1177 - accuracy: 0.9530 - val_loss: 0.3163 - val_accuracy: 0.9442\n",
            "Epoch 1000/1000\n",
            "222/222 [==============================] - 0s 2ms/step - loss: 0.1187 - accuracy: 0.9508 - val_loss: 0.2772 - val_accuracy: 0.9399\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f7a32ff8090>"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ],
      "source": [
        "# To trained the algorithm \n",
        "model_ann.fit(Thyroid_X_train,cat_Thyroid_Y_train, validation_data=(Thyroid_X_test,cat_Thyroid_Y_test) ,epochs=1000,batch_size=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f972d40b-ba70-441d-8744-0fa9e04b8230",
        "id": "Z4HWbKGq3kjI"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "70/70 [==============================] - 0s 820us/step - loss: 0.1332 - accuracy: 0.9431\n"
          ]
        }
      ],
      "source": [
        "# to p[rint the accuracy of the algorithm ]\n",
        "accuracy= model_ann.evaluate(Thyroid_X_train,cat_Thyroid_Y_train)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "t_jRR3GV3kjJ"
      },
      "outputs": [],
      "source": [
        " # importing cr , cm \n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "# initialising the prediction variable\n",
        "pred4 = model_ann.predict(Thyroid_X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76e4ccdc-6ac6-4953-a971-50c9e19c4825",
        "id": "CDquKXww3kjJ"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(949, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ],
      "source": [
        "pred4.shape # to get the shape "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "awMr1dZ03kjJ"
      },
      "outputs": [],
      "source": [
        "# to get the maximum values \n",
        "pred4=np.argmax(pred4, axis=1)\n",
        "cat_Thyroid_Y_test=np.argmax(cat_Thyroid_Y_test,axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "531355bd-157b-4f73-ea58-36e9498c4bb1",
        "id": "klHakzu33kjJ"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(949,)"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ],
      "source": [
        "pred4.shape # to get the shape "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4ae8c55-7a76-473b-ec84-ccb902e601d6",
        "id": "X4H0CPn23kjJ"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(949,)"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ],
      "source": [
        "cat_Thyroid_Y_test.shape #to know the shape  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "B3IXGPL33kjJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b82fc066-a8c1-43e2-aa53-c4d3c6b5e0e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.60      0.67      0.63        73\n",
            "           1       0.97      0.96      0.97       876\n",
            "\n",
            "    accuracy                           0.94       949\n",
            "   macro avg       0.78      0.82      0.80       949\n",
            "weighted avg       0.94      0.94      0.94       949\n",
            "\n",
            "\n",
            "Accuracy:  0.9399367755532139\n"
          ]
        }
      ],
      "source": [
        "# to print the c_r\n",
        "print(classification_report(cat_Thyroid_Y_test, pred4)) \n",
        "print()\n",
        "# reprinting the performance \n",
        "print('Accuracy: ', accuracy_score(cat_Thyroid_Y_test, pred4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3782b99-1866-4caa-c167-ac65c0fc8506",
        "id": "O1-MbUvD3kjJ"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 49,  24],\n",
              "       [ 33, 843]])"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ],
      "source": [
        "# importing c_R\n",
        "from sklearn.metrics import confusion_matrix\n",
        "# initializing the cm\n",
        "cm4 = confusion_matrix(cat_Thyroid_Y_test,pred4)\n",
        "cm4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "outputId": "28537c12-e40d-4cae-c7fe-1cf4a18c1e2f",
        "id": "ax8G6Jdr3kjJ"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(33.0, 0.5, 'Original Values')"
            ]
          },
          "metadata": {},
          "execution_count": 85
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# making heat map \n",
        "import seaborn as sns\n",
        "# assigning the color \n",
        "sns.heatmap(cm4,annot=True, fmt='d' , cmap='Pastel1_r')\n",
        "# assigning the x-axis label\n",
        "plt.xlabel('Predicted values')\n",
        "# assigning the y-axis label\n",
        "plt.ylabel('Original Values')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2cd301ce-0d55-4b16-a4a9-c574a3987cbc",
        "id": "MAHbp2xV3kjK"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sensitivity :  0.9335548172757475\n",
            "Specificity :  0.6712328767123288\n"
          ]
        }
      ],
      "source": [
        "# printing Sensitivity accuracy \n",
        "sensitivity = cm4[1,1]/(cm[1,1]+cm4[1,0])\n",
        "# to get the senstivity score\n",
        "print('Sensitivity : ', sensitivity)\n",
        "# printing Specificity accuracy \n",
        "specificity = cm4[0,0]/(cm4[0,1]+cm4[0,0])\n",
        "# to get the specificity score\n",
        "print('Specificity : ', specificity)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4KQ_KE3EQ7PZ"
      },
      "source": [
        "# **After_Undersampling**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "WMMCV__MT0k6"
      },
      "outputs": [],
      "source": [
        "# example of random undersampling to balance the class distribution\n",
        "from collections import Counter\n",
        "# to make the classification \n",
        "from sklearn.datasets import make_classification\n",
        "# to undersample the data \n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "# initializing the undersampler \n",
        "undersample = RandomUnderSampler(sampling_strategy='majority')\n",
        "# trained  and to transform \n",
        "X,Y = undersample.fit_resample(x, y)\n",
        "# summarize class distribution\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJehWL1AT0gr",
        "outputId": "178d5f73-3ad8-4cb1-e926-a0a5c1d40976"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(488, 15)"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ],
      "source": [
        "X.shape # to print the shape "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EH9nqD9eUFRj",
        "outputId": "d39da231-8e25-44f5-c074-755e81310aee"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(488,)"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ],
      "source": [
        "Y.shape # to get the shape  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uSNsGQ7nUZYp",
        "outputId": "75fdca1c-0881-4814-a363-738f6fb19edc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({0: 244, 1: 244})"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ],
      "source": [
        "# importing the count variable \n",
        "from numpy.ma.core import count\n",
        "# to count the data \n",
        "from collections import Counter\n",
        "# to print the data size\n",
        "a = Counter(Y)\n",
        "a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "BBb4vvF5UWCy",
        "outputId": "f84154e9-9075-4707-c462-0cf99180196d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "#count plot graph of target variable of arrival dataset\n",
        "import warnings\n",
        "# to ignore the warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "# to plot the countplot \n",
        "sns.countplot(Y) \n",
        "#  to print the label of x \n",
        "plt.xlabel ( 'Class')\n",
        "# to print the label of y\n",
        "plt.ylabel ( 'Number')\n",
        "# to print the title of the  graph\n",
        "plt.title ( 'Count Plot of Thyroid disease')  \n",
        "# to draw the chart \n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "SM2tspnupTE3"
      },
      "outputs": [],
      "source": [
        "# splitting the arrival database into fitting and testing \n",
        "from sklearn.model_selection import train_test_split\n",
        "# assigning the attributes \n",
        "Thyroid_X_train, Thyroid_X_test, Thyroid_Y_train, Thyroid_Y_test = train_test_split(X,Y, test_size=0.3, random_state =42,  stratify=Y ,shuffle=True) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJ142NdKpTBO",
        "outputId": "ed21418d-b9a0-42fc-f4cb-7ebeab5babba"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(341, 15)"
            ]
          },
          "metadata": {},
          "execution_count": 93
        }
      ],
      "source": [
        "Thyroid_X_train.shape # to get the shape of the xtrain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Ln78v7tpS9P",
        "outputId": "33ed4b96-0fa7-40ed-e2da-262af26596a1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(341,)"
            ]
          },
          "metadata": {},
          "execution_count": 94
        }
      ],
      "source": [
        "Thyroid_Y_train.shape # to get the shape of ytrain "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k7jQDj3GpS2s",
        "outputId": "a32a8c91-a036-486e-c3fc-feb735a178df"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(147, 15)"
            ]
          },
          "metadata": {},
          "execution_count": 148
        }
      ],
      "source": [
        "Thyroid_X_test.shape   # to get the shape of the xtest "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q_2MjKt4pSz2",
        "outputId": "e4ba1dfe-33b1-4b69-9f55-b00a2383951c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(147,)"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ],
      "source": [
        "Thyroid_Y_test.shape  # toi get the shape of the ytest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1a_aeR9GeFm"
      },
      "source": [
        "\n",
        "# **KNN_Classifier**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bm6nQaQMjUvd",
        "outputId": "24b4f51d-8336-4508-f721-1bc9adfce656"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8231292517006803"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ],
      "source": [
        "# importing the algortithm\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "# initializing the algorithm\n",
        "model1 = KNeighborsClassifier(n_neighbors=2 )\n",
        "# to trained the algo\n",
        "model1.fit(Thyroid_X_train,Thyroid_Y_train)\n",
        "# to get the score of the algo\n",
        "model1.score(Thyroid_X_test, Thyroid_Y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "23l2o_OG8J90"
      },
      "outputs": [],
      "source": [
        "# importing cr , cm \n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "# initialising the predict variable\n",
        "pred1 = model1.predict(Thyroid_X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "7nQGZPoU8J6G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0f3800b-b831-4d09-924d-4ddfff075140"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      0.95      0.84        74\n",
            "           1       0.93      0.70      0.80        73\n",
            "\n",
            "    accuracy                           0.82       147\n",
            "   macro avg       0.84      0.82      0.82       147\n",
            "weighted avg       0.84      0.82      0.82       147\n",
            "\n",
            "\n",
            "Accuracy:  0.8231292517006803\n"
          ]
        }
      ],
      "source": [
        "# to print the c_r\n",
        "print(classification_report(Thyroid_Y_test, pred1)) \n",
        "print()\n",
        "# reprinting the performance \n",
        "print('Accuracy: ', accuracy_score(Thyroid_Y_test, pred1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "wmNnCaYx8JzK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0239c26-2daf-4d95-a60b-71798e857ac5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision by knn of testing data is: 0.823\n",
            "Recall by knn of testing data is: 0.823\n",
            "F1 score by knn of testing data is: 0.823\n"
          ]
        }
      ],
      "source": [
        "# various libraries for report\n",
        "from sklearn.metrics import precision_score,recall_score,f1_score\n",
        "# printing the performance \n",
        "print('Precision by knn of testing data is: %.3f' % precision_score(Thyroid_Y_test, pred1,average='micro')) \n",
        "# checking the precision value\n",
        "print('Recall by knn of testing data is: %.3f' % recall_score(Thyroid_Y_test, pred1,average='micro')) \n",
        "# checking the recall value\n",
        "print('F1 score by knn of testing data is: %.3f' % f1_score(Thyroid_Y_test, pred1,average='micro')) \n",
        "# checking the f2 score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aOvQda5x8JIs",
        "outputId": "b44fe28e-28c3-49d3-d144-0e1240e6204e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[70,  4],\n",
              "       [22, 51]])"
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "# for cm\n",
        "cm = confusion_matrix(Thyroid_Y_test,pred1)\n",
        "cm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "qe5pwQhW86X3",
        "outputId": "0bb27d3c-5e19-42f2-f385-c9cbbb1ca781"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(33.0, 0.5, 'Original Values')"
            ]
          },
          "metadata": {},
          "execution_count": 103
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# making heat map \n",
        "import seaborn as sns\n",
        "# assigning the color \n",
        "sns.heatmap(cm,annot=True, fmt='d' , cmap='Pastel1_r')\n",
        "# assigning the x-axis label\n",
        "plt.xlabel('Predicted values')\n",
        "# assigning the y-axis label\n",
        "plt.ylabel('Original Values')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2nfLGAV9LOL",
        "outputId": "5faecbb8-7e30-4678-e126-b8fd237f97c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sensitivity :  0.6986301369863014\n",
            "Specificity :  0.9459459459459459\n"
          ]
        }
      ],
      "source": [
        "# printing Sensitivity accuracy \n",
        "sensitivity = cm[1,1]/(cm[1,1]+cm[1,0])\n",
        "# to get the score of the senstivity\n",
        "print('Sensitivity : ', sensitivity)\n",
        "# printing Specificity accuracy \n",
        "specificity = cm[0,0]/(cm[0,1]+cm[0,0])\n",
        "# to get the score of the specificity\n",
        "print('Specificity : ', specificity)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PWJKf6lbhsv8",
        "outputId": "0a0ad51a-ec92-4205-e9d7-74f54db82d04"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8367346938775511"
            ]
          },
          "metadata": {},
          "execution_count": 105
        }
      ],
      "source": [
        "# to import the algorithm\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "# initializie the model \n",
        "model1 = KNeighborsClassifier(n_neighbors=10)\n",
        "## to trained the model \n",
        "model1.fit(Thyroid_X_train,Thyroid_Y_train)\n",
        "# to get  the score of the algorithm  \n",
        "model1.score(Thyroid_X_test, Thyroid_Y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "id": "lQkIRVhOhsv9"
      },
      "outputs": [],
      "source": [
        "# importing cr , cm \n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "# initialising the predict variable\n",
        "pred1 = model1.predict(Thyroid_X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "id": "q9q1-kR6hsv-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb752eae-5f68-4a7b-d3a8-8b4630d4fedd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.86      0.84        74\n",
            "           1       0.86      0.81      0.83        73\n",
            "\n",
            "    accuracy                           0.84       147\n",
            "   macro avg       0.84      0.84      0.84       147\n",
            "weighted avg       0.84      0.84      0.84       147\n",
            "\n",
            "\n",
            "Accuracy:  0.8367346938775511\n"
          ]
        }
      ],
      "source": [
        "# to print the c_r\n",
        "print(classification_report(Thyroid_Y_test, pred1)) \n",
        "print()\n",
        "# reprinting the performance \n",
        "print('Accuracy: ', accuracy_score(Thyroid_Y_test, pred1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oFtiPqOBhsv_",
        "outputId": "ee5be7b5-dbe6-4f03-f691-077d133f454a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision by knn of testing data is: 0.837\n",
            "Recall by knn of testing data is: 0.837\n",
            "F1 score by knn of testing data is: 0.837\n"
          ]
        }
      ],
      "source": [
        "# various libraries for report\n",
        "from sklearn.metrics import precision_score,recall_score,f1_score\n",
        "# printing the performance \n",
        "print('Precision by knn of testing data is: %.3f' % precision_score(Thyroid_Y_test, pred1,average='micro')) \n",
        "# checking the precision value\n",
        "print('Recall by knn of testing data is: %.3f' % recall_score(Thyroid_Y_test, pred1,average='micro')) \n",
        "# checking the recall value\n",
        "print('F1 score by knn of testing data is: %.3f' % f1_score(Thyroid_Y_test, pred1,average='micro')) \n",
        "# checking the f2 score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WVMRgvmnhswA",
        "outputId": "850a1a18-d6fe-42e7-9409-31d70372a97a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[64, 10],\n",
              "       [14, 59]])"
            ]
          },
          "metadata": {},
          "execution_count": 109
        }
      ],
      "source": [
        "# importing c_m\n",
        "from sklearn.metrics import confusion_matrix\n",
        "# initializing the cm\n",
        "cm = confusion_matrix(Thyroid_Y_test,pred1)\n",
        "cm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "WJfdDGwEhswA",
        "outputId": "5499205b-1269-4fd5-adc6-5f256c456dab"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(33.0, 0.5, 'Original Values')"
            ]
          },
          "metadata": {},
          "execution_count": 110
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# making heat map \n",
        "import seaborn as sns\n",
        "# assigning the color \n",
        "sns.heatmap(cm,annot=True, fmt='d' , cmap='Pastel1_r')\n",
        "# assigning the x-axis label\n",
        "plt.xlabel('Predicted values')\n",
        "# assigning the y-axis label\n",
        "plt.ylabel('Original Values')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j-NCkoAihswB",
        "outputId": "40bb4c9c-9324-462c-e5af-88cc4f1f5bda"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sensitivity :  0.8082191780821918\n",
            "Specificity :  0.8648648648648649\n"
          ]
        }
      ],
      "source": [
        "# printing Sensitivity accuracy \n",
        "sensitivity = cm[1,1]/(cm[1,1]+cm[1,0])\n",
        "# to get the sensitivity score\n",
        "print('Sensitivity : ', sensitivity)\n",
        "# printing Specificity accuracy \n",
        "specificity = cm[0,0]/(cm[0,1]+cm[0,0])\n",
        "# to get the specificity score \n",
        "print('Specificity : ', specificity)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ALK9YuN3ht5t",
        "outputId": "caa41906-ca7b-4864-94dd-fb030b276e29"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7755102040816326"
            ]
          },
          "metadata": {},
          "execution_count": 112
        }
      ],
      "source": [
        "# to oimport the algo\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        " # to initialize the algo\n",
        "model1 = KNeighborsClassifier(n_neighbors=20)\n",
        " # to trained the algo\n",
        "model1.fit(Thyroid_X_train,Thyroid_Y_train)\n",
        "# to get the score \n",
        "model1.score(Thyroid_X_test, Thyroid_Y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "id": "_FHvxm2Mht5u"
      },
      "outputs": [],
      "source": [
        "# importing cr , cm \n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "# initialising the predict variable\n",
        "pred1 = model1.predict(Thyroid_X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "id": "X_Z1HoRFht5u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "281562fc-1fcc-4d17-f686-176df5c3f997"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.82      0.79        74\n",
            "           1       0.80      0.73      0.76        73\n",
            "\n",
            "    accuracy                           0.78       147\n",
            "   macro avg       0.78      0.78      0.77       147\n",
            "weighted avg       0.78      0.78      0.77       147\n",
            "\n",
            "\n",
            "Accuracy:  0.7755102040816326\n"
          ]
        }
      ],
      "source": [
        "# to print c_r\n",
        "print(classification_report(Thyroid_Y_test, pred1)) \n",
        "print()\n",
        "# reprinting the performance \n",
        "print('Accuracy: ', accuracy_score(Thyroid_Y_test, pred1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UiNed67sht5u",
        "outputId": "aecaf8f5-f294-47d0-811d-5b52a07773c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision by knn of testing data is: 0.776\n",
            "Recall by knn of testing data is: 0.776\n",
            "F1 score by knn of testing data is: 0.776\n"
          ]
        }
      ],
      "source": [
        "# various libraries for report\n",
        "from sklearn.metrics import precision_score,recall_score,f1_score\n",
        "# printing the performance \n",
        "print('Precision by knn of testing data is: %.3f' % precision_score(Thyroid_Y_test, pred1,average='micro')) \n",
        "# checking the precision value\n",
        "print('Recall by knn of testing data is: %.3f' % recall_score(Thyroid_Y_test, pred1,average='micro')) \n",
        "# checking the recall value\n",
        "print('F1 score by knn of testing data is: %.3f' % f1_score(Thyroid_Y_test, pred1,average='micro')) \n",
        "# checking the f2 score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "crJYh9LBht5u",
        "outputId": "a7ba99ed-9a6c-436b-cbd9-6c138af79da2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[61, 13],\n",
              "       [20, 53]])"
            ]
          },
          "metadata": {},
          "execution_count": 116
        }
      ],
      "source": [
        "# importing c_m\n",
        "from sklearn.metrics import confusion_matrix\n",
        "# initilizing cm\n",
        "cm = confusion_matrix(Thyroid_Y_test,pred1)\n",
        "cm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "yiVOWwqVht5u",
        "outputId": "97ebf839-26f2-4d01-a606-33350210e52d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(33.0, 0.5, 'Original Values')"
            ]
          },
          "metadata": {},
          "execution_count": 117
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# making heat map \n",
        "import seaborn as sns\n",
        "# assigning the color \n",
        "sns.heatmap(cm,annot=True, fmt='d' , cmap='Pastel1_r')\n",
        "# assigning the x-axis label\n",
        "plt.xlabel('Predicted values')\n",
        "# assigning the y-axis label\n",
        "plt.ylabel('Original Values')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Taj9vjNzht5u",
        "outputId": "58c2608b-129f-4bda-9911-80ebdf96ee69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sensitivity :  0.726027397260274\n",
            "Specificity :  0.8243243243243243\n"
          ]
        }
      ],
      "source": [
        "# printing Sensitivity accuracy \n",
        "sensitivity = cm[1,1]/(cm[1,1]+cm[1,0])\n",
        "# to get the score of the senstivity\n",
        "print('Sensitivity : ', sensitivity)\n",
        "# printing Specificity accuracy \n",
        "specificity = cm[0,0]/(cm[0,1]+cm[0,0])\n",
        "# to get the score\n",
        "print('Specificity : ', specificity)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4KgyUZC5hu-W",
        "outputId": "3fd8c11a-cf60-41ed-b252-84ba4ed6a135"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7619047619047619"
            ]
          },
          "metadata": {},
          "execution_count": 119
        }
      ],
      "source": [
        "# importing the algo\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "# initializing the algo\n",
        "model1 = KNeighborsClassifier(n_neighbors=25)\n",
        "# to trained the algo\n",
        "model1.fit(Thyroid_X_train,Thyroid_Y_train)\n",
        "# to get the score \n",
        "model1.score(Thyroid_X_test, Thyroid_Y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "id": "S-CvDKpnhu-X"
      },
      "outputs": [],
      "source": [
        "# importing cr , cm \n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "# initialising the predict variable\n",
        "pred1 = model1.predict(Thyroid_X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "id": "NsZfLvTYhu-X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a10e442c-69a3-4246-815c-bae00013df13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.80      0.77        74\n",
            "           1       0.78      0.73      0.75        73\n",
            "\n",
            "    accuracy                           0.76       147\n",
            "   macro avg       0.76      0.76      0.76       147\n",
            "weighted avg       0.76      0.76      0.76       147\n",
            "\n",
            "\n",
            "Accuracy:  0.7619047619047619\n"
          ]
        }
      ],
      "source": [
        "# to print the c_r\n",
        "print(classification_report(Thyroid_Y_test, pred1)) \n",
        "print()\n",
        "# reprinting the performance \n",
        "print('Accuracy: ', accuracy_score(Thyroid_Y_test, pred1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RLjkrYXrhu-Y",
        "outputId": "55748417-0974-4835-bc25-1fe67d7196b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision by knn of testing data is: 0.762\n",
            "Recall by knn of testing data is: 0.762\n",
            "F1 score by knn of testing data is: 0.762\n"
          ]
        }
      ],
      "source": [
        "# various libraries for report\n",
        "from sklearn.metrics import precision_score,recall_score,f1_score\n",
        "# printing the performance \n",
        "print('Precision by knn of testing data is: %.3f' % precision_score(Thyroid_Y_test, pred1,average='micro')) \n",
        "# checking the precision value\n",
        "print('Recall by knn of testing data is: %.3f' % recall_score(Thyroid_Y_test, pred1,average='micro')) \n",
        "# checking the recall value\n",
        "print('F1 score by knn of testing data is: %.3f' % f1_score(Thyroid_Y_test, pred1,average='micro')) \n",
        "# checking the f2 score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kfv65aWwhu-Z",
        "outputId": "f7f92a6d-7727-4443-c877-18133b6b308f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[59, 15],\n",
              "       [20, 53]])"
            ]
          },
          "metadata": {},
          "execution_count": 123
        }
      ],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "# for cm\n",
        "cm = confusion_matrix(Thyroid_Y_test,pred1)\n",
        "cm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "sSRoarFuhu-Z",
        "outputId": "5ac839ba-1b26-40b2-cade-6db6cdd42376"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(33.0, 0.5, 'Original Values')"
            ]
          },
          "metadata": {},
          "execution_count": 124
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# making heat map \n",
        "import seaborn as sns\n",
        "# assigning the color \n",
        "sns.heatmap(cm,annot=True, fmt='d' , cmap='Pastel1_r')\n",
        "# assigning the x-axis label\n",
        "plt.xlabel('Predicted values')\n",
        "# assigning the y-axis label\n",
        "plt.ylabel('Original Values')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5wX6daYWhu-a",
        "outputId": "d9db08cf-a867-42fd-fd2c-a7ef74b30ada"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sensitivity :  0.726027397260274\n",
            "Specificity :  0.7972972972972973\n"
          ]
        }
      ],
      "source": [
        "# printing Sensitivity accuracy \n",
        "sensitivity = cm[1,1]/(cm[1,1]+cm[1,0])\n",
        "# to get the score\n",
        "print('Sensitivity : ', sensitivity)\n",
        "# printing Specificity accuracy \n",
        "specificity = cm[0,0]/(cm[0,1]+cm[0,0])\n",
        "# to get the score of specificity\n",
        "print('Specificity : ', specificity)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URNPvvhJxfVy"
      },
      "source": [
        "# **ANN_Classifier**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {
        "id": "H7xr2_LRPHX2"
      },
      "outputs": [],
      "source": [
        "# librarry to categorical conversion\n",
        "from tensorflow.keras.utils import to_categorical, plot_model\n",
        "# to categorize the ytrain \n",
        "cat_Thyroid_Y_train = to_categorical(Thyroid_Y_train)\n",
        "# to categorize the ytest attribute\n",
        "cat_Thyroid_Y_test = to_categorical(Thyroid_Y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BHIHSz4IPEHA",
        "outputId": "0a90069f-ed93-4a93-e31c-58ce1df1f540"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(341, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 127
        }
      ],
      "source": [
        "cat_Thyroid_Y_train.shape # to get the shape "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "csFAidjbPy9X",
        "outputId": "fc61a535-1bc4-46f8-d704-ca86e6ba8198"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(147, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 128
        }
      ],
      "source": [
        "cat_Thyroid_Y_test.shape # to get the shape "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "id": "ltNlxbKlxe6T"
      },
      "outputs": [],
      "source": [
        "# importing the model ann\n",
        "from keras.models import Sequential\n",
        "# importing some crucial layer\n",
        "from keras.layers import Dense\n",
        "# initializing the algo\n",
        "model = Sequential()\n",
        "# adding hidden layer\n",
        "model.add(Dense(12, activation='relu'))\n",
        "# adding dense layer\n",
        "model.add(Dense(8,activation='relu'))\n",
        "# adding resulting layer\n",
        "model.add(Dense(2,activation='sigmoid'))\n",
        "# to debug the algorithm\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {
        "id": "iChqtq3dxe1y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0c261e8-470c-463d-9de6-1910d38424f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "35/35 [==============================] - 1s 6ms/step - loss: 3.0651 - accuracy: 0.4575 - val_loss: 2.2775 - val_accuracy: 0.3605\n",
            "Epoch 2/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 1.4907 - accuracy: 0.4340 - val_loss: 1.3705 - val_accuracy: 0.3878\n",
            "Epoch 3/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.9767 - accuracy: 0.4663 - val_loss: 0.9831 - val_accuracy: 0.4626\n",
            "Epoch 4/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.7411 - accuracy: 0.5249 - val_loss: 0.7561 - val_accuracy: 0.5578\n",
            "Epoch 5/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.6652 - accuracy: 0.5953 - val_loss: 0.6661 - val_accuracy: 0.6531\n",
            "Epoch 6/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.6261 - accuracy: 0.6100 - val_loss: 0.6232 - val_accuracy: 0.6735\n",
            "Epoch 7/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.6168 - accuracy: 0.6100 - val_loss: 0.6012 - val_accuracy: 0.6735\n",
            "Epoch 8/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.6056 - accuracy: 0.6393 - val_loss: 0.5934 - val_accuracy: 0.7007\n",
            "Epoch 9/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.5998 - accuracy: 0.6716 - val_loss: 0.5651 - val_accuracy: 0.7075\n",
            "Epoch 10/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.5836 - accuracy: 0.7361 - val_loss: 0.5634 - val_accuracy: 0.7075\n",
            "Epoch 11/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.5823 - accuracy: 0.7273 - val_loss: 0.5514 - val_accuracy: 0.7211\n",
            "Epoch 12/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.5682 - accuracy: 0.7243 - val_loss: 0.5517 - val_accuracy: 0.7143\n",
            "Epoch 13/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.5541 - accuracy: 0.7331 - val_loss: 0.5406 - val_accuracy: 0.7279\n",
            "Epoch 14/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.5496 - accuracy: 0.7302 - val_loss: 0.5592 - val_accuracy: 0.7143\n",
            "Epoch 15/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.5503 - accuracy: 0.7243 - val_loss: 0.5368 - val_accuracy: 0.7279\n",
            "Epoch 16/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.5517 - accuracy: 0.7302 - val_loss: 0.5526 - val_accuracy: 0.7075\n",
            "Epoch 17/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.5338 - accuracy: 0.7243 - val_loss: 0.5451 - val_accuracy: 0.7143\n",
            "Epoch 18/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.5373 - accuracy: 0.7214 - val_loss: 0.5379 - val_accuracy: 0.7075\n",
            "Epoch 19/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.5279 - accuracy: 0.7243 - val_loss: 0.5430 - val_accuracy: 0.6871\n",
            "Epoch 20/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.5232 - accuracy: 0.7302 - val_loss: 0.5486 - val_accuracy: 0.6871\n",
            "Epoch 21/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.5212 - accuracy: 0.7214 - val_loss: 0.5758 - val_accuracy: 0.6395\n",
            "Epoch 22/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.5357 - accuracy: 0.7126 - val_loss: 0.5437 - val_accuracy: 0.7075\n",
            "Epoch 23/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.5124 - accuracy: 0.7243 - val_loss: 0.5376 - val_accuracy: 0.7143\n",
            "Epoch 24/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.5141 - accuracy: 0.7243 - val_loss: 0.5549 - val_accuracy: 0.7143\n",
            "Epoch 25/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.5059 - accuracy: 0.7273 - val_loss: 0.5352 - val_accuracy: 0.7007\n",
            "Epoch 26/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.5001 - accuracy: 0.7361 - val_loss: 0.5618 - val_accuracy: 0.6395\n",
            "Epoch 27/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.4989 - accuracy: 0.7449 - val_loss: 0.5700 - val_accuracy: 0.6599\n",
            "Epoch 28/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.4932 - accuracy: 0.7331 - val_loss: 0.5428 - val_accuracy: 0.7075\n",
            "Epoch 29/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.4839 - accuracy: 0.7302 - val_loss: 0.5383 - val_accuracy: 0.7279\n",
            "Epoch 30/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.4796 - accuracy: 0.7419 - val_loss: 0.5543 - val_accuracy: 0.6871\n",
            "Epoch 31/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.4783 - accuracy: 0.7507 - val_loss: 0.5440 - val_accuracy: 0.7007\n",
            "Epoch 32/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.4728 - accuracy: 0.7478 - val_loss: 0.5384 - val_accuracy: 0.7007\n",
            "Epoch 33/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.4702 - accuracy: 0.7507 - val_loss: 0.5809 - val_accuracy: 0.6531\n",
            "Epoch 34/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.4689 - accuracy: 0.7478 - val_loss: 0.5474 - val_accuracy: 0.7007\n",
            "Epoch 35/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.4757 - accuracy: 0.7390 - val_loss: 0.5405 - val_accuracy: 0.7143\n",
            "Epoch 36/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.4573 - accuracy: 0.7683 - val_loss: 0.5511 - val_accuracy: 0.6939\n",
            "Epoch 37/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.4765 - accuracy: 0.7507 - val_loss: 0.5644 - val_accuracy: 0.6599\n",
            "Epoch 38/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.4691 - accuracy: 0.7801 - val_loss: 0.5579 - val_accuracy: 0.6871\n",
            "Epoch 39/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.4880 - accuracy: 0.7537 - val_loss: 0.5384 - val_accuracy: 0.7415\n",
            "Epoch 40/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.4681 - accuracy: 0.7654 - val_loss: 0.5461 - val_accuracy: 0.7075\n",
            "Epoch 41/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.4568 - accuracy: 0.7654 - val_loss: 0.5446 - val_accuracy: 0.6871\n",
            "Epoch 42/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.4523 - accuracy: 0.7654 - val_loss: 0.5443 - val_accuracy: 0.6939\n",
            "Epoch 43/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.4593 - accuracy: 0.7595 - val_loss: 0.5352 - val_accuracy: 0.7347\n",
            "Epoch 44/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.4560 - accuracy: 0.7859 - val_loss: 0.5576 - val_accuracy: 0.6871\n",
            "Epoch 45/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.4441 - accuracy: 0.7683 - val_loss: 0.5421 - val_accuracy: 0.7075\n",
            "Epoch 46/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.4429 - accuracy: 0.7742 - val_loss: 0.5493 - val_accuracy: 0.7143\n",
            "Epoch 47/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.4402 - accuracy: 0.7683 - val_loss: 0.5433 - val_accuracy: 0.7075\n",
            "Epoch 48/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.4521 - accuracy: 0.7654 - val_loss: 0.5187 - val_accuracy: 0.7551\n",
            "Epoch 49/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.4452 - accuracy: 0.7830 - val_loss: 0.5466 - val_accuracy: 0.7143\n",
            "Epoch 50/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.4394 - accuracy: 0.7918 - val_loss: 0.5575 - val_accuracy: 0.7143\n",
            "Epoch 51/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.4450 - accuracy: 0.7683 - val_loss: 0.5478 - val_accuracy: 0.7347\n",
            "Epoch 52/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.4463 - accuracy: 0.7683 - val_loss: 0.5603 - val_accuracy: 0.7075\n",
            "Epoch 53/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.4400 - accuracy: 0.7742 - val_loss: 0.5561 - val_accuracy: 0.7347\n",
            "Epoch 54/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.4423 - accuracy: 0.7830 - val_loss: 0.5529 - val_accuracy: 0.7143\n",
            "Epoch 55/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.4372 - accuracy: 0.7947 - val_loss: 0.5453 - val_accuracy: 0.7279\n",
            "Epoch 56/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.4427 - accuracy: 0.7889 - val_loss: 0.5814 - val_accuracy: 0.7007\n",
            "Epoch 57/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.4456 - accuracy: 0.7830 - val_loss: 0.5471 - val_accuracy: 0.7347\n",
            "Epoch 58/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.4336 - accuracy: 0.7947 - val_loss: 0.5671 - val_accuracy: 0.7347\n",
            "Epoch 59/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.4334 - accuracy: 0.7801 - val_loss: 0.5542 - val_accuracy: 0.7279\n",
            "Epoch 60/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.4519 - accuracy: 0.7683 - val_loss: 0.5466 - val_accuracy: 0.7619\n",
            "Epoch 61/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.4411 - accuracy: 0.7859 - val_loss: 0.5714 - val_accuracy: 0.7279\n",
            "Epoch 62/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.4301 - accuracy: 0.7830 - val_loss: 0.5600 - val_accuracy: 0.7279\n",
            "Epoch 63/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.4344 - accuracy: 0.7889 - val_loss: 0.5604 - val_accuracy: 0.7415\n",
            "Epoch 64/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.4332 - accuracy: 0.7889 - val_loss: 0.5784 - val_accuracy: 0.7347\n",
            "Epoch 65/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.4305 - accuracy: 0.7947 - val_loss: 0.5608 - val_accuracy: 0.7347\n",
            "Epoch 66/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.4327 - accuracy: 0.7918 - val_loss: 0.5659 - val_accuracy: 0.7415\n",
            "Epoch 67/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.4284 - accuracy: 0.8035 - val_loss: 0.5687 - val_accuracy: 0.7279\n",
            "Epoch 68/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.4386 - accuracy: 0.7830 - val_loss: 0.5532 - val_accuracy: 0.7619\n",
            "Epoch 69/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.4311 - accuracy: 0.8006 - val_loss: 0.5858 - val_accuracy: 0.7211\n",
            "Epoch 70/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.4263 - accuracy: 0.7947 - val_loss: 0.5484 - val_accuracy: 0.7347\n",
            "Epoch 71/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.4206 - accuracy: 0.7830 - val_loss: 0.5647 - val_accuracy: 0.7483\n",
            "Epoch 72/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.4444 - accuracy: 0.7713 - val_loss: 0.5791 - val_accuracy: 0.7075\n",
            "Epoch 73/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.4257 - accuracy: 0.7830 - val_loss: 0.5673 - val_accuracy: 0.7415\n",
            "Epoch 74/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.4223 - accuracy: 0.7918 - val_loss: 0.5615 - val_accuracy: 0.7211\n",
            "Epoch 75/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.4466 - accuracy: 0.8006 - val_loss: 0.5539 - val_accuracy: 0.7619\n",
            "Epoch 76/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.4347 - accuracy: 0.7918 - val_loss: 0.5589 - val_accuracy: 0.7347\n",
            "Epoch 77/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.4247 - accuracy: 0.7918 - val_loss: 0.5818 - val_accuracy: 0.7211\n",
            "Epoch 78/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.4265 - accuracy: 0.7801 - val_loss: 0.5740 - val_accuracy: 0.7415\n",
            "Epoch 79/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.4249 - accuracy: 0.7771 - val_loss: 0.5566 - val_accuracy: 0.7619\n",
            "Epoch 80/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.4317 - accuracy: 0.8094 - val_loss: 0.5481 - val_accuracy: 0.7415\n",
            "Epoch 81/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.4259 - accuracy: 0.7830 - val_loss: 0.5787 - val_accuracy: 0.7415\n",
            "Epoch 82/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.4251 - accuracy: 0.7918 - val_loss: 0.5730 - val_accuracy: 0.7143\n",
            "Epoch 83/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.4184 - accuracy: 0.8006 - val_loss: 0.5683 - val_accuracy: 0.7211\n",
            "Epoch 84/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.4141 - accuracy: 0.8006 - val_loss: 0.5584 - val_accuracy: 0.7211\n",
            "Epoch 85/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.4220 - accuracy: 0.7947 - val_loss: 0.5766 - val_accuracy: 0.7007\n",
            "Epoch 86/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.4335 - accuracy: 0.7801 - val_loss: 0.5501 - val_accuracy: 0.7415\n",
            "Epoch 87/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.4192 - accuracy: 0.7918 - val_loss: 0.5653 - val_accuracy: 0.7279\n",
            "Epoch 88/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.4180 - accuracy: 0.8006 - val_loss: 0.5630 - val_accuracy: 0.7075\n",
            "Epoch 89/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.4134 - accuracy: 0.7859 - val_loss: 0.5875 - val_accuracy: 0.7211\n",
            "Epoch 90/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.4264 - accuracy: 0.7859 - val_loss: 0.5598 - val_accuracy: 0.7347\n",
            "Epoch 91/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.4147 - accuracy: 0.8035 - val_loss: 0.5519 - val_accuracy: 0.7143\n",
            "Epoch 92/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.4181 - accuracy: 0.7918 - val_loss: 0.5989 - val_accuracy: 0.7007\n",
            "Epoch 93/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.4194 - accuracy: 0.8006 - val_loss: 0.5644 - val_accuracy: 0.7211\n",
            "Epoch 94/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.4088 - accuracy: 0.8006 - val_loss: 0.5974 - val_accuracy: 0.7211\n",
            "Epoch 95/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.5165 - accuracy: 0.7067 - val_loss: 0.5738 - val_accuracy: 0.7211\n",
            "Epoch 96/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.4251 - accuracy: 0.8006 - val_loss: 0.5533 - val_accuracy: 0.7551\n",
            "Epoch 97/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.4169 - accuracy: 0.7859 - val_loss: 0.5739 - val_accuracy: 0.7143\n",
            "Epoch 98/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.4098 - accuracy: 0.8065 - val_loss: 0.5614 - val_accuracy: 0.7279\n",
            "Epoch 99/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.4118 - accuracy: 0.7918 - val_loss: 0.5673 - val_accuracy: 0.7143\n",
            "Epoch 100/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.4068 - accuracy: 0.8006 - val_loss: 0.5702 - val_accuracy: 0.7279\n",
            "Epoch 101/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.4062 - accuracy: 0.8123 - val_loss: 0.5571 - val_accuracy: 0.7211\n",
            "Epoch 102/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.4056 - accuracy: 0.7977 - val_loss: 0.5691 - val_accuracy: 0.7279\n",
            "Epoch 103/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.4065 - accuracy: 0.8152 - val_loss: 0.5694 - val_accuracy: 0.7415\n",
            "Epoch 104/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.4089 - accuracy: 0.8035 - val_loss: 0.5632 - val_accuracy: 0.7143\n",
            "Epoch 105/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.4061 - accuracy: 0.8006 - val_loss: 0.5755 - val_accuracy: 0.7211\n",
            "Epoch 106/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.4082 - accuracy: 0.8182 - val_loss: 0.5788 - val_accuracy: 0.7415\n",
            "Epoch 107/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.4054 - accuracy: 0.7977 - val_loss: 0.5837 - val_accuracy: 0.7347\n",
            "Epoch 108/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.4047 - accuracy: 0.8094 - val_loss: 0.5755 - val_accuracy: 0.7279\n",
            "Epoch 109/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.4012 - accuracy: 0.8182 - val_loss: 0.5833 - val_accuracy: 0.7075\n",
            "Epoch 110/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.4025 - accuracy: 0.7977 - val_loss: 0.5696 - val_accuracy: 0.7211\n",
            "Epoch 111/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.4102 - accuracy: 0.8065 - val_loss: 0.5623 - val_accuracy: 0.7347\n",
            "Epoch 112/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.4027 - accuracy: 0.8182 - val_loss: 0.5621 - val_accuracy: 0.7347\n",
            "Epoch 113/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.4012 - accuracy: 0.8240 - val_loss: 0.5850 - val_accuracy: 0.7211\n",
            "Epoch 114/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.4151 - accuracy: 0.8065 - val_loss: 0.5622 - val_accuracy: 0.7279\n",
            "Epoch 115/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.4021 - accuracy: 0.8182 - val_loss: 0.5644 - val_accuracy: 0.7211\n",
            "Epoch 116/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.4063 - accuracy: 0.8065 - val_loss: 0.5588 - val_accuracy: 0.7279\n",
            "Epoch 117/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.4274 - accuracy: 0.8006 - val_loss: 0.5665 - val_accuracy: 0.7687\n",
            "Epoch 118/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.4084 - accuracy: 0.8006 - val_loss: 0.5747 - val_accuracy: 0.7211\n",
            "Epoch 119/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.4097 - accuracy: 0.7947 - val_loss: 0.5852 - val_accuracy: 0.7211\n",
            "Epoch 120/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.4091 - accuracy: 0.7947 - val_loss: 0.6196 - val_accuracy: 0.7007\n",
            "Epoch 121/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.4013 - accuracy: 0.8182 - val_loss: 0.5670 - val_accuracy: 0.7347\n",
            "Epoch 122/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3960 - accuracy: 0.8123 - val_loss: 0.6038 - val_accuracy: 0.7211\n",
            "Epoch 123/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3971 - accuracy: 0.8240 - val_loss: 0.5724 - val_accuracy: 0.7279\n",
            "Epoch 124/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3999 - accuracy: 0.8152 - val_loss: 0.5846 - val_accuracy: 0.7279\n",
            "Epoch 125/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3943 - accuracy: 0.8299 - val_loss: 0.6095 - val_accuracy: 0.7279\n",
            "Epoch 126/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3979 - accuracy: 0.8006 - val_loss: 0.5593 - val_accuracy: 0.7415\n",
            "Epoch 127/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3936 - accuracy: 0.8240 - val_loss: 0.5783 - val_accuracy: 0.7551\n",
            "Epoch 128/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.4043 - accuracy: 0.8240 - val_loss: 0.5886 - val_accuracy: 0.7483\n",
            "Epoch 129/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3926 - accuracy: 0.8211 - val_loss: 0.6001 - val_accuracy: 0.7279\n",
            "Epoch 130/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.4194 - accuracy: 0.8065 - val_loss: 0.5450 - val_accuracy: 0.7279\n",
            "Epoch 131/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.4149 - accuracy: 0.8123 - val_loss: 0.5533 - val_accuracy: 0.7483\n",
            "Epoch 132/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3993 - accuracy: 0.8094 - val_loss: 0.6042 - val_accuracy: 0.7279\n",
            "Epoch 133/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3967 - accuracy: 0.7801 - val_loss: 0.5647 - val_accuracy: 0.7415\n",
            "Epoch 134/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3977 - accuracy: 0.8094 - val_loss: 0.5922 - val_accuracy: 0.7279\n",
            "Epoch 135/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3936 - accuracy: 0.8182 - val_loss: 0.5530 - val_accuracy: 0.7415\n",
            "Epoch 136/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3921 - accuracy: 0.8182 - val_loss: 0.5987 - val_accuracy: 0.7347\n",
            "Epoch 137/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3942 - accuracy: 0.8270 - val_loss: 0.5438 - val_accuracy: 0.7483\n",
            "Epoch 138/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3893 - accuracy: 0.8123 - val_loss: 0.5966 - val_accuracy: 0.7347\n",
            "Epoch 139/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3963 - accuracy: 0.8240 - val_loss: 0.5682 - val_accuracy: 0.7551\n",
            "Epoch 140/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3924 - accuracy: 0.8094 - val_loss: 0.5810 - val_accuracy: 0.7551\n",
            "Epoch 141/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3904 - accuracy: 0.8240 - val_loss: 0.5503 - val_accuracy: 0.7279\n",
            "Epoch 142/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3934 - accuracy: 0.7947 - val_loss: 0.5760 - val_accuracy: 0.7483\n",
            "Epoch 143/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3954 - accuracy: 0.8006 - val_loss: 0.5692 - val_accuracy: 0.7483\n",
            "Epoch 144/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3905 - accuracy: 0.8299 - val_loss: 0.6081 - val_accuracy: 0.7211\n",
            "Epoch 145/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3840 - accuracy: 0.8270 - val_loss: 0.5651 - val_accuracy: 0.7415\n",
            "Epoch 146/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3885 - accuracy: 0.8152 - val_loss: 0.5698 - val_accuracy: 0.7483\n",
            "Epoch 147/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3880 - accuracy: 0.8182 - val_loss: 0.5786 - val_accuracy: 0.7551\n",
            "Epoch 148/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3925 - accuracy: 0.8270 - val_loss: 0.5841 - val_accuracy: 0.7415\n",
            "Epoch 149/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3860 - accuracy: 0.8270 - val_loss: 0.5979 - val_accuracy: 0.7551\n",
            "Epoch 150/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3919 - accuracy: 0.8270 - val_loss: 0.5743 - val_accuracy: 0.7551\n",
            "Epoch 151/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3787 - accuracy: 0.8299 - val_loss: 0.6052 - val_accuracy: 0.7619\n",
            "Epoch 152/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3856 - accuracy: 0.8270 - val_loss: 0.5968 - val_accuracy: 0.7211\n",
            "Epoch 153/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3854 - accuracy: 0.8211 - val_loss: 0.6009 - val_accuracy: 0.7483\n",
            "Epoch 154/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3874 - accuracy: 0.8328 - val_loss: 0.5872 - val_accuracy: 0.7143\n",
            "Epoch 155/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.4045 - accuracy: 0.7889 - val_loss: 0.5539 - val_accuracy: 0.7279\n",
            "Epoch 156/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3852 - accuracy: 0.8328 - val_loss: 0.5905 - val_accuracy: 0.7483\n",
            "Epoch 157/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3826 - accuracy: 0.8270 - val_loss: 0.5693 - val_accuracy: 0.7415\n",
            "Epoch 158/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3826 - accuracy: 0.8152 - val_loss: 0.6061 - val_accuracy: 0.7211\n",
            "Epoch 159/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3793 - accuracy: 0.8182 - val_loss: 0.5708 - val_accuracy: 0.7551\n",
            "Epoch 160/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3920 - accuracy: 0.8094 - val_loss: 0.5994 - val_accuracy: 0.7551\n",
            "Epoch 161/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3833 - accuracy: 0.8299 - val_loss: 0.6017 - val_accuracy: 0.7415\n",
            "Epoch 162/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3794 - accuracy: 0.8358 - val_loss: 0.5827 - val_accuracy: 0.7415\n",
            "Epoch 163/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3800 - accuracy: 0.8182 - val_loss: 0.5887 - val_accuracy: 0.7551\n",
            "Epoch 164/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3783 - accuracy: 0.8299 - val_loss: 0.6354 - val_accuracy: 0.7279\n",
            "Epoch 165/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3816 - accuracy: 0.8182 - val_loss: 0.5900 - val_accuracy: 0.7415\n",
            "Epoch 166/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3894 - accuracy: 0.8035 - val_loss: 0.5843 - val_accuracy: 0.7483\n",
            "Epoch 167/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3862 - accuracy: 0.8182 - val_loss: 0.6177 - val_accuracy: 0.7279\n",
            "Epoch 168/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3831 - accuracy: 0.8240 - val_loss: 0.5705 - val_accuracy: 0.7415\n",
            "Epoch 169/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3803 - accuracy: 0.8270 - val_loss: 0.6168 - val_accuracy: 0.7415\n",
            "Epoch 170/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3837 - accuracy: 0.8123 - val_loss: 0.6257 - val_accuracy: 0.7279\n",
            "Epoch 171/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3918 - accuracy: 0.8152 - val_loss: 0.6477 - val_accuracy: 0.7075\n",
            "Epoch 172/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3886 - accuracy: 0.8152 - val_loss: 0.5883 - val_accuracy: 0.7483\n",
            "Epoch 173/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3778 - accuracy: 0.8240 - val_loss: 0.6461 - val_accuracy: 0.7347\n",
            "Epoch 174/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3859 - accuracy: 0.8123 - val_loss: 0.6053 - val_accuracy: 0.7347\n",
            "Epoch 175/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3761 - accuracy: 0.8270 - val_loss: 0.6194 - val_accuracy: 0.7483\n",
            "Epoch 176/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3772 - accuracy: 0.8240 - val_loss: 0.5997 - val_accuracy: 0.7415\n",
            "Epoch 177/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3721 - accuracy: 0.8211 - val_loss: 0.5928 - val_accuracy: 0.7483\n",
            "Epoch 178/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3754 - accuracy: 0.8328 - val_loss: 0.6101 - val_accuracy: 0.7415\n",
            "Epoch 179/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3839 - accuracy: 0.8270 - val_loss: 0.6156 - val_accuracy: 0.7415\n",
            "Epoch 180/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3983 - accuracy: 0.8182 - val_loss: 0.5926 - val_accuracy: 0.7415\n",
            "Epoch 181/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3747 - accuracy: 0.8328 - val_loss: 0.6272 - val_accuracy: 0.7211\n",
            "Epoch 182/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3721 - accuracy: 0.8211 - val_loss: 0.5988 - val_accuracy: 0.7415\n",
            "Epoch 183/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3716 - accuracy: 0.8358 - val_loss: 0.5876 - val_accuracy: 0.7483\n",
            "Epoch 184/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3701 - accuracy: 0.8299 - val_loss: 0.6279 - val_accuracy: 0.7347\n",
            "Epoch 185/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3729 - accuracy: 0.8299 - val_loss: 0.6001 - val_accuracy: 0.7483\n",
            "Epoch 186/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3747 - accuracy: 0.8270 - val_loss: 0.5913 - val_accuracy: 0.7279\n",
            "Epoch 187/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3834 - accuracy: 0.8065 - val_loss: 0.6135 - val_accuracy: 0.7619\n",
            "Epoch 188/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3923 - accuracy: 0.8182 - val_loss: 0.6244 - val_accuracy: 0.7483\n",
            "Epoch 189/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3749 - accuracy: 0.8299 - val_loss: 0.5884 - val_accuracy: 0.7415\n",
            "Epoch 190/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3846 - accuracy: 0.8182 - val_loss: 0.5981 - val_accuracy: 0.7483\n",
            "Epoch 191/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3897 - accuracy: 0.8065 - val_loss: 0.6059 - val_accuracy: 0.7483\n",
            "Epoch 192/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3679 - accuracy: 0.8328 - val_loss: 0.6256 - val_accuracy: 0.7415\n",
            "Epoch 193/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3833 - accuracy: 0.8240 - val_loss: 0.6255 - val_accuracy: 0.7483\n",
            "Epoch 194/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3761 - accuracy: 0.8211 - val_loss: 0.6155 - val_accuracy: 0.7415\n",
            "Epoch 195/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3717 - accuracy: 0.8270 - val_loss: 0.6129 - val_accuracy: 0.7415\n",
            "Epoch 196/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3778 - accuracy: 0.8240 - val_loss: 0.5880 - val_accuracy: 0.7415\n",
            "Epoch 197/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3716 - accuracy: 0.8299 - val_loss: 0.6049 - val_accuracy: 0.7415\n",
            "Epoch 198/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3848 - accuracy: 0.8065 - val_loss: 0.6213 - val_accuracy: 0.7483\n",
            "Epoch 199/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3918 - accuracy: 0.8123 - val_loss: 0.5998 - val_accuracy: 0.7415\n",
            "Epoch 200/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3775 - accuracy: 0.8182 - val_loss: 0.5845 - val_accuracy: 0.7347\n",
            "Epoch 201/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3778 - accuracy: 0.8035 - val_loss: 0.6709 - val_accuracy: 0.7007\n",
            "Epoch 202/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3816 - accuracy: 0.8299 - val_loss: 0.6415 - val_accuracy: 0.7483\n",
            "Epoch 203/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3674 - accuracy: 0.8299 - val_loss: 0.6321 - val_accuracy: 0.7483\n",
            "Epoch 204/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3703 - accuracy: 0.8328 - val_loss: 0.6060 - val_accuracy: 0.7483\n",
            "Epoch 205/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3695 - accuracy: 0.8328 - val_loss: 0.5860 - val_accuracy: 0.7415\n",
            "Epoch 206/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3693 - accuracy: 0.8299 - val_loss: 0.6152 - val_accuracy: 0.7483\n",
            "Epoch 207/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3653 - accuracy: 0.8328 - val_loss: 0.6317 - val_accuracy: 0.7415\n",
            "Epoch 208/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3699 - accuracy: 0.8270 - val_loss: 0.6270 - val_accuracy: 0.7415\n",
            "Epoch 209/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3840 - accuracy: 0.8065 - val_loss: 0.6049 - val_accuracy: 0.7279\n",
            "Epoch 210/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3706 - accuracy: 0.8358 - val_loss: 0.6461 - val_accuracy: 0.7347\n",
            "Epoch 211/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.4275 - accuracy: 0.7977 - val_loss: 0.5846 - val_accuracy: 0.7551\n",
            "Epoch 212/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3976 - accuracy: 0.8123 - val_loss: 0.6206 - val_accuracy: 0.7347\n",
            "Epoch 213/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.4106 - accuracy: 0.8065 - val_loss: 0.6590 - val_accuracy: 0.7143\n",
            "Epoch 214/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3814 - accuracy: 0.8240 - val_loss: 0.6838 - val_accuracy: 0.7143\n",
            "Epoch 215/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3784 - accuracy: 0.8240 - val_loss: 0.5442 - val_accuracy: 0.7483\n",
            "Epoch 216/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3712 - accuracy: 0.8270 - val_loss: 0.6492 - val_accuracy: 0.7415\n",
            "Epoch 217/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3650 - accuracy: 0.8358 - val_loss: 0.6258 - val_accuracy: 0.7483\n",
            "Epoch 218/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3673 - accuracy: 0.8299 - val_loss: 0.6865 - val_accuracy: 0.7347\n",
            "Epoch 219/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3664 - accuracy: 0.8270 - val_loss: 0.6767 - val_accuracy: 0.7415\n",
            "Epoch 220/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3641 - accuracy: 0.8270 - val_loss: 0.6672 - val_accuracy: 0.7279\n",
            "Epoch 221/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3649 - accuracy: 0.8299 - val_loss: 0.6494 - val_accuracy: 0.7415\n",
            "Epoch 222/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3699 - accuracy: 0.8270 - val_loss: 0.6642 - val_accuracy: 0.7415\n",
            "Epoch 223/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3728 - accuracy: 0.8182 - val_loss: 0.6384 - val_accuracy: 0.7347\n",
            "Epoch 224/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3722 - accuracy: 0.8094 - val_loss: 0.6978 - val_accuracy: 0.7211\n",
            "Epoch 225/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3800 - accuracy: 0.8035 - val_loss: 0.6486 - val_accuracy: 0.7551\n",
            "Epoch 226/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3762 - accuracy: 0.8240 - val_loss: 0.6572 - val_accuracy: 0.7279\n",
            "Epoch 227/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3670 - accuracy: 0.8270 - val_loss: 0.6814 - val_accuracy: 0.7347\n",
            "Epoch 228/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3807 - accuracy: 0.8152 - val_loss: 0.6306 - val_accuracy: 0.7415\n",
            "Epoch 229/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3706 - accuracy: 0.8152 - val_loss: 0.6899 - val_accuracy: 0.7347\n",
            "Epoch 230/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3796 - accuracy: 0.8152 - val_loss: 0.6412 - val_accuracy: 0.7415\n",
            "Epoch 231/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3702 - accuracy: 0.8240 - val_loss: 0.6906 - val_accuracy: 0.7415\n",
            "Epoch 232/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3669 - accuracy: 0.8240 - val_loss: 0.6850 - val_accuracy: 0.7279\n",
            "Epoch 233/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3596 - accuracy: 0.8328 - val_loss: 0.6599 - val_accuracy: 0.7415\n",
            "Epoch 234/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3666 - accuracy: 0.8270 - val_loss: 0.6379 - val_accuracy: 0.7211\n",
            "Epoch 235/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3655 - accuracy: 0.8299 - val_loss: 0.6955 - val_accuracy: 0.7211\n",
            "Epoch 236/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3618 - accuracy: 0.8299 - val_loss: 0.6738 - val_accuracy: 0.7279\n",
            "Epoch 237/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3619 - accuracy: 0.8475 - val_loss: 0.6792 - val_accuracy: 0.7415\n",
            "Epoch 238/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3625 - accuracy: 0.8475 - val_loss: 0.6672 - val_accuracy: 0.7279\n",
            "Epoch 239/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3588 - accuracy: 0.8358 - val_loss: 0.6670 - val_accuracy: 0.7347\n",
            "Epoch 240/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3660 - accuracy: 0.8270 - val_loss: 0.6062 - val_accuracy: 0.7075\n",
            "Epoch 241/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.4618 - accuracy: 0.7654 - val_loss: 0.6399 - val_accuracy: 0.7415\n",
            "Epoch 242/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3967 - accuracy: 0.8094 - val_loss: 0.6931 - val_accuracy: 0.7143\n",
            "Epoch 243/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3797 - accuracy: 0.8152 - val_loss: 0.6579 - val_accuracy: 0.7347\n",
            "Epoch 244/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3627 - accuracy: 0.8299 - val_loss: 0.6944 - val_accuracy: 0.7415\n",
            "Epoch 245/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3672 - accuracy: 0.8328 - val_loss: 0.6697 - val_accuracy: 0.7483\n",
            "Epoch 246/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3870 - accuracy: 0.8211 - val_loss: 0.6815 - val_accuracy: 0.7347\n",
            "Epoch 247/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3646 - accuracy: 0.8358 - val_loss: 0.6724 - val_accuracy: 0.7347\n",
            "Epoch 248/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3582 - accuracy: 0.8299 - val_loss: 0.6736 - val_accuracy: 0.7415\n",
            "Epoch 249/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3594 - accuracy: 0.8387 - val_loss: 0.7010 - val_accuracy: 0.7347\n",
            "Epoch 250/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3620 - accuracy: 0.8387 - val_loss: 0.7251 - val_accuracy: 0.7347\n",
            "Epoch 251/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3636 - accuracy: 0.8299 - val_loss: 0.7050 - val_accuracy: 0.7347\n",
            "Epoch 252/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3714 - accuracy: 0.8182 - val_loss: 0.6472 - val_accuracy: 0.7347\n",
            "Epoch 253/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3852 - accuracy: 0.8240 - val_loss: 0.6760 - val_accuracy: 0.7415\n",
            "Epoch 254/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3810 - accuracy: 0.8270 - val_loss: 0.7222 - val_accuracy: 0.7483\n",
            "Epoch 255/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3617 - accuracy: 0.8387 - val_loss: 0.6710 - val_accuracy: 0.7415\n",
            "Epoch 256/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3628 - accuracy: 0.8328 - val_loss: 0.6661 - val_accuracy: 0.7415\n",
            "Epoch 257/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3602 - accuracy: 0.8387 - val_loss: 0.7199 - val_accuracy: 0.7279\n",
            "Epoch 258/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3595 - accuracy: 0.8270 - val_loss: 0.6674 - val_accuracy: 0.7415\n",
            "Epoch 259/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3610 - accuracy: 0.8475 - val_loss: 0.7304 - val_accuracy: 0.7347\n",
            "Epoch 260/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3722 - accuracy: 0.8211 - val_loss: 0.7188 - val_accuracy: 0.7279\n",
            "Epoch 261/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3670 - accuracy: 0.8387 - val_loss: 0.7168 - val_accuracy: 0.7347\n",
            "Epoch 262/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3541 - accuracy: 0.8299 - val_loss: 0.7196 - val_accuracy: 0.7551\n",
            "Epoch 263/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3651 - accuracy: 0.8299 - val_loss: 0.7011 - val_accuracy: 0.7415\n",
            "Epoch 264/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3605 - accuracy: 0.8358 - val_loss: 0.6603 - val_accuracy: 0.7415\n",
            "Epoch 265/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3560 - accuracy: 0.8270 - val_loss: 0.7053 - val_accuracy: 0.7415\n",
            "Epoch 266/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3612 - accuracy: 0.8358 - val_loss: 0.6602 - val_accuracy: 0.7415\n",
            "Epoch 267/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3701 - accuracy: 0.8299 - val_loss: 0.7423 - val_accuracy: 0.7347\n",
            "Epoch 268/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3641 - accuracy: 0.8240 - val_loss: 0.7262 - val_accuracy: 0.7279\n",
            "Epoch 269/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3612 - accuracy: 0.8328 - val_loss: 0.6602 - val_accuracy: 0.7415\n",
            "Epoch 270/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3650 - accuracy: 0.8270 - val_loss: 0.7096 - val_accuracy: 0.7483\n",
            "Epoch 271/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3727 - accuracy: 0.8270 - val_loss: 0.7204 - val_accuracy: 0.7483\n",
            "Epoch 272/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3598 - accuracy: 0.8299 - val_loss: 0.6733 - val_accuracy: 0.7483\n",
            "Epoch 273/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3583 - accuracy: 0.8328 - val_loss: 0.7306 - val_accuracy: 0.7415\n",
            "Epoch 274/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3518 - accuracy: 0.8328 - val_loss: 0.7187 - val_accuracy: 0.7279\n",
            "Epoch 275/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.4261 - accuracy: 0.7859 - val_loss: 0.7211 - val_accuracy: 0.7143\n",
            "Epoch 276/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3649 - accuracy: 0.8416 - val_loss: 0.7281 - val_accuracy: 0.7483\n",
            "Epoch 277/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3660 - accuracy: 0.8270 - val_loss: 0.7145 - val_accuracy: 0.7483\n",
            "Epoch 278/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.4325 - accuracy: 0.7713 - val_loss: 0.6813 - val_accuracy: 0.7415\n",
            "Epoch 279/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.4055 - accuracy: 0.7977 - val_loss: 0.7684 - val_accuracy: 0.7075\n",
            "Epoch 280/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.4080 - accuracy: 0.8006 - val_loss: 0.7006 - val_accuracy: 0.7279\n",
            "Epoch 281/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3700 - accuracy: 0.8299 - val_loss: 0.7564 - val_accuracy: 0.7347\n",
            "Epoch 282/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3595 - accuracy: 0.8328 - val_loss: 0.7421 - val_accuracy: 0.7483\n",
            "Epoch 283/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3560 - accuracy: 0.8446 - val_loss: 0.7411 - val_accuracy: 0.7415\n",
            "Epoch 284/1000\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.3544 - accuracy: 0.8358 - val_loss: 0.7557 - val_accuracy: 0.7415\n",
            "Epoch 285/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3581 - accuracy: 0.8240 - val_loss: 0.7743 - val_accuracy: 0.7347\n",
            "Epoch 286/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3590 - accuracy: 0.8387 - val_loss: 0.7495 - val_accuracy: 0.7415\n",
            "Epoch 287/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3526 - accuracy: 0.8416 - val_loss: 0.7997 - val_accuracy: 0.7279\n",
            "Epoch 288/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3529 - accuracy: 0.8328 - val_loss: 0.7337 - val_accuracy: 0.7415\n",
            "Epoch 289/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3511 - accuracy: 0.8416 - val_loss: 0.7547 - val_accuracy: 0.7347\n",
            "Epoch 290/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3526 - accuracy: 0.8328 - val_loss: 0.7277 - val_accuracy: 0.7415\n",
            "Epoch 291/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3519 - accuracy: 0.8358 - val_loss: 0.7306 - val_accuracy: 0.7279\n",
            "Epoch 292/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3516 - accuracy: 0.8299 - val_loss: 0.7618 - val_accuracy: 0.7415\n",
            "Epoch 293/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3594 - accuracy: 0.8387 - val_loss: 0.7147 - val_accuracy: 0.7415\n",
            "Epoch 294/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3523 - accuracy: 0.8299 - val_loss: 0.7419 - val_accuracy: 0.7483\n",
            "Epoch 295/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3498 - accuracy: 0.8387 - val_loss: 0.7648 - val_accuracy: 0.7279\n",
            "Epoch 296/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3503 - accuracy: 0.8358 - val_loss: 0.7542 - val_accuracy: 0.7347\n",
            "Epoch 297/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3558 - accuracy: 0.8358 - val_loss: 0.7750 - val_accuracy: 0.7211\n",
            "Epoch 298/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3561 - accuracy: 0.8240 - val_loss: 0.7593 - val_accuracy: 0.7279\n",
            "Epoch 299/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3537 - accuracy: 0.8270 - val_loss: 0.6972 - val_accuracy: 0.7347\n",
            "Epoch 300/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3609 - accuracy: 0.8358 - val_loss: 0.7498 - val_accuracy: 0.7279\n",
            "Epoch 301/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3535 - accuracy: 0.8387 - val_loss: 0.8083 - val_accuracy: 0.7279\n",
            "Epoch 302/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3520 - accuracy: 0.8240 - val_loss: 0.7578 - val_accuracy: 0.7279\n",
            "Epoch 303/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3487 - accuracy: 0.8416 - val_loss: 0.7244 - val_accuracy: 0.7551\n",
            "Epoch 304/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3563 - accuracy: 0.8299 - val_loss: 0.7571 - val_accuracy: 0.7415\n",
            "Epoch 305/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3509 - accuracy: 0.8358 - val_loss: 0.7245 - val_accuracy: 0.7551\n",
            "Epoch 306/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3512 - accuracy: 0.8416 - val_loss: 0.7570 - val_accuracy: 0.7415\n",
            "Epoch 307/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3537 - accuracy: 0.8328 - val_loss: 0.7097 - val_accuracy: 0.7551\n",
            "Epoch 308/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3950 - accuracy: 0.8006 - val_loss: 0.7243 - val_accuracy: 0.7347\n",
            "Epoch 309/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3632 - accuracy: 0.8328 - val_loss: 0.6963 - val_accuracy: 0.7483\n",
            "Epoch 310/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3586 - accuracy: 0.8387 - val_loss: 0.7133 - val_accuracy: 0.7551\n",
            "Epoch 311/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3536 - accuracy: 0.8358 - val_loss: 0.7861 - val_accuracy: 0.7415\n",
            "Epoch 312/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3506 - accuracy: 0.8416 - val_loss: 0.7496 - val_accuracy: 0.7415\n",
            "Epoch 313/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3475 - accuracy: 0.8446 - val_loss: 0.7917 - val_accuracy: 0.7415\n",
            "Epoch 314/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3476 - accuracy: 0.8387 - val_loss: 0.7512 - val_accuracy: 0.7415\n",
            "Epoch 315/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3472 - accuracy: 0.8387 - val_loss: 0.7650 - val_accuracy: 0.7483\n",
            "Epoch 316/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3645 - accuracy: 0.8211 - val_loss: 0.8657 - val_accuracy: 0.7483\n",
            "Epoch 317/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3561 - accuracy: 0.8270 - val_loss: 0.7705 - val_accuracy: 0.7483\n",
            "Epoch 318/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3532 - accuracy: 0.8416 - val_loss: 0.7716 - val_accuracy: 0.7279\n",
            "Epoch 319/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3462 - accuracy: 0.8387 - val_loss: 0.7573 - val_accuracy: 0.7347\n",
            "Epoch 320/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3663 - accuracy: 0.8270 - val_loss: 0.8189 - val_accuracy: 0.7347\n",
            "Epoch 321/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3462 - accuracy: 0.8387 - val_loss: 0.8076 - val_accuracy: 0.7279\n",
            "Epoch 322/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3504 - accuracy: 0.8416 - val_loss: 0.7271 - val_accuracy: 0.7347\n",
            "Epoch 323/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3520 - accuracy: 0.8534 - val_loss: 0.8217 - val_accuracy: 0.7483\n",
            "Epoch 324/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3600 - accuracy: 0.8240 - val_loss: 0.8312 - val_accuracy: 0.7007\n",
            "Epoch 325/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3665 - accuracy: 0.8270 - val_loss: 0.7804 - val_accuracy: 0.7483\n",
            "Epoch 326/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3468 - accuracy: 0.8504 - val_loss: 0.7758 - val_accuracy: 0.7415\n",
            "Epoch 327/1000\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.3539 - accuracy: 0.8387 - val_loss: 0.8075 - val_accuracy: 0.7347\n",
            "Epoch 328/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3472 - accuracy: 0.8387 - val_loss: 0.7484 - val_accuracy: 0.7415\n",
            "Epoch 329/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3508 - accuracy: 0.8387 - val_loss: 0.8111 - val_accuracy: 0.7415\n",
            "Epoch 330/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3449 - accuracy: 0.8416 - val_loss: 0.8135 - val_accuracy: 0.7143\n",
            "Epoch 331/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3479 - accuracy: 0.8328 - val_loss: 0.7632 - val_accuracy: 0.7347\n",
            "Epoch 332/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3441 - accuracy: 0.8387 - val_loss: 0.7998 - val_accuracy: 0.7483\n",
            "Epoch 333/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3494 - accuracy: 0.8475 - val_loss: 0.7641 - val_accuracy: 0.7347\n",
            "Epoch 334/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3554 - accuracy: 0.8358 - val_loss: 0.7877 - val_accuracy: 0.7483\n",
            "Epoch 335/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3567 - accuracy: 0.8475 - val_loss: 0.7817 - val_accuracy: 0.7415\n",
            "Epoch 336/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3560 - accuracy: 0.8446 - val_loss: 0.8375 - val_accuracy: 0.7279\n",
            "Epoch 337/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3562 - accuracy: 0.8211 - val_loss: 0.7850 - val_accuracy: 0.7551\n",
            "Epoch 338/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3405 - accuracy: 0.8416 - val_loss: 0.7793 - val_accuracy: 0.7143\n",
            "Epoch 339/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3559 - accuracy: 0.8240 - val_loss: 0.7743 - val_accuracy: 0.7347\n",
            "Epoch 340/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3485 - accuracy: 0.8299 - val_loss: 0.7448 - val_accuracy: 0.7347\n",
            "Epoch 341/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3549 - accuracy: 0.8299 - val_loss: 0.8459 - val_accuracy: 0.7551\n",
            "Epoch 342/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3610 - accuracy: 0.8299 - val_loss: 0.7839 - val_accuracy: 0.7143\n",
            "Epoch 343/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3423 - accuracy: 0.8416 - val_loss: 0.7808 - val_accuracy: 0.7619\n",
            "Epoch 344/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3470 - accuracy: 0.8416 - val_loss: 0.8737 - val_accuracy: 0.7347\n",
            "Epoch 345/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3494 - accuracy: 0.8240 - val_loss: 0.8171 - val_accuracy: 0.7619\n",
            "Epoch 346/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3444 - accuracy: 0.8534 - val_loss: 0.8174 - val_accuracy: 0.7279\n",
            "Epoch 347/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3452 - accuracy: 0.8446 - val_loss: 0.8242 - val_accuracy: 0.7347\n",
            "Epoch 348/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3432 - accuracy: 0.8328 - val_loss: 0.8730 - val_accuracy: 0.7211\n",
            "Epoch 349/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3476 - accuracy: 0.8416 - val_loss: 0.8244 - val_accuracy: 0.7619\n",
            "Epoch 350/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3519 - accuracy: 0.8182 - val_loss: 0.8772 - val_accuracy: 0.6939\n",
            "Epoch 351/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3460 - accuracy: 0.8387 - val_loss: 0.8435 - val_accuracy: 0.7279\n",
            "Epoch 352/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3470 - accuracy: 0.8328 - val_loss: 0.8246 - val_accuracy: 0.7415\n",
            "Epoch 353/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3509 - accuracy: 0.8299 - val_loss: 0.8750 - val_accuracy: 0.7279\n",
            "Epoch 354/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3455 - accuracy: 0.8475 - val_loss: 0.8261 - val_accuracy: 0.7347\n",
            "Epoch 355/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3536 - accuracy: 0.8358 - val_loss: 0.7914 - val_accuracy: 0.7347\n",
            "Epoch 356/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3613 - accuracy: 0.8416 - val_loss: 0.7860 - val_accuracy: 0.7347\n",
            "Epoch 357/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3507 - accuracy: 0.8387 - val_loss: 0.8117 - val_accuracy: 0.7347\n",
            "Epoch 358/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.4216 - accuracy: 0.7859 - val_loss: 0.8075 - val_accuracy: 0.7551\n",
            "Epoch 359/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.4367 - accuracy: 0.8182 - val_loss: 0.6045 - val_accuracy: 0.7279\n",
            "Epoch 360/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3630 - accuracy: 0.8504 - val_loss: 0.9670 - val_accuracy: 0.7415\n",
            "Epoch 361/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3735 - accuracy: 0.8358 - val_loss: 0.7911 - val_accuracy: 0.7483\n",
            "Epoch 362/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3470 - accuracy: 0.8534 - val_loss: 0.7867 - val_accuracy: 0.7347\n",
            "Epoch 363/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3594 - accuracy: 0.8328 - val_loss: 0.9047 - val_accuracy: 0.7483\n",
            "Epoch 364/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3520 - accuracy: 0.8416 - val_loss: 0.8358 - val_accuracy: 0.7551\n",
            "Epoch 365/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3539 - accuracy: 0.8387 - val_loss: 0.8274 - val_accuracy: 0.7619\n",
            "Epoch 366/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3502 - accuracy: 0.8475 - val_loss: 0.8969 - val_accuracy: 0.7551\n",
            "Epoch 367/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3440 - accuracy: 0.8475 - val_loss: 0.8414 - val_accuracy: 0.7347\n",
            "Epoch 368/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3436 - accuracy: 0.8475 - val_loss: 0.8385 - val_accuracy: 0.7483\n",
            "Epoch 369/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3452 - accuracy: 0.8534 - val_loss: 0.9213 - val_accuracy: 0.7075\n",
            "Epoch 370/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3574 - accuracy: 0.8328 - val_loss: 0.8691 - val_accuracy: 0.7551\n",
            "Epoch 371/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3515 - accuracy: 0.8475 - val_loss: 0.8557 - val_accuracy: 0.7619\n",
            "Epoch 372/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3519 - accuracy: 0.8358 - val_loss: 0.8540 - val_accuracy: 0.7483\n",
            "Epoch 373/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3433 - accuracy: 0.8592 - val_loss: 0.8654 - val_accuracy: 0.7415\n",
            "Epoch 374/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3445 - accuracy: 0.8328 - val_loss: 0.9102 - val_accuracy: 0.7347\n",
            "Epoch 375/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3494 - accuracy: 0.8446 - val_loss: 0.8449 - val_accuracy: 0.7483\n",
            "Epoch 376/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3408 - accuracy: 0.8563 - val_loss: 0.8766 - val_accuracy: 0.7279\n",
            "Epoch 377/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3380 - accuracy: 0.8534 - val_loss: 0.8874 - val_accuracy: 0.7279\n",
            "Epoch 378/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3442 - accuracy: 0.8563 - val_loss: 0.8957 - val_accuracy: 0.7143\n",
            "Epoch 379/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3368 - accuracy: 0.8475 - val_loss: 0.8756 - val_accuracy: 0.7347\n",
            "Epoch 380/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3420 - accuracy: 0.8416 - val_loss: 0.9115 - val_accuracy: 0.7347\n",
            "Epoch 381/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3463 - accuracy: 0.8416 - val_loss: 0.8358 - val_accuracy: 0.7687\n",
            "Epoch 382/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3493 - accuracy: 0.8416 - val_loss: 0.9200 - val_accuracy: 0.7007\n",
            "Epoch 383/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3536 - accuracy: 0.8416 - val_loss: 0.8142 - val_accuracy: 0.7551\n",
            "Epoch 384/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3653 - accuracy: 0.8299 - val_loss: 0.8623 - val_accuracy: 0.7347\n",
            "Epoch 385/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3433 - accuracy: 0.8446 - val_loss: 0.8542 - val_accuracy: 0.7551\n",
            "Epoch 386/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3478 - accuracy: 0.8358 - val_loss: 0.8114 - val_accuracy: 0.7619\n",
            "Epoch 387/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3465 - accuracy: 0.8446 - val_loss: 0.8678 - val_accuracy: 0.7211\n",
            "Epoch 388/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3501 - accuracy: 0.8387 - val_loss: 0.8236 - val_accuracy: 0.7415\n",
            "Epoch 389/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3544 - accuracy: 0.8387 - val_loss: 0.8804 - val_accuracy: 0.7551\n",
            "Epoch 390/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3486 - accuracy: 0.8358 - val_loss: 0.8362 - val_accuracy: 0.7551\n",
            "Epoch 391/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3444 - accuracy: 0.8563 - val_loss: 0.8567 - val_accuracy: 0.7347\n",
            "Epoch 392/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3532 - accuracy: 0.8270 - val_loss: 0.8422 - val_accuracy: 0.7619\n",
            "Epoch 393/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.4870 - accuracy: 0.7566 - val_loss: 0.8191 - val_accuracy: 0.7415\n",
            "Epoch 394/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.4182 - accuracy: 0.7918 - val_loss: 0.8093 - val_accuracy: 0.7279\n",
            "Epoch 395/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3944 - accuracy: 0.8152 - val_loss: 0.9159 - val_accuracy: 0.6871\n",
            "Epoch 396/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3890 - accuracy: 0.8240 - val_loss: 0.8143 - val_accuracy: 0.7279\n",
            "Epoch 397/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3527 - accuracy: 0.8475 - val_loss: 0.8617 - val_accuracy: 0.7415\n",
            "Epoch 398/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3490 - accuracy: 0.8475 - val_loss: 0.8793 - val_accuracy: 0.7415\n",
            "Epoch 399/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3438 - accuracy: 0.8475 - val_loss: 0.8041 - val_accuracy: 0.7415\n",
            "Epoch 400/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3550 - accuracy: 0.8240 - val_loss: 0.9473 - val_accuracy: 0.7279\n",
            "Epoch 401/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3416 - accuracy: 0.8475 - val_loss: 0.7882 - val_accuracy: 0.7415\n",
            "Epoch 402/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3438 - accuracy: 0.8446 - val_loss: 0.8617 - val_accuracy: 0.7347\n",
            "Epoch 403/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3552 - accuracy: 0.8328 - val_loss: 0.8475 - val_accuracy: 0.7551\n",
            "Epoch 404/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3442 - accuracy: 0.8387 - val_loss: 0.9229 - val_accuracy: 0.7347\n",
            "Epoch 405/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3399 - accuracy: 0.8328 - val_loss: 0.8725 - val_accuracy: 0.7347\n",
            "Epoch 406/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3413 - accuracy: 0.8446 - val_loss: 0.8657 - val_accuracy: 0.7551\n",
            "Epoch 407/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3423 - accuracy: 0.8328 - val_loss: 0.9100 - val_accuracy: 0.7347\n",
            "Epoch 408/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3990 - accuracy: 0.8152 - val_loss: 0.8287 - val_accuracy: 0.7415\n",
            "Epoch 409/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3704 - accuracy: 0.8094 - val_loss: 0.8134 - val_accuracy: 0.7415\n",
            "Epoch 410/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3587 - accuracy: 0.8387 - val_loss: 0.7776 - val_accuracy: 0.7347\n",
            "Epoch 411/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3442 - accuracy: 0.8270 - val_loss: 0.7649 - val_accuracy: 0.7687\n",
            "Epoch 412/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3449 - accuracy: 0.8446 - val_loss: 0.8696 - val_accuracy: 0.7279\n",
            "Epoch 413/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3406 - accuracy: 0.8475 - val_loss: 0.8563 - val_accuracy: 0.7619\n",
            "Epoch 414/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3416 - accuracy: 0.8446 - val_loss: 0.8317 - val_accuracy: 0.7687\n",
            "Epoch 415/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3413 - accuracy: 0.8563 - val_loss: 0.8164 - val_accuracy: 0.7347\n",
            "Epoch 416/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3409 - accuracy: 0.8387 - val_loss: 0.8232 - val_accuracy: 0.7483\n",
            "Epoch 417/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3363 - accuracy: 0.8446 - val_loss: 0.8983 - val_accuracy: 0.7347\n",
            "Epoch 418/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3387 - accuracy: 0.8534 - val_loss: 0.8704 - val_accuracy: 0.7415\n",
            "Epoch 419/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3390 - accuracy: 0.8534 - val_loss: 0.8713 - val_accuracy: 0.7347\n",
            "Epoch 420/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3371 - accuracy: 0.8504 - val_loss: 0.8006 - val_accuracy: 0.7415\n",
            "Epoch 421/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3521 - accuracy: 0.8504 - val_loss: 0.8708 - val_accuracy: 0.7415\n",
            "Epoch 422/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3376 - accuracy: 0.8534 - val_loss: 0.8582 - val_accuracy: 0.7483\n",
            "Epoch 423/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3462 - accuracy: 0.8299 - val_loss: 0.8862 - val_accuracy: 0.7415\n",
            "Epoch 424/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3396 - accuracy: 0.8534 - val_loss: 0.8690 - val_accuracy: 0.7279\n",
            "Epoch 425/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3406 - accuracy: 0.8416 - val_loss: 0.8295 - val_accuracy: 0.7483\n",
            "Epoch 426/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3397 - accuracy: 0.8416 - val_loss: 0.8771 - val_accuracy: 0.7347\n",
            "Epoch 427/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3354 - accuracy: 0.8387 - val_loss: 0.8842 - val_accuracy: 0.7551\n",
            "Epoch 428/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3395 - accuracy: 0.8563 - val_loss: 0.7916 - val_accuracy: 0.7551\n",
            "Epoch 429/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3362 - accuracy: 0.8446 - val_loss: 0.8841 - val_accuracy: 0.7279\n",
            "Epoch 430/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3359 - accuracy: 0.8534 - val_loss: 0.8566 - val_accuracy: 0.7483\n",
            "Epoch 431/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3491 - accuracy: 0.8358 - val_loss: 0.8523 - val_accuracy: 0.7619\n",
            "Epoch 432/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3368 - accuracy: 0.8475 - val_loss: 0.8945 - val_accuracy: 0.7483\n",
            "Epoch 433/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3375 - accuracy: 0.8387 - val_loss: 0.8610 - val_accuracy: 0.7279\n",
            "Epoch 434/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3328 - accuracy: 0.8622 - val_loss: 0.8388 - val_accuracy: 0.7551\n",
            "Epoch 435/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3349 - accuracy: 0.8475 - val_loss: 0.8621 - val_accuracy: 0.7279\n",
            "Epoch 436/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3443 - accuracy: 0.8446 - val_loss: 0.8861 - val_accuracy: 0.7415\n",
            "Epoch 437/1000\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.3382 - accuracy: 0.8534 - val_loss: 0.8702 - val_accuracy: 0.7211\n",
            "Epoch 438/1000\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.3522 - accuracy: 0.8211 - val_loss: 0.8542 - val_accuracy: 0.7551\n",
            "Epoch 439/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3433 - accuracy: 0.8328 - val_loss: 0.8492 - val_accuracy: 0.7483\n",
            "Epoch 440/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3441 - accuracy: 0.8328 - val_loss: 0.8581 - val_accuracy: 0.7483\n",
            "Epoch 441/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3380 - accuracy: 0.8592 - val_loss: 0.8621 - val_accuracy: 0.7619\n",
            "Epoch 442/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3360 - accuracy: 0.8563 - val_loss: 0.8729 - val_accuracy: 0.7551\n",
            "Epoch 443/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3353 - accuracy: 0.8387 - val_loss: 0.8133 - val_accuracy: 0.7483\n",
            "Epoch 444/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3375 - accuracy: 0.8475 - val_loss: 0.8744 - val_accuracy: 0.7279\n",
            "Epoch 445/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3587 - accuracy: 0.8299 - val_loss: 0.8597 - val_accuracy: 0.7279\n",
            "Epoch 446/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3340 - accuracy: 0.8358 - val_loss: 0.9740 - val_accuracy: 0.7347\n",
            "Epoch 447/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3383 - accuracy: 0.8416 - val_loss: 0.8453 - val_accuracy: 0.7211\n",
            "Epoch 448/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3304 - accuracy: 0.8504 - val_loss: 0.8979 - val_accuracy: 0.7211\n",
            "Epoch 449/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3377 - accuracy: 0.8387 - val_loss: 0.8950 - val_accuracy: 0.7551\n",
            "Epoch 450/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3392 - accuracy: 0.8416 - val_loss: 0.8663 - val_accuracy: 0.7415\n",
            "Epoch 451/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3499 - accuracy: 0.8416 - val_loss: 0.9245 - val_accuracy: 0.7211\n",
            "Epoch 452/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3537 - accuracy: 0.8416 - val_loss: 0.8335 - val_accuracy: 0.7347\n",
            "Epoch 453/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3309 - accuracy: 0.8504 - val_loss: 0.9026 - val_accuracy: 0.7347\n",
            "Epoch 454/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3356 - accuracy: 0.8475 - val_loss: 0.8152 - val_accuracy: 0.7415\n",
            "Epoch 455/1000\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.3305 - accuracy: 0.8563 - val_loss: 0.9214 - val_accuracy: 0.7279\n",
            "Epoch 456/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3334 - accuracy: 0.8504 - val_loss: 0.8793 - val_accuracy: 0.7415\n",
            "Epoch 457/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3570 - accuracy: 0.8475 - val_loss: 0.9014 - val_accuracy: 0.7279\n",
            "Epoch 458/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3327 - accuracy: 0.8446 - val_loss: 0.9071 - val_accuracy: 0.7279\n",
            "Epoch 459/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3368 - accuracy: 0.8446 - val_loss: 0.8552 - val_accuracy: 0.7415\n",
            "Epoch 460/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3312 - accuracy: 0.8504 - val_loss: 0.8754 - val_accuracy: 0.7279\n",
            "Epoch 461/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3456 - accuracy: 0.8387 - val_loss: 0.8095 - val_accuracy: 0.7415\n",
            "Epoch 462/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3336 - accuracy: 0.8592 - val_loss: 0.8687 - val_accuracy: 0.7415\n",
            "Epoch 463/1000\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.3438 - accuracy: 0.8475 - val_loss: 0.8633 - val_accuracy: 0.7415\n",
            "Epoch 464/1000\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.3439 - accuracy: 0.8504 - val_loss: 0.8648 - val_accuracy: 0.7551\n",
            "Epoch 465/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3569 - accuracy: 0.8240 - val_loss: 0.8455 - val_accuracy: 0.7347\n",
            "Epoch 466/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3781 - accuracy: 0.8152 - val_loss: 0.9443 - val_accuracy: 0.7347\n",
            "Epoch 467/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3371 - accuracy: 0.8504 - val_loss: 0.9128 - val_accuracy: 0.7347\n",
            "Epoch 468/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3380 - accuracy: 0.8475 - val_loss: 0.8286 - val_accuracy: 0.7415\n",
            "Epoch 469/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3336 - accuracy: 0.8446 - val_loss: 0.9115 - val_accuracy: 0.7211\n",
            "Epoch 470/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3465 - accuracy: 0.8475 - val_loss: 0.9028 - val_accuracy: 0.7551\n",
            "Epoch 471/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3358 - accuracy: 0.8475 - val_loss: 0.8710 - val_accuracy: 0.7619\n",
            "Epoch 472/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3385 - accuracy: 0.8622 - val_loss: 0.8993 - val_accuracy: 0.7415\n",
            "Epoch 473/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3409 - accuracy: 0.8534 - val_loss: 0.8473 - val_accuracy: 0.7279\n",
            "Epoch 474/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3346 - accuracy: 0.8475 - val_loss: 0.8795 - val_accuracy: 0.7551\n",
            "Epoch 475/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3396 - accuracy: 0.8475 - val_loss: 0.8883 - val_accuracy: 0.7279\n",
            "Epoch 476/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3405 - accuracy: 0.8387 - val_loss: 0.8995 - val_accuracy: 0.7483\n",
            "Epoch 477/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3301 - accuracy: 0.8475 - val_loss: 0.8341 - val_accuracy: 0.7483\n",
            "Epoch 478/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3733 - accuracy: 0.8240 - val_loss: 0.8763 - val_accuracy: 0.7415\n",
            "Epoch 479/1000\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.3375 - accuracy: 0.8622 - val_loss: 0.8425 - val_accuracy: 0.7687\n",
            "Epoch 480/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3413 - accuracy: 0.8475 - val_loss: 0.8899 - val_accuracy: 0.7347\n",
            "Epoch 481/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3343 - accuracy: 0.8504 - val_loss: 0.8389 - val_accuracy: 0.7347\n",
            "Epoch 482/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3446 - accuracy: 0.8211 - val_loss: 0.8455 - val_accuracy: 0.7551\n",
            "Epoch 483/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3348 - accuracy: 0.8475 - val_loss: 0.8625 - val_accuracy: 0.7619\n",
            "Epoch 484/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3371 - accuracy: 0.8446 - val_loss: 0.9021 - val_accuracy: 0.7415\n",
            "Epoch 485/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3337 - accuracy: 0.8475 - val_loss: 0.8995 - val_accuracy: 0.7347\n",
            "Epoch 486/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3443 - accuracy: 0.8534 - val_loss: 0.8486 - val_accuracy: 0.7483\n",
            "Epoch 487/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3385 - accuracy: 0.8416 - val_loss: 0.7306 - val_accuracy: 0.7415\n",
            "Epoch 488/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3350 - accuracy: 0.8504 - val_loss: 0.8499 - val_accuracy: 0.7347\n",
            "Epoch 489/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3405 - accuracy: 0.8475 - val_loss: 0.8982 - val_accuracy: 0.7551\n",
            "Epoch 490/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3417 - accuracy: 0.8328 - val_loss: 0.8751 - val_accuracy: 0.7211\n",
            "Epoch 491/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3337 - accuracy: 0.8534 - val_loss: 0.8683 - val_accuracy: 0.7415\n",
            "Epoch 492/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3398 - accuracy: 0.8299 - val_loss: 0.8864 - val_accuracy: 0.7415\n",
            "Epoch 493/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3366 - accuracy: 0.8358 - val_loss: 0.8921 - val_accuracy: 0.7483\n",
            "Epoch 494/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3318 - accuracy: 0.8592 - val_loss: 0.9540 - val_accuracy: 0.7211\n",
            "Epoch 495/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3338 - accuracy: 0.8475 - val_loss: 0.8963 - val_accuracy: 0.7347\n",
            "Epoch 496/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3505 - accuracy: 0.8504 - val_loss: 0.9351 - val_accuracy: 0.7279\n",
            "Epoch 497/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3376 - accuracy: 0.8504 - val_loss: 0.8573 - val_accuracy: 0.7619\n",
            "Epoch 498/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3297 - accuracy: 0.8592 - val_loss: 0.9366 - val_accuracy: 0.7143\n",
            "Epoch 499/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3356 - accuracy: 0.8504 - val_loss: 0.8818 - val_accuracy: 0.7279\n",
            "Epoch 500/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3279 - accuracy: 0.8563 - val_loss: 0.9178 - val_accuracy: 0.7279\n",
            "Epoch 501/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3559 - accuracy: 0.8387 - val_loss: 0.9581 - val_accuracy: 0.7279\n",
            "Epoch 502/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3464 - accuracy: 0.8240 - val_loss: 0.8473 - val_accuracy: 0.7415\n",
            "Epoch 503/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3275 - accuracy: 0.8416 - val_loss: 0.9184 - val_accuracy: 0.7279\n",
            "Epoch 504/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3258 - accuracy: 0.8504 - val_loss: 0.9106 - val_accuracy: 0.7551\n",
            "Epoch 505/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3337 - accuracy: 0.8475 - val_loss: 0.9600 - val_accuracy: 0.7279\n",
            "Epoch 506/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3312 - accuracy: 0.8299 - val_loss: 0.9411 - val_accuracy: 0.7619\n",
            "Epoch 507/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3252 - accuracy: 0.8534 - val_loss: 0.8960 - val_accuracy: 0.7279\n",
            "Epoch 508/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3278 - accuracy: 0.8504 - val_loss: 0.9020 - val_accuracy: 0.7347\n",
            "Epoch 509/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3250 - accuracy: 0.8622 - val_loss: 0.9486 - val_accuracy: 0.7279\n",
            "Epoch 510/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3317 - accuracy: 0.8534 - val_loss: 0.9002 - val_accuracy: 0.7483\n",
            "Epoch 511/1000\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.3301 - accuracy: 0.8387 - val_loss: 0.9564 - val_accuracy: 0.7347\n",
            "Epoch 512/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3265 - accuracy: 0.8504 - val_loss: 0.8618 - val_accuracy: 0.7619\n",
            "Epoch 513/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3531 - accuracy: 0.8182 - val_loss: 0.8668 - val_accuracy: 0.7347\n",
            "Epoch 514/1000\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.3329 - accuracy: 0.8475 - val_loss: 0.9469 - val_accuracy: 0.7279\n",
            "Epoch 515/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3375 - accuracy: 0.8504 - val_loss: 0.8892 - val_accuracy: 0.7279\n",
            "Epoch 516/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3338 - accuracy: 0.8475 - val_loss: 0.8981 - val_accuracy: 0.7619\n",
            "Epoch 517/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3240 - accuracy: 0.8534 - val_loss: 0.9384 - val_accuracy: 0.7483\n",
            "Epoch 518/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3376 - accuracy: 0.8563 - val_loss: 0.9319 - val_accuracy: 0.7551\n",
            "Epoch 519/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3485 - accuracy: 0.8416 - val_loss: 0.9939 - val_accuracy: 0.7551\n",
            "Epoch 520/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3310 - accuracy: 0.8563 - val_loss: 0.9629 - val_accuracy: 0.7347\n",
            "Epoch 521/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3356 - accuracy: 0.8387 - val_loss: 0.9533 - val_accuracy: 0.7415\n",
            "Epoch 522/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3344 - accuracy: 0.8504 - val_loss: 0.9097 - val_accuracy: 0.7755\n",
            "Epoch 523/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3545 - accuracy: 0.8328 - val_loss: 0.9417 - val_accuracy: 0.7415\n",
            "Epoch 524/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3344 - accuracy: 0.8592 - val_loss: 0.9579 - val_accuracy: 0.7483\n",
            "Epoch 525/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3265 - accuracy: 0.8563 - val_loss: 0.9523 - val_accuracy: 0.7347\n",
            "Epoch 526/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3346 - accuracy: 0.8563 - val_loss: 0.9554 - val_accuracy: 0.7415\n",
            "Epoch 527/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3374 - accuracy: 0.8504 - val_loss: 0.9168 - val_accuracy: 0.7619\n",
            "Epoch 528/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3389 - accuracy: 0.8358 - val_loss: 0.9295 - val_accuracy: 0.7619\n",
            "Epoch 529/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3271 - accuracy: 0.8504 - val_loss: 0.9315 - val_accuracy: 0.7687\n",
            "Epoch 530/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3422 - accuracy: 0.8446 - val_loss: 0.9767 - val_accuracy: 0.7279\n",
            "Epoch 531/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3343 - accuracy: 0.8416 - val_loss: 0.9461 - val_accuracy: 0.7415\n",
            "Epoch 532/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.5690 - accuracy: 0.7126 - val_loss: 0.8633 - val_accuracy: 0.7347\n",
            "Epoch 533/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.4642 - accuracy: 0.7537 - val_loss: 1.0340 - val_accuracy: 0.7143\n",
            "Epoch 534/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3953 - accuracy: 0.8182 - val_loss: 0.8423 - val_accuracy: 0.7483\n",
            "Epoch 535/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3482 - accuracy: 0.8416 - val_loss: 0.9186 - val_accuracy: 0.7211\n",
            "Epoch 536/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3346 - accuracy: 0.8475 - val_loss: 0.8855 - val_accuracy: 0.7347\n",
            "Epoch 537/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3270 - accuracy: 0.8504 - val_loss: 0.9399 - val_accuracy: 0.7347\n",
            "Epoch 538/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3425 - accuracy: 0.8475 - val_loss: 0.8683 - val_accuracy: 0.7551\n",
            "Epoch 539/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3303 - accuracy: 0.8358 - val_loss: 0.9334 - val_accuracy: 0.7279\n",
            "Epoch 540/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3382 - accuracy: 0.8387 - val_loss: 0.9036 - val_accuracy: 0.7687\n",
            "Epoch 541/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3248 - accuracy: 0.8651 - val_loss: 0.9054 - val_accuracy: 0.7551\n",
            "Epoch 542/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3360 - accuracy: 0.8446 - val_loss: 0.8742 - val_accuracy: 0.7415\n",
            "Epoch 543/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3371 - accuracy: 0.8563 - val_loss: 0.9180 - val_accuracy: 0.7347\n",
            "Epoch 544/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3261 - accuracy: 0.8446 - val_loss: 0.8627 - val_accuracy: 0.7551\n",
            "Epoch 545/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3295 - accuracy: 0.8446 - val_loss: 0.8973 - val_accuracy: 0.7551\n",
            "Epoch 546/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3305 - accuracy: 0.8446 - val_loss: 0.8823 - val_accuracy: 0.7347\n",
            "Epoch 547/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3288 - accuracy: 0.8563 - val_loss: 0.9124 - val_accuracy: 0.7347\n",
            "Epoch 548/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3218 - accuracy: 0.8592 - val_loss: 0.8643 - val_accuracy: 0.7823\n",
            "Epoch 549/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3704 - accuracy: 0.8211 - val_loss: 0.8684 - val_accuracy: 0.7415\n",
            "Epoch 550/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3364 - accuracy: 0.8534 - val_loss: 0.9392 - val_accuracy: 0.7415\n",
            "Epoch 551/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3265 - accuracy: 0.8563 - val_loss: 0.9193 - val_accuracy: 0.7483\n",
            "Epoch 552/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3296 - accuracy: 0.8592 - val_loss: 0.9462 - val_accuracy: 0.7143\n",
            "Epoch 553/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3257 - accuracy: 0.8592 - val_loss: 0.8874 - val_accuracy: 0.7415\n",
            "Epoch 554/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3333 - accuracy: 0.8592 - val_loss: 0.8946 - val_accuracy: 0.7483\n",
            "Epoch 555/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3294 - accuracy: 0.8592 - val_loss: 0.9214 - val_accuracy: 0.7279\n",
            "Epoch 556/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3378 - accuracy: 0.8446 - val_loss: 0.8742 - val_accuracy: 0.7415\n",
            "Epoch 557/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3361 - accuracy: 0.8651 - val_loss: 0.9230 - val_accuracy: 0.7619\n",
            "Epoch 558/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3282 - accuracy: 0.8416 - val_loss: 0.8827 - val_accuracy: 0.7211\n",
            "Epoch 559/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3304 - accuracy: 0.8622 - val_loss: 0.8978 - val_accuracy: 0.7551\n",
            "Epoch 560/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3294 - accuracy: 0.8563 - val_loss: 0.9435 - val_accuracy: 0.7211\n",
            "Epoch 561/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3255 - accuracy: 0.8534 - val_loss: 0.9543 - val_accuracy: 0.7483\n",
            "Epoch 562/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3375 - accuracy: 0.8475 - val_loss: 0.8624 - val_accuracy: 0.7483\n",
            "Epoch 563/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3243 - accuracy: 0.8504 - val_loss: 1.0209 - val_accuracy: 0.7211\n",
            "Epoch 564/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3249 - accuracy: 0.8592 - val_loss: 0.9197 - val_accuracy: 0.7483\n",
            "Epoch 565/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3246 - accuracy: 0.8592 - val_loss: 0.9552 - val_accuracy: 0.7279\n",
            "Epoch 566/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3231 - accuracy: 0.8622 - val_loss: 0.9930 - val_accuracy: 0.7415\n",
            "Epoch 567/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3279 - accuracy: 0.8446 - val_loss: 0.9128 - val_accuracy: 0.7619\n",
            "Epoch 568/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3246 - accuracy: 0.8475 - val_loss: 0.9076 - val_accuracy: 0.7347\n",
            "Epoch 569/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3390 - accuracy: 0.8504 - val_loss: 0.9245 - val_accuracy: 0.7211\n",
            "Epoch 570/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3488 - accuracy: 0.8416 - val_loss: 1.0180 - val_accuracy: 0.7007\n",
            "Epoch 571/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3354 - accuracy: 0.8446 - val_loss: 0.9527 - val_accuracy: 0.7551\n",
            "Epoch 572/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3358 - accuracy: 0.8446 - val_loss: 0.9016 - val_accuracy: 0.7619\n",
            "Epoch 573/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3200 - accuracy: 0.8563 - val_loss: 0.9808 - val_accuracy: 0.7279\n",
            "Epoch 574/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3217 - accuracy: 0.8592 - val_loss: 0.9062 - val_accuracy: 0.7347\n",
            "Epoch 575/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3482 - accuracy: 0.8358 - val_loss: 0.9407 - val_accuracy: 0.7279\n",
            "Epoch 576/1000\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.3270 - accuracy: 0.8446 - val_loss: 0.9586 - val_accuracy: 0.7415\n",
            "Epoch 577/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3197 - accuracy: 0.8739 - val_loss: 0.9339 - val_accuracy: 0.7347\n",
            "Epoch 578/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3349 - accuracy: 0.8358 - val_loss: 0.9060 - val_accuracy: 0.7415\n",
            "Epoch 579/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3896 - accuracy: 0.8299 - val_loss: 0.8659 - val_accuracy: 0.7347\n",
            "Epoch 580/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3483 - accuracy: 0.8622 - val_loss: 0.9211 - val_accuracy: 0.7279\n",
            "Epoch 581/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3511 - accuracy: 0.8299 - val_loss: 0.9070 - val_accuracy: 0.7415\n",
            "Epoch 582/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3387 - accuracy: 0.8504 - val_loss: 0.8945 - val_accuracy: 0.7551\n",
            "Epoch 583/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3368 - accuracy: 0.8651 - val_loss: 0.9231 - val_accuracy: 0.7415\n",
            "Epoch 584/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3375 - accuracy: 0.8387 - val_loss: 0.9830 - val_accuracy: 0.7279\n",
            "Epoch 585/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3353 - accuracy: 0.8387 - val_loss: 0.9420 - val_accuracy: 0.7347\n",
            "Epoch 586/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3470 - accuracy: 0.8211 - val_loss: 0.7513 - val_accuracy: 0.7619\n",
            "Epoch 587/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3556 - accuracy: 0.8446 - val_loss: 1.0401 - val_accuracy: 0.7551\n",
            "Epoch 588/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3303 - accuracy: 0.8592 - val_loss: 0.9468 - val_accuracy: 0.7551\n",
            "Epoch 589/1000\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.3294 - accuracy: 0.8504 - val_loss: 0.9491 - val_accuracy: 0.7415\n",
            "Epoch 590/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3255 - accuracy: 0.8622 - val_loss: 0.9071 - val_accuracy: 0.7891\n",
            "Epoch 591/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3245 - accuracy: 0.8710 - val_loss: 0.9490 - val_accuracy: 0.7415\n",
            "Epoch 592/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3139 - accuracy: 0.8622 - val_loss: 0.9183 - val_accuracy: 0.7755\n",
            "Epoch 593/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3153 - accuracy: 0.8680 - val_loss: 0.9623 - val_accuracy: 0.7551\n",
            "Epoch 594/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3099 - accuracy: 0.8680 - val_loss: 0.9167 - val_accuracy: 0.7619\n",
            "Epoch 595/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3218 - accuracy: 0.8680 - val_loss: 0.9870 - val_accuracy: 0.7143\n",
            "Epoch 596/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3276 - accuracy: 0.8622 - val_loss: 0.9535 - val_accuracy: 0.7551\n",
            "Epoch 597/1000\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.3228 - accuracy: 0.8651 - val_loss: 0.9231 - val_accuracy: 0.7823\n",
            "Epoch 598/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3206 - accuracy: 0.8563 - val_loss: 0.9161 - val_accuracy: 0.7551\n",
            "Epoch 599/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3336 - accuracy: 0.8446 - val_loss: 1.0173 - val_accuracy: 0.7075\n",
            "Epoch 600/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3191 - accuracy: 0.8563 - val_loss: 0.8542 - val_accuracy: 0.7483\n",
            "Epoch 601/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3276 - accuracy: 0.8563 - val_loss: 0.8709 - val_accuracy: 0.7687\n",
            "Epoch 602/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3274 - accuracy: 0.8504 - val_loss: 0.8951 - val_accuracy: 0.7687\n",
            "Epoch 603/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3265 - accuracy: 0.8592 - val_loss: 0.9125 - val_accuracy: 0.7415\n",
            "Epoch 604/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3268 - accuracy: 0.8592 - val_loss: 0.9464 - val_accuracy: 0.7211\n",
            "Epoch 605/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3412 - accuracy: 0.8563 - val_loss: 0.9222 - val_accuracy: 0.7347\n",
            "Epoch 606/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3231 - accuracy: 0.8504 - val_loss: 0.9244 - val_accuracy: 0.7279\n",
            "Epoch 607/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3354 - accuracy: 0.8622 - val_loss: 0.8994 - val_accuracy: 0.7619\n",
            "Epoch 608/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3207 - accuracy: 0.8504 - val_loss: 0.8912 - val_accuracy: 0.7755\n",
            "Epoch 609/1000\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.3126 - accuracy: 0.8739 - val_loss: 0.9316 - val_accuracy: 0.7279\n",
            "Epoch 610/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3229 - accuracy: 0.8651 - val_loss: 0.8628 - val_accuracy: 0.7619\n",
            "Epoch 611/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3092 - accuracy: 0.8710 - val_loss: 0.9448 - val_accuracy: 0.7483\n",
            "Epoch 612/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3117 - accuracy: 0.8622 - val_loss: 0.9275 - val_accuracy: 0.7483\n",
            "Epoch 613/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3479 - accuracy: 0.8387 - val_loss: 0.8188 - val_accuracy: 0.7687\n",
            "Epoch 614/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3203 - accuracy: 0.8710 - val_loss: 1.0143 - val_accuracy: 0.7279\n",
            "Epoch 615/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3226 - accuracy: 0.8680 - val_loss: 0.9505 - val_accuracy: 0.7347\n",
            "Epoch 616/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3439 - accuracy: 0.8534 - val_loss: 0.9805 - val_accuracy: 0.7551\n",
            "Epoch 617/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3292 - accuracy: 0.8534 - val_loss: 0.9372 - val_accuracy: 0.7551\n",
            "Epoch 618/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3203 - accuracy: 0.8622 - val_loss: 0.9511 - val_accuracy: 0.7415\n",
            "Epoch 619/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3191 - accuracy: 0.8592 - val_loss: 0.8454 - val_accuracy: 0.7687\n",
            "Epoch 620/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3077 - accuracy: 0.8768 - val_loss: 0.8830 - val_accuracy: 0.7415\n",
            "Epoch 621/1000\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.3181 - accuracy: 0.8710 - val_loss: 0.9246 - val_accuracy: 0.7347\n",
            "Epoch 622/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3099 - accuracy: 0.8622 - val_loss: 0.9145 - val_accuracy: 0.7483\n",
            "Epoch 623/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3090 - accuracy: 0.8739 - val_loss: 0.9280 - val_accuracy: 0.7551\n",
            "Epoch 624/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3047 - accuracy: 0.8710 - val_loss: 0.9338 - val_accuracy: 0.7551\n",
            "Epoch 625/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3073 - accuracy: 0.8622 - val_loss: 0.9629 - val_accuracy: 0.7415\n",
            "Epoch 626/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3208 - accuracy: 0.8710 - val_loss: 0.9308 - val_accuracy: 0.7551\n",
            "Epoch 627/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3498 - accuracy: 0.8504 - val_loss: 0.9070 - val_accuracy: 0.7279\n",
            "Epoch 628/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3181 - accuracy: 0.8504 - val_loss: 0.9462 - val_accuracy: 0.7347\n",
            "Epoch 629/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3246 - accuracy: 0.8475 - val_loss: 0.9083 - val_accuracy: 0.7415\n",
            "Epoch 630/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3108 - accuracy: 0.8798 - val_loss: 0.9951 - val_accuracy: 0.7415\n",
            "Epoch 631/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3295 - accuracy: 0.8563 - val_loss: 1.0052 - val_accuracy: 0.7483\n",
            "Epoch 632/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3227 - accuracy: 0.8622 - val_loss: 0.9282 - val_accuracy: 0.7415\n",
            "Epoch 633/1000\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.3243 - accuracy: 0.8651 - val_loss: 0.9840 - val_accuracy: 0.7347\n",
            "Epoch 634/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3075 - accuracy: 0.8651 - val_loss: 0.9639 - val_accuracy: 0.7347\n",
            "Epoch 635/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3083 - accuracy: 0.8592 - val_loss: 0.9807 - val_accuracy: 0.7415\n",
            "Epoch 636/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3098 - accuracy: 0.8710 - val_loss: 0.9764 - val_accuracy: 0.7483\n",
            "Epoch 637/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3033 - accuracy: 0.8592 - val_loss: 0.9585 - val_accuracy: 0.7347\n",
            "Epoch 638/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3093 - accuracy: 0.8680 - val_loss: 0.9153 - val_accuracy: 0.7483\n",
            "Epoch 639/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3281 - accuracy: 0.8475 - val_loss: 1.0055 - val_accuracy: 0.7007\n",
            "Epoch 640/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3197 - accuracy: 0.8592 - val_loss: 1.0188 - val_accuracy: 0.7415\n",
            "Epoch 641/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3091 - accuracy: 0.8739 - val_loss: 0.9955 - val_accuracy: 0.7211\n",
            "Epoch 642/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3184 - accuracy: 0.8563 - val_loss: 0.9512 - val_accuracy: 0.7551\n",
            "Epoch 643/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3178 - accuracy: 0.8504 - val_loss: 0.9197 - val_accuracy: 0.7483\n",
            "Epoch 644/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3168 - accuracy: 0.8622 - val_loss: 0.9416 - val_accuracy: 0.7551\n",
            "Epoch 645/1000\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.3036 - accuracy: 0.8592 - val_loss: 0.9329 - val_accuracy: 0.7619\n",
            "Epoch 646/1000\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.3066 - accuracy: 0.8739 - val_loss: 1.0043 - val_accuracy: 0.7279\n",
            "Epoch 647/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3363 - accuracy: 0.8622 - val_loss: 0.9269 - val_accuracy: 0.7687\n",
            "Epoch 648/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3068 - accuracy: 0.8622 - val_loss: 0.9125 - val_accuracy: 0.7551\n",
            "Epoch 649/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3196 - accuracy: 0.8680 - val_loss: 0.9395 - val_accuracy: 0.7415\n",
            "Epoch 650/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3197 - accuracy: 0.8622 - val_loss: 0.9844 - val_accuracy: 0.7619\n",
            "Epoch 651/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3239 - accuracy: 0.8416 - val_loss: 0.9066 - val_accuracy: 0.7619\n",
            "Epoch 652/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3186 - accuracy: 0.8622 - val_loss: 0.9867 - val_accuracy: 0.7347\n",
            "Epoch 653/1000\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.3230 - accuracy: 0.8563 - val_loss: 1.0149 - val_accuracy: 0.7347\n",
            "Epoch 654/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3197 - accuracy: 0.8534 - val_loss: 1.0030 - val_accuracy: 0.7483\n",
            "Epoch 655/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3071 - accuracy: 0.8651 - val_loss: 1.0259 - val_accuracy: 0.7279\n",
            "Epoch 656/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3309 - accuracy: 0.8446 - val_loss: 0.9561 - val_accuracy: 0.7687\n",
            "Epoch 657/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3145 - accuracy: 0.8592 - val_loss: 0.9918 - val_accuracy: 0.7687\n",
            "Epoch 658/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3062 - accuracy: 0.8651 - val_loss: 1.0346 - val_accuracy: 0.7347\n",
            "Epoch 659/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3212 - accuracy: 0.8475 - val_loss: 0.9338 - val_accuracy: 0.7619\n",
            "Epoch 660/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3180 - accuracy: 0.8680 - val_loss: 0.9925 - val_accuracy: 0.7415\n",
            "Epoch 661/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3155 - accuracy: 0.8739 - val_loss: 1.0027 - val_accuracy: 0.7483\n",
            "Epoch 662/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3170 - accuracy: 0.8563 - val_loss: 1.0212 - val_accuracy: 0.7347\n",
            "Epoch 663/1000\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.3326 - accuracy: 0.8475 - val_loss: 0.9883 - val_accuracy: 0.7415\n",
            "Epoch 664/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3091 - accuracy: 0.8651 - val_loss: 0.9735 - val_accuracy: 0.7551\n",
            "Epoch 665/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3113 - accuracy: 0.8680 - val_loss: 1.0303 - val_accuracy: 0.7415\n",
            "Epoch 666/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3242 - accuracy: 0.8592 - val_loss: 0.9481 - val_accuracy: 0.7687\n",
            "Epoch 667/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3191 - accuracy: 0.8563 - val_loss: 0.9960 - val_accuracy: 0.7347\n",
            "Epoch 668/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3131 - accuracy: 0.8592 - val_loss: 0.9706 - val_accuracy: 0.7415\n",
            "Epoch 669/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3149 - accuracy: 0.8563 - val_loss: 1.0361 - val_accuracy: 0.7415\n",
            "Epoch 670/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.2994 - accuracy: 0.8768 - val_loss: 0.8625 - val_accuracy: 0.7619\n",
            "Epoch 671/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3101 - accuracy: 0.8651 - val_loss: 0.9784 - val_accuracy: 0.7279\n",
            "Epoch 672/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3177 - accuracy: 0.8534 - val_loss: 0.9805 - val_accuracy: 0.7347\n",
            "Epoch 673/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3060 - accuracy: 0.8710 - val_loss: 0.9878 - val_accuracy: 0.7483\n",
            "Epoch 674/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3041 - accuracy: 0.8651 - val_loss: 1.0222 - val_accuracy: 0.7483\n",
            "Epoch 675/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3041 - accuracy: 0.8739 - val_loss: 0.9744 - val_accuracy: 0.7483\n",
            "Epoch 676/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3174 - accuracy: 0.8563 - val_loss: 1.0452 - val_accuracy: 0.7415\n",
            "Epoch 677/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3211 - accuracy: 0.8622 - val_loss: 0.9372 - val_accuracy: 0.7347\n",
            "Epoch 678/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3423 - accuracy: 0.8563 - val_loss: 1.0379 - val_accuracy: 0.7551\n",
            "Epoch 679/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3153 - accuracy: 0.8622 - val_loss: 0.9807 - val_accuracy: 0.7551\n",
            "Epoch 680/1000\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.3090 - accuracy: 0.8592 - val_loss: 0.9774 - val_accuracy: 0.7619\n",
            "Epoch 681/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3030 - accuracy: 0.8592 - val_loss: 0.9612 - val_accuracy: 0.7551\n",
            "Epoch 682/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3018 - accuracy: 0.8710 - val_loss: 1.0663 - val_accuracy: 0.7415\n",
            "Epoch 683/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3044 - accuracy: 0.8592 - val_loss: 0.9579 - val_accuracy: 0.7687\n",
            "Epoch 684/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3046 - accuracy: 0.8798 - val_loss: 1.0343 - val_accuracy: 0.7483\n",
            "Epoch 685/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.2973 - accuracy: 0.8739 - val_loss: 1.0229 - val_accuracy: 0.7551\n",
            "Epoch 686/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3243 - accuracy: 0.8651 - val_loss: 1.0367 - val_accuracy: 0.7483\n",
            "Epoch 687/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3282 - accuracy: 0.8563 - val_loss: 1.0712 - val_accuracy: 0.7415\n",
            "Epoch 688/1000\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.3066 - accuracy: 0.8710 - val_loss: 1.0076 - val_accuracy: 0.7551\n",
            "Epoch 689/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3294 - accuracy: 0.8475 - val_loss: 1.0015 - val_accuracy: 0.7551\n",
            "Epoch 690/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3098 - accuracy: 0.8563 - val_loss: 1.0132 - val_accuracy: 0.7687\n",
            "Epoch 691/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3088 - accuracy: 0.8680 - val_loss: 0.9233 - val_accuracy: 0.7483\n",
            "Epoch 692/1000\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.3161 - accuracy: 0.8622 - val_loss: 1.0072 - val_accuracy: 0.7687\n",
            "Epoch 693/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3045 - accuracy: 0.8798 - val_loss: 0.9862 - val_accuracy: 0.7687\n",
            "Epoch 694/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3095 - accuracy: 0.8710 - val_loss: 1.0420 - val_accuracy: 0.7551\n",
            "Epoch 695/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3136 - accuracy: 0.8592 - val_loss: 0.9751 - val_accuracy: 0.7279\n",
            "Epoch 696/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3063 - accuracy: 0.8534 - val_loss: 0.9916 - val_accuracy: 0.7551\n",
            "Epoch 697/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3092 - accuracy: 0.8710 - val_loss: 1.0464 - val_accuracy: 0.7619\n",
            "Epoch 698/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3113 - accuracy: 0.8592 - val_loss: 0.9160 - val_accuracy: 0.7551\n",
            "Epoch 699/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3033 - accuracy: 0.8768 - val_loss: 1.0281 - val_accuracy: 0.7619\n",
            "Epoch 700/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3255 - accuracy: 0.8504 - val_loss: 1.0510 - val_accuracy: 0.7415\n",
            "Epoch 701/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3703 - accuracy: 0.8299 - val_loss: 0.9771 - val_accuracy: 0.7483\n",
            "Epoch 702/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.6376 - accuracy: 0.6598 - val_loss: 0.9235 - val_accuracy: 0.6735\n",
            "Epoch 703/1000\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.4669 - accuracy: 0.7419 - val_loss: 1.0447 - val_accuracy: 0.7415\n",
            "Epoch 704/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3567 - accuracy: 0.8387 - val_loss: 0.9890 - val_accuracy: 0.7687\n",
            "Epoch 705/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3259 - accuracy: 0.8622 - val_loss: 1.0256 - val_accuracy: 0.7483\n",
            "Epoch 706/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3143 - accuracy: 0.8680 - val_loss: 1.0108 - val_accuracy: 0.7551\n",
            "Epoch 707/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3368 - accuracy: 0.8416 - val_loss: 1.0414 - val_accuracy: 0.7483\n",
            "Epoch 708/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3208 - accuracy: 0.8592 - val_loss: 1.0637 - val_accuracy: 0.7347\n",
            "Epoch 709/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3182 - accuracy: 0.8592 - val_loss: 1.0099 - val_accuracy: 0.7687\n",
            "Epoch 710/1000\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.3170 - accuracy: 0.8680 - val_loss: 1.0333 - val_accuracy: 0.7619\n",
            "Epoch 711/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3264 - accuracy: 0.8475 - val_loss: 1.0350 - val_accuracy: 0.7687\n",
            "Epoch 712/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3210 - accuracy: 0.8622 - val_loss: 1.0053 - val_accuracy: 0.7619\n",
            "Epoch 713/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3266 - accuracy: 0.8651 - val_loss: 1.0168 - val_accuracy: 0.7619\n",
            "Epoch 714/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3165 - accuracy: 0.8680 - val_loss: 1.0303 - val_accuracy: 0.7551\n",
            "Epoch 715/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3276 - accuracy: 0.8651 - val_loss: 1.0055 - val_accuracy: 0.7619\n",
            "Epoch 716/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3272 - accuracy: 0.8534 - val_loss: 1.0783 - val_accuracy: 0.7415\n",
            "Epoch 717/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3199 - accuracy: 0.8622 - val_loss: 1.0159 - val_accuracy: 0.7619\n",
            "Epoch 718/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3079 - accuracy: 0.8739 - val_loss: 1.0597 - val_accuracy: 0.7415\n",
            "Epoch 719/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3134 - accuracy: 0.8680 - val_loss: 1.0614 - val_accuracy: 0.7415\n",
            "Epoch 720/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3150 - accuracy: 0.8592 - val_loss: 0.9739 - val_accuracy: 0.7619\n",
            "Epoch 721/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3445 - accuracy: 0.8328 - val_loss: 1.0468 - val_accuracy: 0.7619\n",
            "Epoch 722/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3244 - accuracy: 0.8592 - val_loss: 0.9902 - val_accuracy: 0.7551\n",
            "Epoch 723/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3078 - accuracy: 0.8680 - val_loss: 1.0289 - val_accuracy: 0.7619\n",
            "Epoch 724/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3062 - accuracy: 0.8680 - val_loss: 1.0165 - val_accuracy: 0.7551\n",
            "Epoch 725/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3196 - accuracy: 0.8710 - val_loss: 1.0505 - val_accuracy: 0.7619\n",
            "Epoch 726/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3173 - accuracy: 0.8563 - val_loss: 1.0234 - val_accuracy: 0.7687\n",
            "Epoch 727/1000\n",
            "35/35 [==============================] - 0s 8ms/step - loss: 0.3117 - accuracy: 0.8680 - val_loss: 1.0701 - val_accuracy: 0.7415\n",
            "Epoch 728/1000\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.3230 - accuracy: 0.8504 - val_loss: 0.9820 - val_accuracy: 0.7687\n",
            "Epoch 729/1000\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.3089 - accuracy: 0.8710 - val_loss: 1.0112 - val_accuracy: 0.7687\n",
            "Epoch 730/1000\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.3083 - accuracy: 0.8592 - val_loss: 1.0141 - val_accuracy: 0.7619\n",
            "Epoch 731/1000\n",
            "35/35 [==============================] - 0s 8ms/step - loss: 0.3095 - accuracy: 0.8710 - val_loss: 1.0325 - val_accuracy: 0.7619\n",
            "Epoch 732/1000\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.3031 - accuracy: 0.8739 - val_loss: 1.0691 - val_accuracy: 0.7687\n",
            "Epoch 733/1000\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.3085 - accuracy: 0.8680 - val_loss: 0.9570 - val_accuracy: 0.7483\n",
            "Epoch 734/1000\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.3203 - accuracy: 0.8563 - val_loss: 1.0060 - val_accuracy: 0.7483\n",
            "Epoch 735/1000\n",
            "35/35 [==============================] - 0s 7ms/step - loss: 0.3274 - accuracy: 0.8504 - val_loss: 1.0564 - val_accuracy: 0.7347\n",
            "Epoch 736/1000\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.3137 - accuracy: 0.8622 - val_loss: 1.0353 - val_accuracy: 0.7687\n",
            "Epoch 737/1000\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.3021 - accuracy: 0.8768 - val_loss: 1.1086 - val_accuracy: 0.7483\n",
            "Epoch 738/1000\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.3023 - accuracy: 0.8651 - val_loss: 1.0478 - val_accuracy: 0.7551\n",
            "Epoch 739/1000\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.3108 - accuracy: 0.8739 - val_loss: 1.0915 - val_accuracy: 0.7619\n",
            "Epoch 740/1000\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.3043 - accuracy: 0.8680 - val_loss: 1.0444 - val_accuracy: 0.7551\n",
            "Epoch 741/1000\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.3147 - accuracy: 0.8592 - val_loss: 0.9826 - val_accuracy: 0.7687\n",
            "Epoch 742/1000\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.3297 - accuracy: 0.8475 - val_loss: 1.2681 - val_accuracy: 0.7347\n",
            "Epoch 743/1000\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.3261 - accuracy: 0.8592 - val_loss: 0.9644 - val_accuracy: 0.7551\n",
            "Epoch 744/1000\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.3089 - accuracy: 0.8563 - val_loss: 1.0061 - val_accuracy: 0.7687\n",
            "Epoch 745/1000\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.3146 - accuracy: 0.8680 - val_loss: 1.0943 - val_accuracy: 0.7551\n",
            "Epoch 746/1000\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.3315 - accuracy: 0.8475 - val_loss: 1.0003 - val_accuracy: 0.7619\n",
            "Epoch 747/1000\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.3065 - accuracy: 0.8680 - val_loss: 1.0476 - val_accuracy: 0.7687\n",
            "Epoch 748/1000\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.3077 - accuracy: 0.8563 - val_loss: 1.1369 - val_accuracy: 0.7347\n",
            "Epoch 749/1000\n",
            "35/35 [==============================] - 0s 11ms/step - loss: 0.3111 - accuracy: 0.8622 - val_loss: 1.0435 - val_accuracy: 0.7619\n",
            "Epoch 750/1000\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.3129 - accuracy: 0.8710 - val_loss: 1.1069 - val_accuracy: 0.7483\n",
            "Epoch 751/1000\n",
            "35/35 [==============================] - 0s 10ms/step - loss: 0.3057 - accuracy: 0.8651 - val_loss: 1.0762 - val_accuracy: 0.7619\n",
            "Epoch 752/1000\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.3080 - accuracy: 0.8651 - val_loss: 1.1049 - val_accuracy: 0.7551\n",
            "Epoch 753/1000\n",
            "35/35 [==============================] - 0s 8ms/step - loss: 0.3152 - accuracy: 0.8504 - val_loss: 1.1141 - val_accuracy: 0.7211\n",
            "Epoch 754/1000\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.3272 - accuracy: 0.8563 - val_loss: 1.1054 - val_accuracy: 0.7483\n",
            "Epoch 755/1000\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.3095 - accuracy: 0.8710 - val_loss: 1.0934 - val_accuracy: 0.7483\n",
            "Epoch 756/1000\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.3154 - accuracy: 0.8563 - val_loss: 1.0419 - val_accuracy: 0.7551\n",
            "Epoch 757/1000\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.3202 - accuracy: 0.8651 - val_loss: 1.0033 - val_accuracy: 0.7483\n",
            "Epoch 758/1000\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.3112 - accuracy: 0.8622 - val_loss: 1.0506 - val_accuracy: 0.7483\n",
            "Epoch 759/1000\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.3018 - accuracy: 0.8534 - val_loss: 1.0617 - val_accuracy: 0.7347\n",
            "Epoch 760/1000\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.3060 - accuracy: 0.8592 - val_loss: 1.0518 - val_accuracy: 0.7755\n",
            "Epoch 761/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3079 - accuracy: 0.8710 - val_loss: 1.0352 - val_accuracy: 0.7619\n",
            "Epoch 762/1000\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.3113 - accuracy: 0.8475 - val_loss: 1.0590 - val_accuracy: 0.7619\n",
            "Epoch 763/1000\n",
            "35/35 [==============================] - 0s 8ms/step - loss: 0.3078 - accuracy: 0.8592 - val_loss: 1.0780 - val_accuracy: 0.7347\n",
            "Epoch 764/1000\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.3202 - accuracy: 0.8592 - val_loss: 1.0694 - val_accuracy: 0.7483\n",
            "Epoch 765/1000\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.3059 - accuracy: 0.8592 - val_loss: 1.1086 - val_accuracy: 0.7347\n",
            "Epoch 766/1000\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.3082 - accuracy: 0.8680 - val_loss: 1.0237 - val_accuracy: 0.7551\n",
            "Epoch 767/1000\n",
            "35/35 [==============================] - 0s 7ms/step - loss: 0.3016 - accuracy: 0.8680 - val_loss: 1.1094 - val_accuracy: 0.7551\n",
            "Epoch 768/1000\n",
            "35/35 [==============================] - 0s 7ms/step - loss: 0.2970 - accuracy: 0.8680 - val_loss: 1.0888 - val_accuracy: 0.7619\n",
            "Epoch 769/1000\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.3070 - accuracy: 0.8651 - val_loss: 1.0665 - val_accuracy: 0.7687\n",
            "Epoch 770/1000\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.3131 - accuracy: 0.8739 - val_loss: 1.1332 - val_accuracy: 0.7347\n",
            "Epoch 771/1000\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.2970 - accuracy: 0.8739 - val_loss: 1.0846 - val_accuracy: 0.7619\n",
            "Epoch 772/1000\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.2987 - accuracy: 0.8710 - val_loss: 1.0715 - val_accuracy: 0.7619\n",
            "Epoch 773/1000\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.3085 - accuracy: 0.8739 - val_loss: 1.1527 - val_accuracy: 0.7483\n",
            "Epoch 774/1000\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.3027 - accuracy: 0.8622 - val_loss: 1.0280 - val_accuracy: 0.7619\n",
            "Epoch 775/1000\n",
            "35/35 [==============================] - 0s 7ms/step - loss: 0.3087 - accuracy: 0.8563 - val_loss: 1.0500 - val_accuracy: 0.7211\n",
            "Epoch 776/1000\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.3115 - accuracy: 0.8563 - val_loss: 1.0465 - val_accuracy: 0.7551\n",
            "Epoch 777/1000\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.3093 - accuracy: 0.8592 - val_loss: 1.0570 - val_accuracy: 0.7483\n",
            "Epoch 778/1000\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.3123 - accuracy: 0.8504 - val_loss: 1.1124 - val_accuracy: 0.7279\n",
            "Epoch 779/1000\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.3055 - accuracy: 0.8739 - val_loss: 1.0223 - val_accuracy: 0.7619\n",
            "Epoch 780/1000\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.2990 - accuracy: 0.8680 - val_loss: 1.0747 - val_accuracy: 0.7551\n",
            "Epoch 781/1000\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.3427 - accuracy: 0.8416 - val_loss: 1.1813 - val_accuracy: 0.7415\n",
            "Epoch 782/1000\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.3407 - accuracy: 0.8504 - val_loss: 0.8012 - val_accuracy: 0.7415\n",
            "Epoch 783/1000\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.3250 - accuracy: 0.8592 - val_loss: 0.8615 - val_accuracy: 0.7347\n",
            "Epoch 784/1000\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.3230 - accuracy: 0.8680 - val_loss: 1.1338 - val_accuracy: 0.7483\n",
            "Epoch 785/1000\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.3021 - accuracy: 0.8651 - val_loss: 1.1013 - val_accuracy: 0.7415\n",
            "Epoch 786/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.2949 - accuracy: 0.8768 - val_loss: 1.1218 - val_accuracy: 0.7347\n",
            "Epoch 787/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.2889 - accuracy: 0.8739 - val_loss: 1.1987 - val_accuracy: 0.7415\n",
            "Epoch 788/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3334 - accuracy: 0.8504 - val_loss: 1.1404 - val_accuracy: 0.7551\n",
            "Epoch 789/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.2937 - accuracy: 0.8739 - val_loss: 1.1470 - val_accuracy: 0.7415\n",
            "Epoch 790/1000\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.2959 - accuracy: 0.8798 - val_loss: 1.1147 - val_accuracy: 0.7483\n",
            "Epoch 791/1000\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.3144 - accuracy: 0.8475 - val_loss: 1.1872 - val_accuracy: 0.7347\n",
            "Epoch 792/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3144 - accuracy: 0.8651 - val_loss: 1.1131 - val_accuracy: 0.7415\n",
            "Epoch 793/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3002 - accuracy: 0.8651 - val_loss: 1.1448 - val_accuracy: 0.7483\n",
            "Epoch 794/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.2932 - accuracy: 0.8739 - val_loss: 1.1291 - val_accuracy: 0.7551\n",
            "Epoch 795/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.2940 - accuracy: 0.8886 - val_loss: 1.1043 - val_accuracy: 0.7483\n",
            "Epoch 796/1000\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.3450 - accuracy: 0.8475 - val_loss: 1.0834 - val_accuracy: 0.7211\n",
            "Epoch 797/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3005 - accuracy: 0.8563 - val_loss: 1.0063 - val_accuracy: 0.7619\n",
            "Epoch 798/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.2953 - accuracy: 0.8622 - val_loss: 1.0421 - val_accuracy: 0.7687\n",
            "Epoch 799/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3035 - accuracy: 0.8768 - val_loss: 1.0610 - val_accuracy: 0.7551\n",
            "Epoch 800/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.2956 - accuracy: 0.8798 - val_loss: 1.1010 - val_accuracy: 0.7551\n",
            "Epoch 801/1000\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.2953 - accuracy: 0.8739 - val_loss: 1.1349 - val_accuracy: 0.7415\n",
            "Epoch 802/1000\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.2989 - accuracy: 0.8592 - val_loss: 1.1120 - val_accuracy: 0.7551\n",
            "Epoch 803/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.2905 - accuracy: 0.8739 - val_loss: 1.1361 - val_accuracy: 0.7551\n",
            "Epoch 804/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.2996 - accuracy: 0.8710 - val_loss: 1.1429 - val_accuracy: 0.7415\n",
            "Epoch 805/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3016 - accuracy: 0.8534 - val_loss: 1.1021 - val_accuracy: 0.7551\n",
            "Epoch 806/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.2992 - accuracy: 0.8680 - val_loss: 1.1237 - val_accuracy: 0.7551\n",
            "Epoch 807/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3165 - accuracy: 0.8739 - val_loss: 1.1202 - val_accuracy: 0.7211\n",
            "Epoch 808/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3006 - accuracy: 0.8739 - val_loss: 1.1462 - val_accuracy: 0.7211\n",
            "Epoch 809/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3121 - accuracy: 0.8446 - val_loss: 1.1963 - val_accuracy: 0.7347\n",
            "Epoch 810/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.2980 - accuracy: 0.8651 - val_loss: 1.1400 - val_accuracy: 0.7415\n",
            "Epoch 811/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.2968 - accuracy: 0.8651 - val_loss: 1.1319 - val_accuracy: 0.7483\n",
            "Epoch 812/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3088 - accuracy: 0.8592 - val_loss: 1.0984 - val_accuracy: 0.7347\n",
            "Epoch 813/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3026 - accuracy: 0.8622 - val_loss: 1.0831 - val_accuracy: 0.7415\n",
            "Epoch 814/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.2949 - accuracy: 0.8680 - val_loss: 1.0898 - val_accuracy: 0.7551\n",
            "Epoch 815/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3165 - accuracy: 0.8710 - val_loss: 1.1293 - val_accuracy: 0.7347\n",
            "Epoch 816/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3055 - accuracy: 0.8475 - val_loss: 1.0752 - val_accuracy: 0.7755\n",
            "Epoch 817/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.2963 - accuracy: 0.8651 - val_loss: 1.0768 - val_accuracy: 0.7551\n",
            "Epoch 818/1000\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.3113 - accuracy: 0.8651 - val_loss: 1.1036 - val_accuracy: 0.7347\n",
            "Epoch 819/1000\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.2961 - accuracy: 0.8827 - val_loss: 1.1354 - val_accuracy: 0.7483\n",
            "Epoch 820/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.2926 - accuracy: 0.8680 - val_loss: 1.1682 - val_accuracy: 0.7483\n",
            "Epoch 821/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.2995 - accuracy: 0.8710 - val_loss: 1.0857 - val_accuracy: 0.7483\n",
            "Epoch 822/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3071 - accuracy: 0.8622 - val_loss: 1.0952 - val_accuracy: 0.7619\n",
            "Epoch 823/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3109 - accuracy: 0.8416 - val_loss: 1.1713 - val_accuracy: 0.7279\n",
            "Epoch 824/1000\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.3032 - accuracy: 0.8710 - val_loss: 1.1439 - val_accuracy: 0.7347\n",
            "Epoch 825/1000\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.2853 - accuracy: 0.8710 - val_loss: 1.1358 - val_accuracy: 0.7415\n",
            "Epoch 826/1000\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.2927 - accuracy: 0.8739 - val_loss: 1.1080 - val_accuracy: 0.7619\n",
            "Epoch 827/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.2983 - accuracy: 0.8739 - val_loss: 1.1514 - val_accuracy: 0.7483\n",
            "Epoch 828/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3230 - accuracy: 0.8622 - val_loss: 1.0829 - val_accuracy: 0.7347\n",
            "Epoch 829/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3104 - accuracy: 0.8504 - val_loss: 1.1080 - val_accuracy: 0.7415\n",
            "Epoch 830/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3036 - accuracy: 0.8622 - val_loss: 1.0885 - val_accuracy: 0.7551\n",
            "Epoch 831/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.2936 - accuracy: 0.8768 - val_loss: 1.1263 - val_accuracy: 0.7483\n",
            "Epoch 832/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.2888 - accuracy: 0.8768 - val_loss: 1.1476 - val_accuracy: 0.7415\n",
            "Epoch 833/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.2913 - accuracy: 0.8768 - val_loss: 1.1321 - val_accuracy: 0.7415\n",
            "Epoch 834/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.2925 - accuracy: 0.8768 - val_loss: 1.1730 - val_accuracy: 0.7551\n",
            "Epoch 835/1000\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.3216 - accuracy: 0.8475 - val_loss: 1.0879 - val_accuracy: 0.7483\n",
            "Epoch 836/1000\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.3103 - accuracy: 0.8534 - val_loss: 1.0474 - val_accuracy: 0.7755\n",
            "Epoch 837/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.2962 - accuracy: 0.8622 - val_loss: 1.1453 - val_accuracy: 0.7211\n",
            "Epoch 838/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.2972 - accuracy: 0.8622 - val_loss: 1.1372 - val_accuracy: 0.7415\n",
            "Epoch 839/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.2897 - accuracy: 0.8680 - val_loss: 1.0740 - val_accuracy: 0.7687\n",
            "Epoch 840/1000\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.3162 - accuracy: 0.8710 - val_loss: 1.1387 - val_accuracy: 0.7415\n",
            "Epoch 841/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3080 - accuracy: 0.8622 - val_loss: 1.1532 - val_accuracy: 0.7483\n",
            "Epoch 842/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.2937 - accuracy: 0.8710 - val_loss: 1.1120 - val_accuracy: 0.7551\n",
            "Epoch 843/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.2932 - accuracy: 0.8710 - val_loss: 1.0922 - val_accuracy: 0.7687\n",
            "Epoch 844/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.2986 - accuracy: 0.8710 - val_loss: 1.1248 - val_accuracy: 0.7483\n",
            "Epoch 845/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.2879 - accuracy: 0.8710 - val_loss: 1.1456 - val_accuracy: 0.7483\n",
            "Epoch 846/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.2930 - accuracy: 0.8768 - val_loss: 1.1032 - val_accuracy: 0.7415\n",
            "Epoch 847/1000\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.3029 - accuracy: 0.8768 - val_loss: 1.0904 - val_accuracy: 0.7415\n",
            "Epoch 848/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.2938 - accuracy: 0.8739 - val_loss: 1.1105 - val_accuracy: 0.7619\n",
            "Epoch 849/1000\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.2963 - accuracy: 0.8651 - val_loss: 1.1047 - val_accuracy: 0.7619\n",
            "Epoch 850/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3042 - accuracy: 0.8651 - val_loss: 1.1562 - val_accuracy: 0.7279\n",
            "Epoch 851/1000\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.2955 - accuracy: 0.8710 - val_loss: 1.1592 - val_accuracy: 0.7415\n",
            "Epoch 852/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.2859 - accuracy: 0.8710 - val_loss: 1.1814 - val_accuracy: 0.7415\n",
            "Epoch 853/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.2966 - accuracy: 0.8739 - val_loss: 1.1168 - val_accuracy: 0.7483\n",
            "Epoch 854/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3169 - accuracy: 0.8622 - val_loss: 1.0121 - val_accuracy: 0.7619\n",
            "Epoch 855/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3053 - accuracy: 0.8651 - val_loss: 1.1563 - val_accuracy: 0.7483\n",
            "Epoch 856/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.2863 - accuracy: 0.8827 - val_loss: 1.1148 - val_accuracy: 0.7415\n",
            "Epoch 857/1000\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.2924 - accuracy: 0.8827 - val_loss: 1.1485 - val_accuracy: 0.7483\n",
            "Epoch 858/1000\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.2976 - accuracy: 0.8680 - val_loss: 1.1457 - val_accuracy: 0.7687\n",
            "Epoch 859/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.2991 - accuracy: 0.8710 - val_loss: 1.1032 - val_accuracy: 0.7483\n",
            "Epoch 860/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3106 - accuracy: 0.8651 - val_loss: 1.1461 - val_accuracy: 0.7483\n",
            "Epoch 861/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3488 - accuracy: 0.8563 - val_loss: 1.1173 - val_accuracy: 0.7347\n",
            "Epoch 862/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3071 - accuracy: 0.8592 - val_loss: 1.0446 - val_accuracy: 0.7415\n",
            "Epoch 863/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.2921 - accuracy: 0.8739 - val_loss: 1.1246 - val_accuracy: 0.7551\n",
            "Epoch 864/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.2922 - accuracy: 0.8710 - val_loss: 1.1251 - val_accuracy: 0.7483\n",
            "Epoch 865/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.2878 - accuracy: 0.8768 - val_loss: 1.1619 - val_accuracy: 0.7415\n",
            "Epoch 866/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3014 - accuracy: 0.8680 - val_loss: 1.1725 - val_accuracy: 0.7551\n",
            "Epoch 867/1000\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.2997 - accuracy: 0.8827 - val_loss: 1.1354 - val_accuracy: 0.7347\n",
            "Epoch 868/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3084 - accuracy: 0.8651 - val_loss: 1.0966 - val_accuracy: 0.7483\n",
            "Epoch 869/1000\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.2956 - accuracy: 0.8739 - val_loss: 1.1107 - val_accuracy: 0.7415\n",
            "Epoch 870/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.2877 - accuracy: 0.8739 - val_loss: 1.0949 - val_accuracy: 0.7483\n",
            "Epoch 871/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.2891 - accuracy: 0.8798 - val_loss: 1.1771 - val_accuracy: 0.7347\n",
            "Epoch 872/1000\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.2941 - accuracy: 0.8739 - val_loss: 1.1364 - val_accuracy: 0.7619\n",
            "Epoch 873/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.2904 - accuracy: 0.8622 - val_loss: 1.1413 - val_accuracy: 0.7551\n",
            "Epoch 874/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3045 - accuracy: 0.8710 - val_loss: 1.2047 - val_accuracy: 0.7211\n",
            "Epoch 875/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3125 - accuracy: 0.8475 - val_loss: 1.1705 - val_accuracy: 0.7211\n",
            "Epoch 876/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.2950 - accuracy: 0.8710 - val_loss: 1.1686 - val_accuracy: 0.7347\n",
            "Epoch 877/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3077 - accuracy: 0.8651 - val_loss: 1.1262 - val_accuracy: 0.7551\n",
            "Epoch 878/1000\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.2993 - accuracy: 0.8563 - val_loss: 1.0405 - val_accuracy: 0.7415\n",
            "Epoch 879/1000\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.2902 - accuracy: 0.8710 - val_loss: 1.0843 - val_accuracy: 0.7415\n",
            "Epoch 880/1000\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.2930 - accuracy: 0.8798 - val_loss: 1.0884 - val_accuracy: 0.7619\n",
            "Epoch 881/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.2956 - accuracy: 0.8651 - val_loss: 1.0936 - val_accuracy: 0.7551\n",
            "Epoch 882/1000\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.3874 - accuracy: 0.8358 - val_loss: 1.1247 - val_accuracy: 0.7143\n",
            "Epoch 883/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3251 - accuracy: 0.8387 - val_loss: 1.1724 - val_accuracy: 0.7551\n",
            "Epoch 884/1000\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.3133 - accuracy: 0.8534 - val_loss: 1.2006 - val_accuracy: 0.7483\n",
            "Epoch 885/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3261 - accuracy: 0.8504 - val_loss: 1.0989 - val_accuracy: 0.7483\n",
            "Epoch 886/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3069 - accuracy: 0.8622 - val_loss: 1.2703 - val_accuracy: 0.7415\n",
            "Epoch 887/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3243 - accuracy: 0.8622 - val_loss: 0.9070 - val_accuracy: 0.7415\n",
            "Epoch 888/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.2978 - accuracy: 0.8651 - val_loss: 1.1813 - val_accuracy: 0.7279\n",
            "Epoch 889/1000\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.2861 - accuracy: 0.8622 - val_loss: 1.1795 - val_accuracy: 0.7483\n",
            "Epoch 890/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3043 - accuracy: 0.8798 - val_loss: 1.1767 - val_accuracy: 0.7551\n",
            "Epoch 891/1000\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.2999 - accuracy: 0.8680 - val_loss: 1.1648 - val_accuracy: 0.7551\n",
            "Epoch 892/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3176 - accuracy: 0.8416 - val_loss: 1.2731 - val_accuracy: 0.7143\n",
            "Epoch 893/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.2868 - accuracy: 0.8680 - val_loss: 1.2578 - val_accuracy: 0.7415\n",
            "Epoch 894/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3005 - accuracy: 0.8563 - val_loss: 1.1138 - val_accuracy: 0.7415\n",
            "Epoch 895/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.2925 - accuracy: 0.8768 - val_loss: 1.1332 - val_accuracy: 0.7483\n",
            "Epoch 896/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3023 - accuracy: 0.8680 - val_loss: 1.1614 - val_accuracy: 0.7551\n",
            "Epoch 897/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.2990 - accuracy: 0.8592 - val_loss: 1.2225 - val_accuracy: 0.7483\n",
            "Epoch 898/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.2975 - accuracy: 0.8680 - val_loss: 1.0683 - val_accuracy: 0.7619\n",
            "Epoch 899/1000\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.2965 - accuracy: 0.8622 - val_loss: 1.2611 - val_accuracy: 0.6531\n",
            "Epoch 900/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3330 - accuracy: 0.8270 - val_loss: 1.1564 - val_accuracy: 0.7347\n",
            "Epoch 901/1000\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.2960 - accuracy: 0.8856 - val_loss: 1.1534 - val_accuracy: 0.7551\n",
            "Epoch 902/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3029 - accuracy: 0.8592 - val_loss: 1.1982 - val_accuracy: 0.7483\n",
            "Epoch 903/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.2858 - accuracy: 0.8710 - val_loss: 1.1320 - val_accuracy: 0.7687\n",
            "Epoch 904/1000\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.2876 - accuracy: 0.8680 - val_loss: 1.1956 - val_accuracy: 0.7551\n",
            "Epoch 905/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.2894 - accuracy: 0.8739 - val_loss: 1.1393 - val_accuracy: 0.7687\n",
            "Epoch 906/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3028 - accuracy: 0.8710 - val_loss: 1.1541 - val_accuracy: 0.7619\n",
            "Epoch 907/1000\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.3033 - accuracy: 0.8504 - val_loss: 1.1111 - val_accuracy: 0.7415\n",
            "Epoch 908/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.2949 - accuracy: 0.8739 - val_loss: 1.0941 - val_accuracy: 0.7551\n",
            "Epoch 909/1000\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.2861 - accuracy: 0.8710 - val_loss: 1.1661 - val_accuracy: 0.7551\n",
            "Epoch 910/1000\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.2879 - accuracy: 0.8710 - val_loss: 1.1178 - val_accuracy: 0.7687\n",
            "Epoch 911/1000\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.2912 - accuracy: 0.8827 - val_loss: 1.1822 - val_accuracy: 0.7347\n",
            "Epoch 912/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.2842 - accuracy: 0.8651 - val_loss: 1.1206 - val_accuracy: 0.7619\n",
            "Epoch 913/1000\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.3015 - accuracy: 0.8592 - val_loss: 1.1770 - val_accuracy: 0.7619\n",
            "Epoch 914/1000\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.2860 - accuracy: 0.8798 - val_loss: 1.1934 - val_accuracy: 0.7483\n",
            "Epoch 915/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.2943 - accuracy: 0.8710 - val_loss: 1.1934 - val_accuracy: 0.7687\n",
            "Epoch 916/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3239 - accuracy: 0.8387 - val_loss: 1.1047 - val_accuracy: 0.7687\n",
            "Epoch 917/1000\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.2909 - accuracy: 0.8592 - val_loss: 1.1351 - val_accuracy: 0.7687\n",
            "Epoch 918/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.2972 - accuracy: 0.8563 - val_loss: 1.2087 - val_accuracy: 0.7279\n",
            "Epoch 919/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.2899 - accuracy: 0.8710 - val_loss: 1.1537 - val_accuracy: 0.7347\n",
            "Epoch 920/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3236 - accuracy: 0.8534 - val_loss: 1.1033 - val_accuracy: 0.7619\n",
            "Epoch 921/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.2958 - accuracy: 0.8739 - val_loss: 1.1404 - val_accuracy: 0.7551\n",
            "Epoch 922/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.4037 - accuracy: 0.8006 - val_loss: 1.1021 - val_accuracy: 0.7279\n",
            "Epoch 923/1000\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.3548 - accuracy: 0.8416 - val_loss: 1.0702 - val_accuracy: 0.7279\n",
            "Epoch 924/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3289 - accuracy: 0.8387 - val_loss: 1.1235 - val_accuracy: 0.7551\n",
            "Epoch 925/1000\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.3092 - accuracy: 0.8475 - val_loss: 1.0875 - val_accuracy: 0.7483\n",
            "Epoch 926/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3045 - accuracy: 0.8592 - val_loss: 1.1661 - val_accuracy: 0.7347\n",
            "Epoch 927/1000\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.2949 - accuracy: 0.8710 - val_loss: 1.1487 - val_accuracy: 0.7279\n",
            "Epoch 928/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3188 - accuracy: 0.8651 - val_loss: 1.2245 - val_accuracy: 0.7211\n",
            "Epoch 929/1000\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.3161 - accuracy: 0.8475 - val_loss: 1.1370 - val_accuracy: 0.7551\n",
            "Epoch 930/1000\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.3026 - accuracy: 0.8592 - val_loss: 1.1765 - val_accuracy: 0.7347\n",
            "Epoch 931/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.2911 - accuracy: 0.8739 - val_loss: 1.1802 - val_accuracy: 0.7483\n",
            "Epoch 932/1000\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.2871 - accuracy: 0.8827 - val_loss: 1.1603 - val_accuracy: 0.7483\n",
            "Epoch 933/1000\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.2926 - accuracy: 0.8504 - val_loss: 1.1616 - val_accuracy: 0.7551\n",
            "Epoch 934/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.2860 - accuracy: 0.8622 - val_loss: 1.1640 - val_accuracy: 0.7483\n",
            "Epoch 935/1000\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.2856 - accuracy: 0.8622 - val_loss: 1.2032 - val_accuracy: 0.7483\n",
            "Epoch 936/1000\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.2842 - accuracy: 0.8680 - val_loss: 1.1526 - val_accuracy: 0.7619\n",
            "Epoch 937/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.2858 - accuracy: 0.8592 - val_loss: 1.1926 - val_accuracy: 0.7551\n",
            "Epoch 938/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3088 - accuracy: 0.8768 - val_loss: 1.1126 - val_accuracy: 0.7483\n",
            "Epoch 939/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.2853 - accuracy: 0.8710 - val_loss: 1.1359 - val_accuracy: 0.7619\n",
            "Epoch 940/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.2878 - accuracy: 0.8622 - val_loss: 1.1679 - val_accuracy: 0.7415\n",
            "Epoch 941/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.2882 - accuracy: 0.8739 - val_loss: 1.1618 - val_accuracy: 0.7347\n",
            "Epoch 942/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.2884 - accuracy: 0.8827 - val_loss: 1.1634 - val_accuracy: 0.7551\n",
            "Epoch 943/1000\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.6319 - accuracy: 0.7038 - val_loss: 1.1624 - val_accuracy: 0.7347\n",
            "Epoch 944/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.4498 - accuracy: 0.7859 - val_loss: 1.2342 - val_accuracy: 0.7415\n",
            "Epoch 945/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3548 - accuracy: 0.8446 - val_loss: 1.1352 - val_accuracy: 0.7483\n",
            "Epoch 946/1000\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.3563 - accuracy: 0.8270 - val_loss: 1.1530 - val_accuracy: 0.7619\n",
            "Epoch 947/1000\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.3560 - accuracy: 0.8446 - val_loss: 1.1974 - val_accuracy: 0.7347\n",
            "Epoch 948/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3238 - accuracy: 0.8592 - val_loss: 1.2113 - val_accuracy: 0.7415\n",
            "Epoch 949/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3270 - accuracy: 0.8622 - val_loss: 1.1709 - val_accuracy: 0.7483\n",
            "Epoch 950/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3245 - accuracy: 0.8563 - val_loss: 1.1631 - val_accuracy: 0.7483\n",
            "Epoch 951/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3216 - accuracy: 0.8710 - val_loss: 1.1989 - val_accuracy: 0.7483\n",
            "Epoch 952/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3235 - accuracy: 0.8563 - val_loss: 1.1823 - val_accuracy: 0.7551\n",
            "Epoch 953/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3216 - accuracy: 0.8563 - val_loss: 1.1911 - val_accuracy: 0.7551\n",
            "Epoch 954/1000\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.3185 - accuracy: 0.8622 - val_loss: 1.1899 - val_accuracy: 0.7483\n",
            "Epoch 955/1000\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.3132 - accuracy: 0.8651 - val_loss: 1.1777 - val_accuracy: 0.7483\n",
            "Epoch 956/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3008 - accuracy: 0.8651 - val_loss: 1.2610 - val_accuracy: 0.7483\n",
            "Epoch 957/1000\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.2941 - accuracy: 0.8651 - val_loss: 1.2232 - val_accuracy: 0.7619\n",
            "Epoch 958/1000\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.2889 - accuracy: 0.8886 - val_loss: 1.1910 - val_accuracy: 0.7823\n",
            "Epoch 959/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.2872 - accuracy: 0.8710 - val_loss: 1.1766 - val_accuracy: 0.7687\n",
            "Epoch 960/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.2897 - accuracy: 0.8710 - val_loss: 1.2193 - val_accuracy: 0.7211\n",
            "Epoch 961/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.2913 - accuracy: 0.8798 - val_loss: 1.1136 - val_accuracy: 0.7551\n",
            "Epoch 962/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.2954 - accuracy: 0.8592 - val_loss: 1.2011 - val_accuracy: 0.7551\n",
            "Epoch 963/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3064 - accuracy: 0.8534 - val_loss: 1.1519 - val_accuracy: 0.7687\n",
            "Epoch 964/1000\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.2861 - accuracy: 0.8739 - val_loss: 1.1747 - val_accuracy: 0.7551\n",
            "Epoch 965/1000\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.2835 - accuracy: 0.8739 - val_loss: 1.1916 - val_accuracy: 0.7551\n",
            "Epoch 966/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.2833 - accuracy: 0.8680 - val_loss: 1.1764 - val_accuracy: 0.7483\n",
            "Epoch 967/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.2997 - accuracy: 0.8651 - val_loss: 1.1829 - val_accuracy: 0.7551\n",
            "Epoch 968/1000\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.2936 - accuracy: 0.8710 - val_loss: 1.1981 - val_accuracy: 0.7347\n",
            "Epoch 969/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3012 - accuracy: 0.8563 - val_loss: 1.1284 - val_accuracy: 0.7687\n",
            "Epoch 970/1000\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.2916 - accuracy: 0.8739 - val_loss: 1.1320 - val_accuracy: 0.7619\n",
            "Epoch 971/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.2812 - accuracy: 0.8739 - val_loss: 1.2276 - val_accuracy: 0.7415\n",
            "Epoch 972/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.2868 - accuracy: 0.8768 - val_loss: 1.1310 - val_accuracy: 0.7551\n",
            "Epoch 973/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.2823 - accuracy: 0.8680 - val_loss: 1.1618 - val_accuracy: 0.7483\n",
            "Epoch 974/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.2868 - accuracy: 0.8710 - val_loss: 1.1412 - val_accuracy: 0.7551\n",
            "Epoch 975/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.2858 - accuracy: 0.8768 - val_loss: 1.1461 - val_accuracy: 0.7483\n",
            "Epoch 976/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.2952 - accuracy: 0.8710 - val_loss: 1.1493 - val_accuracy: 0.7347\n",
            "Epoch 977/1000\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.2842 - accuracy: 0.8622 - val_loss: 1.1716 - val_accuracy: 0.7551\n",
            "Epoch 978/1000\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.3313 - accuracy: 0.8710 - val_loss: 1.1180 - val_accuracy: 0.7687\n",
            "Epoch 979/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3023 - accuracy: 0.8710 - val_loss: 1.1171 - val_accuracy: 0.7483\n",
            "Epoch 980/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3042 - accuracy: 0.8710 - val_loss: 1.1268 - val_accuracy: 0.7483\n",
            "Epoch 981/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.2947 - accuracy: 0.8651 - val_loss: 1.1700 - val_accuracy: 0.7279\n",
            "Epoch 982/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.4508 - accuracy: 0.7742 - val_loss: 0.9139 - val_accuracy: 0.7551\n",
            "Epoch 983/1000\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.3944 - accuracy: 0.8240 - val_loss: 0.9728 - val_accuracy: 0.7347\n",
            "Epoch 984/1000\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.3524 - accuracy: 0.8416 - val_loss: 1.1379 - val_accuracy: 0.7483\n",
            "Epoch 985/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3157 - accuracy: 0.8622 - val_loss: 1.1232 - val_accuracy: 0.7619\n",
            "Epoch 986/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.2949 - accuracy: 0.8768 - val_loss: 1.0842 - val_accuracy: 0.7483\n",
            "Epoch 987/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.2869 - accuracy: 0.8739 - val_loss: 1.1375 - val_accuracy: 0.7551\n",
            "Epoch 988/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.2860 - accuracy: 0.8739 - val_loss: 1.1522 - val_accuracy: 0.7687\n",
            "Epoch 989/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.2887 - accuracy: 0.8768 - val_loss: 1.1847 - val_accuracy: 0.7687\n",
            "Epoch 990/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.2836 - accuracy: 0.8739 - val_loss: 1.1396 - val_accuracy: 0.7483\n",
            "Epoch 991/1000\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.2853 - accuracy: 0.8768 - val_loss: 1.1857 - val_accuracy: 0.7483\n",
            "Epoch 992/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.2849 - accuracy: 0.8710 - val_loss: 1.1066 - val_accuracy: 0.7687\n",
            "Epoch 993/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.2802 - accuracy: 0.8798 - val_loss: 1.1984 - val_accuracy: 0.7483\n",
            "Epoch 994/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.2846 - accuracy: 0.8768 - val_loss: 1.1047 - val_accuracy: 0.7551\n",
            "Epoch 995/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.2850 - accuracy: 0.8768 - val_loss: 1.1292 - val_accuracy: 0.7687\n",
            "Epoch 996/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.2977 - accuracy: 0.8798 - val_loss: 1.1730 - val_accuracy: 0.7619\n",
            "Epoch 997/1000\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.3093 - accuracy: 0.8680 - val_loss: 1.1026 - val_accuracy: 0.7483\n",
            "Epoch 998/1000\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.2931 - accuracy: 0.8798 - val_loss: 1.0670 - val_accuracy: 0.7551\n",
            "Epoch 999/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.2876 - accuracy: 0.8798 - val_loss: 1.1147 - val_accuracy: 0.7551\n",
            "Epoch 1000/1000\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.2851 - accuracy: 0.8798 - val_loss: 1.1071 - val_accuracy: 0.7551\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f7a28867450>"
            ]
          },
          "metadata": {},
          "execution_count": 130
        }
      ],
      "source": [
        "# to trained the model\n",
        "model.fit(Thyroid_X_train,cat_Thyroid_Y_train, validation_data=(Thyroid_X_test,cat_Thyroid_Y_test) ,epochs=1000,batch_size=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qkteKSEhxerT",
        "outputId": "f9597e26-f337-4955-a868-712801b23371"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11/11 [==============================] - 0s 1ms/step - loss: 0.2849 - accuracy: 0.8680\n"
          ]
        }
      ],
      "source": [
        "# to get the accuracy of ann algo\n",
        "accuracy= model.evaluate(Thyroid_X_train,cat_Thyroid_Y_train)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {
        "id": "Ss_Vu_Wpxenn"
      },
      "outputs": [],
      "source": [
        " # importing cr , cm \n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "# initialising the predict variable\n",
        "pred4 = model.predict(Thyroid_X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qgP50HooN5f9",
        "outputId": "5d4eb289-b295-423e-e67a-742d8146c8e4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(147, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 133
        }
      ],
      "source": [
        "pred4.shape # to get the shape "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {
        "id": "FVNx1CSDKJhA"
      },
      "outputs": [],
      "source": [
        "# to get the maximum argument\n",
        "pred4=np.argmax(pred4, axis=1)\n",
        "cat_Thyroid_Y_test=np.argmax(cat_Thyroid_Y_test,axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UxuziaonNEpo",
        "outputId": "2584a0d0-350a-4531-d2cd-4b97fe4d85d6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(147,)"
            ]
          },
          "metadata": {},
          "execution_count": 135
        }
      ],
      "source": [
        "pred4.shape # to get the shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SsH1RF5wNKjT",
        "outputId": "03cefcde-bbb4-4774-ad35-f1614bc02770"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(147,)"
            ]
          },
          "metadata": {},
          "execution_count": 136
        }
      ],
      "source": [
        "cat_Thyroid_Y_test.shape # to get the shape "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {
        "id": "SvaWECONxejx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d00f1def-0385-4928-e8f8-34a1982a2aca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.73      0.75        74\n",
            "           1       0.74      0.78      0.76        73\n",
            "\n",
            "    accuracy                           0.76       147\n",
            "   macro avg       0.76      0.76      0.76       147\n",
            "weighted avg       0.76      0.76      0.75       147\n",
            "\n",
            "\n",
            "Accuracy:  0.7551020408163265\n"
          ]
        }
      ],
      "source": [
        "# to print the c_r\n",
        "print(classification_report(cat_Thyroid_Y_test, pred4)) \n",
        "print()\n",
        "# reprinting the performance \n",
        "print('Accuracy: ', accuracy_score(cat_Thyroid_Y_test, pred4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJvb_l4oxeey",
        "outputId": "07f8ba31-289a-497f-e801-cceaa2d09aed"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[54, 20],\n",
              "       [16, 57]])"
            ]
          },
          "metadata": {},
          "execution_count": 138
        }
      ],
      "source": [
        "#  to import the cm\n",
        "from sklearn.metrics import confusion_matrix\n",
        "# initialize the  cm\n",
        "cm4 = confusion_matrix(cat_Thyroid_Y_test,pred4)\n",
        "cm4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "EdQ9CFM_xeZy",
        "outputId": "c19ef454-9f6b-44ad-9417-6a242f98d2c0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(33.0, 0.5, 'Original Values')"
            ]
          },
          "metadata": {},
          "execution_count": 139
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# making heat map \n",
        "import seaborn as sns\n",
        "# assigning the color \n",
        "sns.heatmap(cm4,annot=True, fmt='d' , cmap='Pastel1_r')\n",
        "# assigning the x-axis label\n",
        "plt.xlabel('Predicted values')\n",
        "# assigning the y-axis label\n",
        "plt.ylabel('Original Values')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3zIT8StExeRk",
        "outputId": "4cd77919-ef07-4b5d-b75a-940359119e25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sensitivity :  0.8260869565217391\n",
            "Specificity :  0.7297297297297297\n"
          ]
        }
      ],
      "source": [
        "# printing Sensitivity accuracy \n",
        "sensitivity = cm4[1,1]/(cm[1,1]+cm4[1,0])\n",
        "# to get the score of senstivity\n",
        "print('Sensitivity : ', sensitivity)\n",
        "# printing Specificity accuracy \n",
        "specificity = cm4[0,0]/(cm4[0,1]+cm4[0,0])\n",
        "# to get the score of specificity\n",
        "print('Specificity : ', specificity)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dldjA-6OE4_Z"
      },
      "source": [
        "# **RandomForestClassifier**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EoOFfQUypStV",
        "outputId": "c5e2e955-8753-44f6-b4f5-42e36031a2b8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8775510204081632"
            ]
          },
          "metadata": {},
          "execution_count": 141
        }
      ],
      "source": [
        "# importing rf algo\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "# initialize the algo\n",
        "model2 = RandomForestClassifier(max_depth=10, n_estimators=8 )\n",
        "# to trained the algo\n",
        "model2.fit(Thyroid_X_train,Thyroid_Y_train)\n",
        "# to get the score \n",
        "model2.score(Thyroid_X_test,Thyroid_Y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {
        "id": "srKNZmrkpSqT"
      },
      "outputs": [],
      "source": [
        "\n",
        "# importing cr , cm \n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "# initialising the predict variable\n",
        "pred2 = model2.predict(Thyroid_X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {
        "id": "9ce13T8KpSkC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54c53940-6ef5-4721-c4e5-fb8a3a44af01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.92      0.88        74\n",
            "           1       0.91      0.84      0.87        73\n",
            "\n",
            "    accuracy                           0.88       147\n",
            "   macro avg       0.88      0.88      0.88       147\n",
            "weighted avg       0.88      0.88      0.88       147\n",
            "\n",
            "\n",
            "Accuracy:  0.8775510204081632\n"
          ]
        }
      ],
      "source": [
        "# to get the cr\n",
        "print(classification_report(Thyroid_Y_test, pred2)) \n",
        "print()\n",
        "# reprinting the performance \n",
        "print('Accuracy: ', accuracy_score(Thyroid_Y_test, pred2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8-Jv-D3pShC",
        "outputId": "0e085edc-65b3-4aeb-d02b-ad2bb847491e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision by RandomForest of testing data is: 0.878\n",
            "Recall by RandomForest of testing data is: 0.878\n",
            "F1 score by RandomForest of testing data is: 0.878\n"
          ]
        }
      ],
      "source": [
        "# various libraries for report\n",
        "from sklearn.metrics import precision_score,recall_score,f1_score\n",
        "# printing the performance \n",
        "print('Precision by RandomForest of testing data is: %.3f' % precision_score(Thyroid_Y_test, pred2,average='micro')) \n",
        "# checking the precision value\n",
        "print('Recall by RandomForest of testing data is: %.3f' % recall_score(Thyroid_Y_test, pred2,average='micro')) \n",
        "# checking the recall value\n",
        "print('F1 score by RandomForest of testing data is: %.3f' % f1_score(Thyroid_Y_test, pred2,average='micro')) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {
        "id": "I8bG-hM7pSaA"
      },
      "outputs": [],
      "source": [
        "# importing cm\n",
        "from sklearn.metrics import confusion_matrix\n",
        "# initializing cm\n",
        "cm1 = confusion_matrix(Thyroid_Y_test,pred2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "e___Uk0ipSXH",
        "outputId": "3fe99aa3-b1ca-401a-d264-8df747104be8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(33.0, 0.5, 'Original Values')"
            ]
          },
          "metadata": {},
          "execution_count": 146
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# making heat map \n",
        "import seaborn as sns\n",
        "# assigning the color \n",
        "sns.heatmap(cm,annot=True, fmt='d' , cmap='Pastel2_r')\n",
        "# assigning the x-axis label\n",
        "plt.xlabel('Predicted values')\n",
        "# assigning the y-axis label\n",
        "plt.ylabel('Original Values')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kaHsDFmeEaqz",
        "outputId": "5ebcb1de-cc95-4be4-f7ea-48c7e15338dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sensitivity :  0.8356164383561644\n",
            "Specificity :  0.918918918918919\n"
          ]
        }
      ],
      "source": [
        "# printing Sensitivity accuracy \n",
        "sensitivity = cm1[1,1]/(cm1[1,1]+cm1[1,0])\n",
        "# to get the score of sentivity\n",
        "print('Sensitivity : ', sensitivity)\n",
        "# printing Specificity accuracy \n",
        "specificity = cm1[0,0]/(cm1[0,1]+cm1[0,0])\n",
        "# to get the score of specificity\n",
        "print('Specificity : ', specificity)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {
        "id": "kf_HISjt1oS1"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}